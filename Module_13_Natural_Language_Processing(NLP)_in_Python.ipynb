{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP24ESVx7rIsOFnN7AOTfNh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pattiecodes/DC_MLScientist_in_Py/blob/main/Module_13_Natural_Language_Processing(NLP)_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Module Start ---"
      ],
      "metadata": {
        "id": "z1sdzBM3jUqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentence and word tokenization\n",
        "Tokenization is an important first step in NLP. It involves breaking text into smaller units called tokens, which is key to working with language data. Your task is to tokenize a snippet of a news article into both sentences and words.\n",
        "\n",
        "Instructions 1/2\n",
        "\n",
        "Import the nltk library.\n",
        "Download the punkt_tab package.\n",
        "Tokenize the text into sentences."
      ],
      "metadata": {
        "id": "tC6Be4dSj9px"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwtSDNFMhDYZ"
      },
      "outputs": [],
      "source": [
        "# Import nltk\n",
        "import nltk\n",
        "# Download the punkt_tab package\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text = \"\"\"\n",
        "The stock market saw a significant dip today. Experts believe the downturn may continue.\n",
        "However, many investors are optimistic about future growth.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize the text into sentences\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "print(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "50 XP\n",
        "2\n",
        "Tokenize the first sentence you obtained into words."
      ],
      "metadata": {
        "id": "U7HDcLuVkCUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text = \"\"\"\n",
        "The stock market saw a significant dip today. Experts believe the downturn may continue.\n",
        "However, many investors are optimistic about future growth.\n",
        "\"\"\"\n",
        "\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "# Tokenize the first sentence into words\n",
        "words = nltk.word_tokenize(sentences[0])\n",
        "print(words)"
      ],
      "metadata": {
        "id": "DtXsf-1XkCjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removing stop words\n",
        "You're working on a project where the goal is to classify feedback from users into different categories like \"product issues\", \"service issues\", and \"suggestions\". Often, stop words don't carry much meaning in distinguishing between categories. Your task is to remove these stop words to focus on the important words that will help a machine later on categorize the feedback into the correct topics.\n",
        "\n",
        "The functions word_tokenize from nltk.tokenize and stopwords.words from nltk.corpus have been imported for you. Additionally, the NLTK resources punkt_tab and stopwords have already been downloaded.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Tokenize the provided feedback into words.\n",
        "Get the list of English stopwords.\n",
        "Remove English stop words and save the result in filtered_tokens.\n"
      ],
      "metadata": {
        "id": "fCsR_HlIk-1M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feedback = \"I reached out to support and got a helpful response within minutes!!! Very #impressed\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(feedback)\n",
        "\n",
        "# Get the list of English stop words\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "# Remove stop words\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "id": "t41gknTck_Hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removing punctuation\n",
        "Now that you've removed stop words from the feedback text, it's time to handle punctuation. The tokens you obtained in the previous exercise still contain punctuation marks, which are often unnecessary when categorizing feedback.\n",
        "\n",
        "Your task is to remove punctuation from the list of tokens provided, helping to clean up the data even further.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Clean the filtered_tokens list by removing all punctuation."
      ],
      "metadata": {
        "id": "JswJ_MmylGnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "filtered_tokens = ['reached', 'support', 'got', 'helpful', 'response', 'within', 'minutes', '!', '!', '!', '#', 'impressed']\n",
        "\n",
        "# Remove punctuation\n",
        "clean_tokens = [word for word in filtered_tokens if word not in string.punctuation]\n",
        "\n",
        "print(clean_tokens)"
      ],
      "metadata": {
        "id": "7Qg-0thElGzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lowercasing\n",
        "You're analyzing user reviews for a travel website. These reviews often include inconsistent capitalization like \"TRAVEL\" and \"travel\". To prepare the text for sentiment analysis and topic extraction, you'll first convert all words to lowercase, then tokenize them and clean them from stop words and punctuation.\n",
        "\n",
        "The word_tokenize() function, a stop_words list have been provided. NLTK resources are already downloaded.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Convert the provided review into lowercase.\n",
        "Tokenize the lower_text into words.\n",
        "Use list comprehension to remove stop words and punctuation"
      ],
      "metadata": {
        "id": "0t34KKcUluWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review = \"I have been FLYING a lot lately and the Flights just keep getting DELAYED. Honestly, traveling for WORK gets exhausting with endless delays, but every trip teaches you something new!\"\n",
        "\n",
        "# Lowercase the review\n",
        "lower_text = review.lower()\n",
        "\n",
        "# Tokenize the lower_text into words\n",
        "tokens = word_tokenize(lower_text)\n",
        "\n",
        "# Remove stop words and punctuation\n",
        "clean_tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
        "\n",
        "print(clean_tokens)"
      ],
      "metadata": {
        "id": "dghWmtsGlupG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming\n",
        "Now that you've cleaned the review text and removed stop words and punctuation, you're ready to normalize the remaining words using stemming to reduce words to their root form. This helps group similar words together, making your analysis more consistent and efficient.\n",
        "\n",
        "The PorterStemmer class has been provided, along with a list of clean_tokens.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Initialize the PorterStemmer().\n",
        "Use a list comprehension to stem each token from the clean_tokens list."
      ],
      "metadata": {
        "id": "UbFBI4AKmA68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_tokens = ['flying', 'lot', 'lately', 'flights', 'keep', 'getting', 'delayed', 'honestly', 'traveling', 'work', 'gets', 'exhausting', 'endless', 'delays', 'every', 'travel', 'teaches', 'something', 'new']\n",
        "\n",
        "# Create stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Stem each token\n",
        "stemmed_tokens = [stemmer.stem(word) for word in clean_tokens]\n",
        "\n",
        "print(stemmed_tokens)"
      ],
      "metadata": {
        "id": "nRg0wbVUmBKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization\n",
        "While continuing your analysis of user reviews, you noticed that stemming sometimes produces non-standard words like \"fli\" from \"flying\", which can reduce interpretability. To address this, you'll now use lemmatization, which returns actual words and helps improve the clarity and accuracy of your analysis.\n",
        "\n",
        "WordNetLemmatizer has been imported, stop_words has been defined, and the necessary NLTK resources have been downloaded.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create an instance lemmatizer of the WordNetLemmatizer() class.\n",
        "Use the lemmatizer to lemmatize the lower_tokens."
      ],
      "metadata": {
        "id": "FPuzIWPvmLIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_tokens = ['flying', 'lot', 'lately', 'flights', 'keep', 'getting', 'delayed', 'honestly', 'traveling', 'work', 'gets', 'exhausting', 'endless', 'delays', 'every', 'travel', 'teaches', 'something', 'new']\n",
        "\n",
        "# Create lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize each token\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in clean_tokens]\n",
        "\n",
        "print(lemmatized_tokens)"
      ],
      "metadata": {
        "id": "EtNDkiBpmLhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building vocabulary from customer reviews\n",
        "You're part of a product analytics team at TechZone, a consumer electronics company. You've received a small batch of customer reviews for a new gadget. To analyze the reviews, you'll first preprocess the text and build a vocabulary, a list of unique words that defines the features used to represent each review as numerical data.\n",
        "\n",
        "A preprocess() function is pre-loaded for you. It lowercases the text, tokenizes it, and removes punctuation.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Preprocess each review in the dataset using the preprocess() function.\n",
        "Fit the vectorizer on the preprocessed reviews.\n",
        "Print the resulting vocabulary."
      ],
      "metadata": {
        "id": "vU_5aG67oI6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = [\n",
        "    \"The product is fantastic! It works like a charm.\",\n",
        "    \"I hated the product. It broke after one use.\",\n",
        "    \"Product was okay, not the best, but fine overall.\"\n",
        "]\n",
        "# Preprocess the reviews\n",
        "cleaned_reviews = [preprocess(review) for review in reviews]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "# Fit the vectorizer\n",
        "vectorizer.fit(cleaned_reviews)\n",
        "# Print the vocabulary\n",
        "print(vectorizer.get_feature_names_out())"
      ],
      "metadata": {
        "id": "9ditSBFWoJL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transforming text to numbers with BoW\n",
        "Now that you've built a vocabulary from the customer reviews, you're ready to transform each review into a numerical format using the Bag-of-Words (BoW) model. This step creates a structured matrix where each row represents a review and each column corresponds to a word from the vocabulary.\n",
        "\n",
        "The cleaned_reviews list and the fitted vectorizer are pre-loaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Transform the cleaned_reviews into a bow_matrix.\n",
        "Print the BoW representation as a NumPy array."
      ],
      "metadata": {
        "id": "7_uZIttEoUFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the reviews\n",
        "bow_matrix = vectorizer.transform(cleaned_reviews)\n",
        "\n",
        "# Print the BoW representation\n",
        "print(bow_matrix.toarray())"
      ],
      "metadata": {
        "id": "NlW1myF_oUQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frequency analysis of product reviews\n",
        "You now have access to a larger dataset of TechZone product reviews. Just like before, you've preprocessed and transformed the reviews into a BoW representation X. Your task now is to analyze the word frequencies and identify the most common terms in the dataset.\n",
        "\n",
        "To help with the analysis, a helper function called get_top_ten() is provided. It takes in a list of words and their corresponding counts, and returns the 10 most frequent words and their counts.\n",
        "\n",
        "Instructions 1/2\n",
        "0 XP\n",
        "1\n",
        "2\n",
        "Derive word_counts, the total count for each word across all reviews.\n",
        "Retrieve the list of unique words learned by the vectorizer."
      ],
      "metadata": {
        "id": "Rok8KkUDoYWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in string.punctuation]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "cleaned_reviews = [preprocess(review) for review in product_reviews]\n",
        "X = vectorizer.fit_transform(cleaned_reviews)\n",
        "\n",
        "# Get word counts\n",
        "word_counts = np.sum(X.toarray(), axis=0)\n",
        "# Get words\n",
        "words = vectorizer.get_feature_names_out()\n",
        "\n",
        "top_words_with_stopwords, top_counts_with_stopwords = get_top_ten(words, word_counts)\n",
        "print(top_words_with_stopwords, top_counts_with_stopwords)"
      ],
      "metadata": {
        "id": "dfaDnKBroY3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "0 XP\n",
        "2\n",
        "Modify the preprocess() function to remove stop_words as well.\n",
        "Run the code to observe the new outcomes."
      ],
      "metadata": {
        "id": "MGpxY_r1odNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify the function to remove stop words\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in string.punctuation]\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "cleaned_reviews = [preprocess(review) for review in product_reviews]\n",
        "X = vectorizer.fit_transform(cleaned_reviews)\n",
        "\n",
        "# Get word counts\n",
        "word_counts = np.sum(X.toarray(), axis=0)\n",
        "# Get words\n",
        "words = vectorizer.get_feature_names_out()\n",
        "\n",
        "top_words_without_stopwords, top_counts_without_stopwords = get_top_ten(words, word_counts)\n",
        "print(top_words_without_stopwords, top_counts_without_stopwords)"
      ],
      "metadata": {
        "id": "IAAsWQV0odc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing word frequencies\n",
        "Now that you've computed the most frequent words with and without stop words, it's time to visualize the differences. In this exercise, you'll use matplotlib to plot bar charts for both cases.\n",
        "\n",
        "The following lists have been pre-loaded for you: top_words_without_stopwords, top_counts_without_stopwords, top_words_with_stopwords, top_counts_with_stopwords.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Use plt.bar() to plot the top 10 word frequencies with stop words.\n",
        "Use plt.bar() to plot the top 10 word frequencies without stop words."
      ],
      "metadata": {
        "id": "tg-4xd9KonjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Plot the frequencies with stop words\n",
        "plt.bar(top_words_with_stopwords, top_counts_with_stopwords)\n",
        "plt.title(\"Top 10 word frequencies (with stop words)\")\n",
        "plt.xlabel(\"Words\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "\n",
        "# Plot the frequencies without stop words\n",
        "plt.figure()\n",
        "plt.bar(top_words_without_stopwords, top_counts_without_stopwords)\n",
        "plt.title(\"Top 10 word frequencies (without stop words)\")\n",
        "plt.xlabel(\"Words\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_AS_b-IEon1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF representation of product feedback\n",
        "You're working with a customer support team at a smart home company. They've collected user feedback on a range of smart devices and want to identify which words stand out in each review. You suggest using the TF-IDF technique to highlight the most relevant terms across feedback entries. Let's help them get started!\n",
        "\n",
        "A preprocess() function that receives a text and returns a processed one is pre-loaded for you. This function applies lowercasing, tokenization, and punctuation removal. Pandas has been imported as pd, and the TfidfVectorizer class is ready to use.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Initialize a TF-IDF vectorizer.\n",
        "Transform the cleaned reviews into a tfidf_matrix.\n",
        "Create a DataFrame df for the tfidf_matrix, having the vocabulary words as columns."
      ],
      "metadata": {
        "id": "r18i4bU-ppB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = [\"The smart speaker is incredible. Clear sound and fast responses!\",\n",
        "           \"I am disappointed with the smart bulb. It stopped working in a week.\",\n",
        "           \"The thermostat is okay. Not too smart, but functional.\"]\n",
        "cleaned_reviews = [preprocess(review) for review in reviews]\n",
        "\n",
        "# Initialize the vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "# Transform the cleaned reviews\n",
        "tfidf_matrix = vectorizer.fit_transform(cleaned_reviews)\n",
        "# Create a DataFrame for TF-IDF\n",
        "df = pd.DataFrame(\n",
        "  tfidf_matrix.toarray(),\n",
        "  columns=vectorizer.get_feature_names_out()\n",
        ")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "UmvhvU61ppRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing BoW and TF-IDF representations\n",
        "You're part of the analytics team at a wearable tech company. Your goal is to help product managers understand customer feedback on the company's new smartwatch. You've already preprocessed the text and created two representations: bow_matrix using CountVectorizer(), and tfidf_matrix using TfidfVectorizer(). In this exercise, you'll visualize and compare the two to better understand how each captures word importance.\n",
        "\n",
        "Instructions 1/2\n",
        "0 XP\n",
        "1\n",
        "Create a DataFrame using the provided bow_matrix, and plot a heatmap to visualize word frequencies for each review."
      ],
      "metadata": {
        "id": "819EC0X9ps9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert BoW matrix to a DataFrame\n",
        "df_bow = pd.DataFrame(\n",
        "    bow_matrix.toarray(),\n",
        "    columns=vectorizer.get_feature_names_out()\n",
        ")\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df_bow, annot=True)\n",
        "plt.title(\"BoW Scores Across Reviews\")\n",
        "plt.xlabel(\"Terms\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel(\"Documents\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0vfcDR7WptlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "\n",
        "Create a DataFrame from the tfidf_matrix, and plot a heatmap to visualize the TF-IDF scores."
      ],
      "metadata": {
        "id": "lRjk7Wvmp3ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert TF-IDF matrix to a DataFrame\n",
        "df_tfidf = pd.DataFrame(\n",
        "    tfidf_matrix.toarray(),\n",
        "    columns=vectorizer.get_feature_names_out()\n",
        ")\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df_tfidf, annot=True)\n",
        "plt.title(\"TF-IDF Scores Across Reviews\")\n",
        "plt.xlabel(\"Terms\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel(\"Documents\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bIdAVT9Up40q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring word relationships with embeddings\n",
        "Word embeddings capture the meanings of words based on their usage in large text datasets. By placing similar words closer together in a continuous vector space, they allow models to recognize context and semantic relationships that more basic methods can't capture. Now You'll work with embeddings to explore these kinds of word relationships firsthand.\n",
        "\n",
        "The glove-wiki-gigaword-50 word embedding model has been successfully loaded and is ready for use through the variable model_glove_wiki.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Compute the similarity score between \"king\" and \"queen\".\n",
        "Get the top 10 most similar words to \"computer\"."
      ],
      "metadata": {
        "id": "w6Wbw2XyqwpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute similarity between \"king\" and \"queen\"\n",
        "similarity_score = model_glove_wiki.similarity(\"king\", \"queen\")\n",
        "\n",
        "print(similarity_score)\n",
        "\n",
        "# Get top 10 most similar words to \"computer\"\n",
        "similar_words = model_glove_wiki.most_similar(\"computer\", topn=10)\n",
        "\n",
        "print(similar_words)"
      ],
      "metadata": {
        "id": "m5cAZ-3Nqw2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing and comparing word embeddings\n",
        "Word embeddings are high-dimensional, making them hard to interpret directly. In this exercise, you'll project a few word vectors down to 2D using Principal Component Analysis (PCA) and visualize them. This helps reveal semantic groupings or similarities between words in the embedding space. Then, you will compare the embedding representations of two models: glove-wiki-gigaword-50 available through the variable model_glove_wiki, and glove-twitter-25 available through model_glove_twitter.\n",
        "\n",
        "Instructions 1/2\n",
        "0 XP\n",
        "1\n",
        "Extract the embeddings of each word using model_glove_wiki and reduce the dimensions with PCA."
      ],
      "metadata": {
        "id": "n2lWYL4LrDOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"lion\", \"tiger\", \"leopard\", \"banana\", \"strawberry\", \"truck\", \"car\", \"bus\"]\n",
        "\n",
        "# Extract word embeddings\n",
        "word_vectors = [model_glove_wiki[word] for word in words]\n",
        "\n",
        "# Reduce dimensions with PCA\n",
        "pca = PCA(n_components=2)\n",
        "word_vectors_2d = pca.fit_transform(word_vectors)\n",
        "\n",
        "plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1])\n",
        "for word, (x, y) in zip(words, word_vectors_2d):\n",
        "    plt.annotate(word, (x, y))\n",
        "plt.title(\"GloVe Wikipedia Word Embeddings (2D PCA)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DoZ41kZLrEBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "\n",
        "\n",
        "Extract the embeddings of each word using model_glove_twitter."
      ],
      "metadata": {
        "id": "AL-Hn2j5rJWs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"lion\", \"tiger\", \"leopard\", \"banana\", \"strawberry\", \"truck\", \"car\", \"bus\"]\n",
        "\n",
        "# Change the embedding model\n",
        "word_vectors = [model_glove_twitter[word] for word in words]\n",
        "\n",
        "# Reduce dimensions with PCA\n",
        "pca = PCA(n_components=2)\n",
        "word_vectors_2d = pca.fit_transform(word_vectors)\n",
        "\n",
        "plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1])\n",
        "for word, (x, y) in zip(words, word_vectors_2d):\n",
        "    plt.annotate(word, (x, y))\n",
        "plt.title(\"GloVe Twitter Word Embeddings (2D PCA)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-XjmAM3qrKaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyzing the sentiment of a review\n",
        "Your team is building a tool to monitor customer sentiment in product reviews. As a first step, you're testing the sentiment of individual reviews using a pre-trained pipeline.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Initialize a pipeline for sentiment-analysis with the \"distilbert-base-uncased-finetuned-sst-2-english\" model.\n",
        "Use the pipeline to classify the sentiment of review_text."
      ],
      "metadata": {
        "id": "gOUlW9_7sV70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Define the sentiment analysis pipeline\n",
        "classifier = pipeline(task=\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "review_text = \"The new update made the app much faster and easier to use!\"\n",
        "\n",
        "# Get sentiment prediction\n",
        "result = classifier(review_text)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "IGM2fYSbsWVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch classifying multiple reviews\n",
        "Your sentiment analysis pipeline works well on one review. Now it's time to handle multiple reviews in one batch. This is a key step before analyzing user feedback at scale.\n",
        "\n",
        "Instructions\n",
        "0 XP\n",
        "Initialize a pipeline for sentiment-analysis using \"distilbert-base-uncased-finetuned-sst-2-english\".\n",
        "Use the pipeline to classify all reviews in the review_batch list."
      ],
      "metadata": {
        "id": "2GNW1jiXslk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(task=\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "review_batch = [\n",
        "    \"Absolutely love the new design!\",\n",
        "    \"The app crashes every time I open it.\",\n",
        "    \"Customer support was helpful and quick.\",\n",
        "    \"Too many ads make it unusable.\",\n",
        "    \"Everything works fine, but it’s a bit slow.\"\n",
        "]\n",
        "\n",
        "# Classify sentiments\n",
        "results = classifier(review_batch)\n",
        "print(results)"
      ],
      "metadata": {
        "id": "2ZsiTz8bslzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing models on labeled review data\n",
        "Now that you can classify sentiment in bulk, your team wants to evaluate which model is more reliable. You'll compare two models using a larger labeled dataset of reviews and measure their accuracy.\n",
        "\n",
        "A texts list and its true_labels are pre-loaded for you.\n",
        "\n",
        "Instructions 1/2\n",
        "35 XP\n",
        "1\n",
        "2\n",
        "Load two pipelines (pipe_a and pipe_a) using the models \"distilbert-base-uncased-finetuned-sst-2-english\" and \"abilfad/sentiment-binary-dicoding\", respectively.\n",
        "Extract the 'label' values from the predictions of both pipelines."
      ],
      "metadata": {
        "id": "QR7ETFKjstCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Load sentiment analysis models\n",
        "pipe_a = pipeline(task=\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "pipe_b = pipeline(task=\"sentiment-analysis\", model=\"abilfad/sentiment-binary-dicoding\")\n",
        "\n",
        "# Generate predictions\n",
        "preds_a = [res[\"label\"] for res in pipe_a(texts)]\n",
        "preds_b = [res[\"label\"] for res in pipe_b(texts)]"
      ],
      "metadata": {
        "id": "YHS811fUstUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "50 XP\n",
        "2\n",
        "Compute the accuracies of both models (acc_a and acc_b).\n"
      ],
      "metadata": {
        "id": "FXsfjVyEs0az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Load sentiment analysis models\n",
        "pipe_a = pipeline(task=\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "pipe_b = pipeline(task=\"sentiment-analysis\", model=\"abilfad/sentiment-binary-dicoding\")\n",
        "\n",
        "# Generate predictions\n",
        "preds_a = [res[\"label\"] for res in pipe_a(texts)]\n",
        "preds_b = [res[\"label\"] for res in pipe_b(texts)]\n",
        "\n",
        "# Evaluate accuracies\n",
        "acc_a = accuracy_score(true_labels, preds_a)\n",
        "acc_b = accuracy_score(true_labels, preds_b)\n",
        "print(f\"Accuracy - Model A: {acc_a:.2f}\")\n",
        "print(f\"Accuracy - Model B: {acc_b:.2f}\")"
      ],
      "metadata": {
        "id": "3PJnPL4ds0xD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero-shot classification of support tickets\n",
        "A company receives hundreds of support tickets daily, covering topics like billing issues, technical problems, and account management. Manually sorting these tickets is inefficient. You've been asked to use a zero-shot classification model to automatically categorize incoming ticket messages without needing a custom-trained classifier.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a zero-shot classifier pipeline using the \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\" model.\n",
        "Use it to classify the ticket_text into one of the categories listed in candidate_labels."
      ],
      "metadata": {
        "id": "0RQe0iDAthgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the zero-shot classifier\n",
        "classifier = pipeline(task=\"zero-shot-classification\", model=\"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\")\n",
        "\n",
        "ticket_text = \"I was charged twice for my subscription this month. Can you please refund the extra charge?\"\n",
        "candidate_labels = [\"Billing\", \"Technical Issue\", \"Account Access\"]\n",
        "\n",
        "# Classify the ticket\n",
        "result = classifier(ticket_text, candidate_labels)\n",
        "\n",
        "print(result['labels'])\n",
        "print(result['scores'])"
      ],
      "metadata": {
        "id": "_DRMWJHXthy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Does the text answer the question?\n",
        "A content moderation team in a large tech company needs to automatically validate whether a passage from a knowledge base answers a customer query. They want to speed up the process using a pre-trained QNLI model to assess the relevance of each response. Your goal is to implement a solution that can classify whether a given passage contains the answer to a specific question.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Initialize a classifier pipeline with a suitable QNLI model, such as \"cross-encoder/qnli-electra-base\".\n",
        "Use this pipeline to evaluate whether the given passage answers the question."
      ],
      "metadata": {
        "id": "SR21p5HRtrfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the QNLI pipeline\n",
        "classifier = pipeline(task=\"text-classification\", model=\"cross-encoder/qnli-electra-base\")\n",
        "\n",
        "passage = \"Our refund policy allows customers to return any item within 30 days of purchase, provided the item is in its original condition and accompanied by the receipt. Refunds are issued to the original payment method within 5–7 business days.\"\n",
        "question = \"Can I get a refund if I return a product after 20 days?\"\n",
        "\n",
        "# Get the result\n",
        "result = classifier({\n",
        "    \"text\": question,\n",
        "    \"text_pair\": passage\n",
        "})\n",
        "print(result)"
      ],
      "metadata": {
        "id": "EK8DDo99truQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detecting duplicate questions\n",
        "A startup is developing a Q&A assistant to improve the user experience on their support forum. One key feature is to detect when users ask the same question using different words. You've been asked to implement a solution using a pre-trained QQP model that can determine whether two questions are duplicates.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Initialize a suitable classifier pipeline with the \"textattack/bert-base-uncased-QQP\" model.\n",
        "Use the pipeline to classify whether question_1 and question_2 are paraphrases."
      ],
      "metadata": {
        "id": "TtnTblkYuflP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the pipeline\n",
        "classifier = pipeline(task=\"text-classification\", model=\"textattack/distilbert-base-uncased-QQP\")\n",
        "\n",
        "question_1 = \"What's the process to change my password?\"\n",
        "question_2 = \"How do I reset my account password?\"\n",
        "\n",
        "# Detect if the two questions are paraphrases\n",
        "result = classifier({\n",
        "    \"text\": question_1,\n",
        "    \"text_pair\": question_2\n",
        "})\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "NF8-PxSRuguR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking grammatical correctness\n",
        "An educational app is being built to help users improve their grammar. One core feature automatically checks whether user-submitted sentences are grammatically acceptable. You've been asked to implement this feature using a model trained on the Corpus of Linguistic Acceptability (CoLA) to classify sentence correctness.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Initialize a classifier pipeline with the \"textattack/bert-base-uncased-CoLA\" model.\n",
        "Use the pipeline to check if the user_text is grammatically acceptable."
      ],
      "metadata": {
        "id": "DZ-ouWYquvkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the pipeline\n",
        "classifier = pipeline(task=\"text-classification\", model=\"textattack/bert-base-uncased-CoLA\")\n",
        "\n",
        "user_text = \"Although she was knowing the answer, she didn't raised her hand during the class discussion.\"\n",
        "\n",
        "# Classify grammatical acceptability\n",
        "result = classifier(user_text)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "PQ-d3cEBuv2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Identifying named entities in news headlines\n",
        "News organizations often tag named entities like people, locations, and organizations in headlines to improve search, indexing, and recommendations. Your job is to use a Hugging Face pipeline to automatically detect and group these entities in a news headline.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a ner_pipeline using the \"dslim/bert-base-NER\" model.\n",
        "Extract the named entities from the given headline."
      ],
      "metadata": {
        "id": "W0VY96v2v4c5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "# Create the NER pipeline\n",
        "ner_pipeline = pipeline(\n",
        "    task=\"ner\",\n",
        "    model=\"dslim/bert-base-NER\",\n",
        "    grouped_entities=True\n",
        ")\n",
        "headline = \"Apple is planning to open a new office in San Francisco next year.\"\n",
        "\n",
        "# Get named entities\n",
        "entities = ner_pipeline(headline)\n",
        "\n",
        "for entity in entities:\n",
        "    print(f\"{entity['entity_group']}: {entity['word']}\")"
      ],
      "metadata": {
        "id": "VS9VfttIv4rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part of Speech tagging for text analysis\n",
        "A language learning app wants to help users understand sentence structure by highlighting the grammatical role of each word. Your task is to use a Hugging Face pipeline to label each word in a given sentence with its corresponding PoS tag.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a pos_pipeline using the \"vblagoje/bert-english-uncased-finetuned-pos\" model.\n",
        "Apply the pipeline on the provided sentence."
      ],
      "metadata": {
        "id": "FLsZFaMcwj0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "# Create the PoS tagging pipeline\n",
        "pos_pipeline = pipeline(\n",
        "    task=\"token-classification\",\n",
        "    model=\"vblagoje/bert-english-uncased-finetuned-pos\",\n",
        "    grouped_entities=True\n",
        ")\n",
        "\n",
        "sentence = \"I am meeting my friends for coffee this afternoon.\"\n",
        "\n",
        "# Get PoS tags\n",
        "pos_tags = pos_pipeline(sentence)\n",
        "for token in pos_tags:\n",
        "    print(f\"{token['word']}: {token['entity_group']}\")"
      ],
      "metadata": {
        "id": "Q3oCGi2qwkLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answering questions from product descriptions\n",
        "An online retailer wants to improve its customer support by automatically answering common questions about products using their descriptions. Your task is to use a Hugging Face pipeline to extract precise answers from a product description based on customer queries.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a qa_pipeline using the \"distilbert/distilbert-base-cased-distilled-squad\" model for question answering.\n",
        "Use the provided context (product description) and question to get an answer."
      ],
      "metadata": {
        "id": "qfRVzA6PxNFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create the question-answering pipeline\n",
        "qa_pipeline = pipeline(\n",
        "    task=\"question-answering\",\n",
        "    model=\"distilbert/distilbert-base-cased-distilled-squad\"\n",
        ")\n",
        "\n",
        "context = \"\"\"This smartphone features a 6.5-inch OLED display, 128GB of storage, and a 48MP camera with night mode. It supports 5G connectivity and has a battery life of up to 24 hours.\"\"\"\n",
        "\n",
        "question = \"What is the size of the smartphone's display?\"\n",
        "\n",
        "# Get the answer\n",
        "result = qa_pipeline(question=question, context=context)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "xUrlgsUYxNYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating natural answers with abstractive QA\n",
        "Customer support chatbots aim to provide helpful, conversational answers, not just exact text snippets. To achieve this, they use abstractive question answering, which generates concise and fluent responses based on the context. Your task is to apply Hugging Face's \"text2text-generation\" pipeline with a model trained for abstractive QA to create natural answers from product information.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a qa_pipeline using the \"fangyuan/hotpotqa_abstractive\" model with the \"text2text-generation\" task.\n",
        "Use the provided context and question to generate an abstractive answer."
      ],
      "metadata": {
        "id": "ZDg-a027xZwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create the abstractive question-answering pipeline\n",
        "qa_pipeline = pipeline(\n",
        "    task=\"text2text-generation\",\n",
        "    model=\"fangyuan/hotpotqa_abstractive\"\n",
        ")\n",
        "\n",
        "context = \"\"\"This smartphone features a 6.5-inch OLED display, 128GB of storage, and a 48MP camera with night mode. It supports 5G connectivity and has a battery life of up to 24 hours.\"\"\"\n",
        "\n",
        "question = \"What is the size of the smartphone's display?\"\n",
        "\n",
        "# Generate abstractive answer\n",
        "result = qa_pipeline(f\"question: {question} context: {context}\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "Dom35MXtxaEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summarizing news articles for quick insights\n",
        "A news aggregation app needs to provide users with concise summaries of lengthy news articles so they can quickly grasp the main points without reading the entire text. Your task is to use a Hugging Face pipeline to generate clear, brief summaries from full news articles.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a summarizer pipeline using the \"cnicu/t5-small-booksum\" model.\n",
        "Summarize the given article text."
      ],
      "metadata": {
        "id": "I_1eG68kyRD1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create the summarization pipeline\n",
        "summarizer = pipeline(task=\"summarization\", model=\"cnicu/t5-small-booksum\")\n",
        "\n",
        "article = \"\"\"NASA's Perseverance rover has successfully collected its first rock samples from Mars, marking a significant milestone in the mission. The samples will be stored for potential return to Earth in the future, providing valuable insight into the planet's geology and potential signs of past microbial life.\"\"\"\n",
        "\n",
        "# Generate the summary\n",
        "summary = summarizer(article)\n",
        "\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "5XZvEouGyRT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translating customer reviews to French\n",
        "A global e-commerce company is analyzing customer feedback from its English-speaking users. To support its French customer support team, the company needs to translate these reviews into French. As part of the NLP pipeline, your role is to automate the translation of reviews using the Hugging Face pipeline.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a translator pipeline using the \"Helsinki-NLP/opus-mt-en-fr\" model.\n",
        "Translate the provided review from English to French."
      ],
      "metadata": {
        "id": "TP0e7yiWyZS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create the translation pipeline\n",
        "translator = pipeline(task=\"translation\", model=\"Helsinki-NLP/opus-mt-en-fr\")\n",
        "\n",
        "review = \"The hotel was clean and the staff were very friendly.\"\n",
        "\n",
        "# Translate the review\n",
        "translation = translator(review)\n",
        "\n",
        "print(translation)"
      ],
      "metadata": {
        "id": "e4k2-JoDyZij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a search completion system\n",
        "Search completion, or auto-complete, is a common NLP application used in search engines and messaging apps. The goal is to suggest possible completions based on a user's partial input. Your task is to use Hugging Face's \"text-generation\" pipeline to implement a basic auto-complete system that generates relevant completions from the user's query.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create an autocomplete pipeline with the \"distilgpt2\" model.\n",
        "Generate five search query suggestions for the given prompt, limiting each to a maximum of eight tokens."
      ],
      "metadata": {
        "id": "s3HXtnieygz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create the pipeline\n",
        "autocomplete = pipeline(task=\"text-generation\", model=\"distilgpt2\")\n",
        "\n",
        "prompt = \"Best books to read for\"\n",
        "\n",
        "# Generate search query completions\n",
        "suggestions = autocomplete(prompt, max_length=8, num_return_sequences=5)\n",
        "\n",
        "for suggestion in suggestions:\n",
        "    print(suggestion['generated_text'])"
      ],
      "metadata": {
        "id": "dtytVxIdyhEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Module End ---"
      ],
      "metadata": {
        "id": "WRGsitWMywFb"
      }
    }
  ]
}