{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbhtcUr59v73BzZDiNvp4w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pattiecodes/DC_MLScientist_in_Py/blob/main/Module_8_Preprocessing_for_ML_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Module Start ---"
      ],
      "metadata": {
        "id": "0vA4LOQYyv67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring missing data\n",
        "You've been given a dataset comprised of volunteer information from New York City, stored in the volunteer DataFrame. Explore the dataset using the plethora of methods and attributes pandas has to offer to answer the following question.\n",
        "\n",
        "How many missing values are in the locality column?\n",
        "\n",
        "Instructions\n",
        "50 XP\n",
        "Possible answers\n",
        "\n",
        "\n",
        "665\n",
        "\n",
        "595\n",
        "\n",
        "**70**\n",
        "\n",
        "35"
      ],
      "metadata": {
        "id": "jbQ7L9X54mLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dropping missing data\n",
        "Now that you've explored the volunteer dataset and understand its structure and contents, it's time to begin dropping missing values.\n",
        "\n",
        "In this exercise, you'll drop both columns and rows to create a subset of the volunteer dataset.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Drop the Latitude and Longitude columns from volunteer, storing as volunteer_cols.\n",
        "Subset volunteer_cols by dropping rows containing missing values in the category_desc, and store in a new variable called volunteer_subset.\n",
        "Take a look at the .shape attribute of volunteer_subset, to verify it worked correctly."
      ],
      "metadata": {
        "id": "OaNFOozt5frC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_5DrNDyyoEY"
      },
      "outputs": [],
      "source": [
        "# Drop the Latitude and Longitude columns from volunteer\n",
        "volunteer_cols = volunteer.drop([\"Latitude\", \"Longitude\"], axis=1)\n",
        "\n",
        "# Drop rows with missing category_desc values from volunteer_cols\n",
        "volunteer_subset = volunteer_cols.dropna(subset=[\"category_desc\"])\n",
        "\n",
        "# Print out the shape of the subset\n",
        "print(volunteer_subset.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting a column type\n",
        "If you take a look at the volunteer dataset types, you'll see that the column hits is type object. But, if you actually look at the column, you'll see that it consists of integers. Let's convert that column to type int.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Take a look at the .head() of the hits column.\n",
        "Convert the hits column to type int.\n",
        "Take a look at the .dtypes of the dataset again, and notice that the column type has changed."
      ],
      "metadata": {
        "id": "vmo6kv71_ZuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the head of the hits column\n",
        "print(volunteer[\"hits\"].head())\n",
        "\n",
        "# Convert the hits column to type int\n",
        "volunteer[\"hits\"] = volunteer[\"hits\"].astype(\"int\")\n",
        "\n",
        "# Look at the dtypes of the dataset\n",
        "print(volunteer.dtypes)"
      ],
      "metadata": {
        "id": "tXMaZ8PC_bkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stratified sampling\n",
        "You now know that the distribution of class labels in the category_desc column of the volunteer dataset is uneven. If you wanted to train a model to predict category_desc, you'll need to ensure that the model is trained on a sample of data that is representative of the entire dataset. Stratified sampling is a way to achieve this!\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a DataFrame of features, X, with all of the columns except category_desc.\n",
        "Create a DataFrame of labels, y from the category_desc column.\n",
        "Split X and y into training and test sets, ensuring that the class distribution in the labels is the same in both sets\n",
        "Print the labels and counts in y_train using .value_counts()."
      ],
      "metadata": {
        "id": "1449RVKE-TyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame with all columns except category_desc\n",
        "X = volunteer.drop(\"category_desc\", axis=1)\n",
        "\n",
        "# Create a category_desc labels dataset\n",
        "y = volunteer[[\"category_desc\"]]\n",
        "\n",
        "# Use stratified sampling to split up the dataset according to the y dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
        "\n",
        "# Print the category_desc counts from y_train\n",
        "print(y_train[\"category_desc\"].value_counts())"
      ],
      "metadata": {
        "id": "0dmP9cRq-UYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking the variance\n",
        "Check the variance of the columns in the wine dataset. Out of the four columns listed, which column is the most appropriate candidate for normalization?\n",
        "\n",
        "Instructions\n",
        "50 XP\n",
        "Possible answers\n",
        "\n",
        "\n",
        "Alcohol\n",
        "\n",
        "**Proline**\n",
        "\n",
        "Proanthocyanins\n",
        "\n",
        "Ash"
      ],
      "metadata": {
        "id": "_JDbgGUTYfSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(wine.var())"
      ],
      "metadata": {
        "id": "gWF8T_NWYfo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Log normalization in Python\n",
        "Now that we know that the Proline column in our wine dataset has a large amount of variance, let's log normalize it.\n",
        "\n",
        "numpy has been imported as np.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Print out the variance of the Proline column for reference.\n",
        "Use the np.log() function on the Proline column to create a new, log-normalized column named Proline_log.\n",
        "Print out the variance of the Proline_log column to see the difference."
      ],
      "metadata": {
        "id": "MdprSm9XYkp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out the variance of the Proline column\n",
        "print(wine[\"Proline\"].var())\n",
        "\n",
        "# Apply the log normalization function to the Proline column\n",
        "wine[\"Proline_log\"] = np.log(wine[\"Proline\"])\n",
        "\n",
        "# Check the variance of the normalized Proline column\n",
        "print(wine[\"Proline_log\"].var())"
      ],
      "metadata": {
        "id": "0Qj_N8Y_ZiE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scaling data - standardizing columns\n",
        "Since we know that the Ash, Alcalinity of ash, and Magnesium columns in the wine dataset are all on different scales, let's standardize them in a way that allows for use in a linear model.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import the StandardScaler class.\n",
        "Instantiate a StandardScaler() and store it in the variable, scaler.\n",
        "Create a subset of the wine DataFrame containing the Ash, Alcalinity of ash, and Magnesium columns, assign it to wine_subset.\n",
        "Fit and transform the standard scaler to wine_subset."
      ],
      "metadata": {
        "id": "AqV-xifNVwod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import StandardScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Subset the DataFrame you want to scale\n",
        "wine_subset = wine[[\"Ash\", \"Alcalinity of ash\", \"Magnesium\"]]\n",
        "\n",
        "# Apply the scaler to wine_subset\n",
        "wine_subset_scaled = scaler.fit_transform(wine_subset)"
      ],
      "metadata": {
        "id": "9959Rz_rVxZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN on non-scaled data\n",
        "Before adding standardization to your scikit-learn workflow, you'll first take a look at the accuracy of a K-nearest neighbors model on the wine dataset without standardizing the data.\n",
        "\n",
        "The knn model as well as the X and y data and labels sets have been created already.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Split the dataset into training and test sets.\n",
        "Fit the knn model to the training data.\n",
        "Print out the test set accuracy of your trained knn model."
      ],
      "metadata": {
        "id": "qphhj_vEYB7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset and labels into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
        "\n",
        "# Fit the k-nearest neighbors model to the training data\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Score the model on the test data\n",
        "print(knn.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "DgXIl_A6YCT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN on scaled data\n",
        "The accuracy score on the unscaled wine dataset was decent, but let's see what you can achieve by using standardization. Once again, the knn model as well as the X and y data and labels set have already been created for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create the StandardScaler() method, stored in a variable named scaler.\n",
        "Scale the training and test features, being careful not to introduce data leakage.\n",
        "Fit the knn model to the scaled training data.\n",
        "Evaluate the model's performance by computing the test set accuracy."
      ],
      "metadata": {
        "id": "PXF-na_O6IIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
        "\n",
        "# Instantiate a StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Scale the training and test features\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Fit the k-nearest neighbors model to the training data\n",
        "knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Score the model on the test data\n",
        "print(knn.score(X_test_scaled, y_test))"
      ],
      "metadata": {
        "id": "-3uh-yas6JKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding categorical variables - binary\n",
        "Take a look at the hiking dataset. There are several columns here that need encoding before they can be modeled, one of which is the Accessible column. Accessible is a binary feature, so it has two values, Y or N, which need to be encoded into 1's and 0's. Use scikit-learn's LabelEncoder method to perform this transformation.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Store LabelEncoder() in a variable named enc.\n",
        "Using the encoder's .fit_transform() method, encode the hiking dataset's \"Accessible\" column. Call the new column Accessible_enc.\n",
        "Compare the two columns side-by-side to see the encoding."
      ],
      "metadata": {
        "id": "w5jUbgSwawIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the LabelEncoder object\n",
        "enc = LabelEncoder()\n",
        "\n",
        "# Apply the encoding to the \"Accessible\" column\n",
        "hiking[\"Accessible_enc\"] = enc.fit_transform(hiking[\"Accessible\"])\n",
        "\n",
        "# Compare the two columns\n",
        "print(hiking[[\"Accessible\", \"Accessible_enc\"]].head())"
      ],
      "metadata": {
        "id": "Dle0cjbWawiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding categorical variables - one-hot\n",
        "One of the columns in the volunteer dataset, category_desc, gives category descriptions for the volunteer opportunities listed. Because it is a categorical variable with more than two categories, we need to use one-hot encoding to transform this column numerically. Use pandas' pd.get_dummies() function to do so.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Call get_dummies() on the volunteer[\"category_desc\"] column to create the encoded columns and assign it to category_enc.\n",
        "Print out the .head() of the category_enc variable to take a look at the encoded columns."
      ],
      "metadata": {
        "id": "qtDQZOHVbGBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the category_desc column\n",
        "category_enc = pd.get_dummies(volunteer[\"category_desc\"])\n",
        "\n",
        "# Take a look at the encoded columns\n",
        "print(category_enc.head())"
      ],
      "metadata": {
        "id": "fZIcCxRqbGVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aggregating numerical features\n",
        "A good use case for taking an aggregate statistic to create a new feature is when you have many features with similar, related values. Here, you have a DataFrame of running times named running_times_5k. For each name in the dataset, take the mean of their 5 run times.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Use the .loc[] method to select all rows and columns to find the .mean() of the each columns.\n",
        "Print the .head() of the DataFrame to see the mean column."
      ],
      "metadata": {
        "id": "kFU2ZyB_prqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use .loc to create a mean column\n",
        "running_times_5k[\"mean\"] = running_times_5k.loc[:, \"run1\":\"run5\"].mean(axis=1)\n",
        "\n",
        "# Take a look at the results\n",
        "print(running_times_5k.head())"
      ],
      "metadata": {
        "id": "XiQtC6hQpsg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting datetime components\n",
        "There are several columns in the volunteer dataset comprised of datetimes. Let's take a look at the start_date_date column and extract just the month to use as a feature for modeling.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Convert the start_date_date column into a pandas datetime column and store it in a new column called start_date_converted.\n",
        "Retrieve the month component of start_date_converted and store it in a new column called start_date_month.\n",
        "Print the .head() of just the start_date_converted and start_date_month columns."
      ],
      "metadata": {
        "id": "NMJehQpBqlxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, convert string column to date column\n",
        "volunteer[\"start_date_converted\"] = pd.to_datetime(volunteer[\"start_date_date\"])\n",
        "\n",
        "# Extract just the month from the converted column\n",
        "volunteer[\"start_date_month\"] = volunteer[\"start_date_converted\"].dt.month\n",
        "\n",
        "# Take a look at the converted and new month columns\n",
        "print(volunteer[[\"start_date_converted\", \"start_date_month\"]].head())"
      ],
      "metadata": {
        "id": "RGsAWJuyql__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting string patterns\n",
        "The Length column in the hiking dataset is a column of strings, but contained in the column is the mileage for the hike. We're going to extract this mileage using regular expressions, and then use a lambda in pandas to apply the extraction to the DataFrame.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Search the text in the length argument for numbers and decimals using an appropriate pattern.\n",
        "Extract the matched pattern and convert it to a float.\n",
        "Apply the return_mileage() function to each row in the hiking[\"Length\"] column."
      ],
      "metadata": {
        "id": "EbL8SpmQ-Qm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write a pattern to extract numbers and decimals\n",
        "def return_mileage(length):\n",
        "\n",
        "    # Search the text for matches\n",
        "    mile = re.search(r\"\\d+\\.\\d+\", length)\n",
        "\n",
        "    # If a value is returned, use group(0) to return the found value\n",
        "    if mile is not None:\n",
        "        return float(mile.group(0))\n",
        "\n",
        "# Apply the function to the Length column and take a look at both columns\n",
        "hiking[\"Length_num\"] = hiking[\"Length\"].apply(return_mileage)\n",
        "print(hiking[[\"Length\", \"Length_num\"]].head())"
      ],
      "metadata": {
        "id": "B6Txb40k-RAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorizing text\n",
        "You'll now transform the volunteer dataset's title column into a text vector, which you'll use in a prediction task in the next exercise.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Store the volunteer[\"title\"] column in a variable named title_text.\n",
        "Instantiate a TfidfVectorizer as tfidf_vec.\n",
        "Transform the text in title_text into a tf-idf vector using tfidf_vec."
      ],
      "metadata": {
        "id": "JSEy6a_Y_Oeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take the title text\n",
        "title_text = title_text = volunteer[\"title\"]\n",
        "\n",
        "# Create the vectorizer method\n",
        "tfidf_vec = TfidfVectorizer()\n",
        "\n",
        "# Transform the text into tf-idf vectors\n",
        "text_tfidf = tfidf_vec.fit_transform(title_text)"
      ],
      "metadata": {
        "id": "ACGpUhLL_Qh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text classification using tf/idf vectors\n",
        "Now that you've encoded the volunteer dataset's title column into tf/idf vectors, you'll use those vectors to predict the category_desc column.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Split the text_tfidf vector and y target variable into training and test sets, setting the stratify parameter equal to y, since the class distribution is uneven. Notice that we have to run the .toarray() method on the tf/idf vector, in order to get in it the proper format for scikit-learn.\n",
        "Fit the X_train and y_train data to the Naive Bayes model, nb.\n",
        "Print out the test set accuracy.\n"
      ],
      "metadata": {
        "id": "4XS08Xv96Bl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # Split the dataset according to the class distribution of category_desc\n",
        "y = volunteer[\"category_desc\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(text_tfidf.toarray(), y, stratify=y, random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Print out the model's accuracy\n",
        "print(nb.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "3g0Ez-U26CLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Selecting relevant features\n",
        "In this exercise, you'll identify the redundant columns in the volunteer dataset, and perform feature selection on the dataset to return a DataFrame of the relevant features.\n",
        "\n",
        "For example, if you explore the volunteer dataset in the console, you'll see three features which are related to location: locality, region, and postalcode. They contain related information, so it would make sense to keep only one of the features.\n",
        "\n",
        "Take some time to examine the features of volunteer in the console, and try to identify the redundant features.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a list of redundant column names and store it in the to_drop variable:\n",
        "Out of all the location-related features, keep only postalcode.\n",
        "Features that have gone through the feature engineering process are redundant as well.\n",
        "Drop the columns in the to_drop list from the dataset.\n",
        "Print out the .head() of volunteer_subset to see the selected columns."
      ],
      "metadata": {
        "id": "7dWFhKIr1oth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of redundant column names to drop\n",
        "to_drop = [\"locality\", \"region\", \"created_date\", \"category_desc\", \"vol_requests\"]\n",
        "\n",
        "# Drop those columns from the dataset\n",
        "volunteer_subset = volunteer.drop(to_drop, axis=1)\n",
        "\n",
        "# Print out the head of volunteer_subset\n",
        "print(volunteer_subset.head())"
      ],
      "metadata": {
        "id": "WcOMqd161pCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking for correlated features\n",
        "You'll now return to the wine dataset, which consists of continuous, numerical features. Run Pearson's correlation coefficient on the dataset to determine which columns are good candidates for eliminating. Then, remove those columns from the DataFrame.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Print out the Pearson correlation coefficients for each pair of features in the wine dataset.\n",
        "Drop any columns from wine that have a correlation coefficient above 0.75 with at least two other columns."
      ],
      "metadata": {
        "id": "Cedqo_QR2F86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out the column correlations of the wine dataset\n",
        "print(wine.corr())\n",
        "\n",
        "# Drop that column from the DataFrame\n",
        "wine = wine.drop(\"Flavanoids\", axis=1)\n",
        "\n",
        "print(wine.head())"
      ],
      "metadata": {
        "id": "87XdxFsx2GQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring text vectors, part 1\n",
        "Let's expand on the text vector exploration method we just learned about, using the volunteer dataset's title tf/idf vectors. In this first part of text vector exploration, we're going to add to that function we learned about in the slides. We'll return a list of numbers with the function. In the next exercise, we'll write another function to collect the top words across all documents, extract them, and then use that list to filter down our text_tfidf vector.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Add parameters called original_vocab, for the tfidf_vec.vocabulary_, and top_n.\n",
        "Call pd.Series() on the zipped dictionary. This will make it easier to operate on.\n",
        "Use the .sort_values() function to sort the series and slice the index up to top_n words.\n",
        "Call the function, setting original_vocab=tfidf_vec.vocabulary_, setting vector_index=8 to grab the 9th row, and setting top_n=3, to grab the top 3 weighted words."
      ],
      "metadata": {
        "id": "eB84gV-T3gSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add in the rest of the arguments\n",
        "def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
        "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
        "\n",
        "    # Transform that zipped dict into a series\n",
        "    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
        "\n",
        "    # Sort the series to pull out the top n weighted words\n",
        "    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
        "    return [original_vocab[i] for i in zipped_index]\n",
        "\n",
        "# Print out the weighted words\n",
        "print(return_weights(vocab, tfidf_vec.vocabulary_, text_tfidf, vector_index=8, top_n=3))"
      ],
      "metadata": {
        "id": "7W5MfBTb3gs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring text vectors, part 2\n",
        "Using the return_weights() function you wrote in the previous exercise, you're now going to extract the top words from each document in the text vector, return a list of the word indices, and use that list to filter the text vector down to those top words.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Call return_weights() to return the top weighted words for that document.\n",
        "Call set() on the returned filter_list to remove duplicated numbers.\n",
        "Call words_to_filter, passing in the following parameters: vocab for the vocab parameter, tfidf_vec.vocabulary_ for the original_vocab parameter, text_tfidf for the vector parameter, and 3 to grab the top_n 3 weighted words from each document.\n",
        "Finally, pass that filtered_words set into a list to use as a filter for the text vector."
      ],
      "metadata": {
        "id": "dHHAjjAY4AFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
        "    filter_list = []\n",
        "    for i in range(0, vector.shape[0]):\n",
        "\n",
        "        # Call the return_weights function and extend filter_list\n",
        "        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
        "        filter_list.extend(filtered)\n",
        "\n",
        "    # Return the list in a set, so we don't get duplicate word indices\n",
        "    return set(filter_list)\n",
        "\n",
        "# Call the function to get the list of word indices\n",
        "filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, 3)\n",
        "\n",
        "# Filter the columns in text_tfidf to only those in filtered_words\n",
        "filtered_text = text_tfidf[:, list(filtered_words)]"
      ],
      "metadata": {
        "id": "NeoOR4eV4AZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Naive Bayes with feature selection\n",
        "You'll now re-run the Naive Bayes text classification model that you ran at the end of Chapter 3 with our selection choices from the previous exercise: the volunteer dataset's title and category_desc columns.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Use train_test_split() on the filtered_text text vector, the y labels (which is the category_desc labels), and pass the y set to the stratify parameter, since we have an uneven class distribution.\n",
        "Fit the nb Naive Bayes model to X_train and y_train.\n",
        "Calculate the test set accuracy of nb."
      ],
      "metadata": {
        "id": "aYyD7_KEN3sl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset according to the class distribution of category_desc\n",
        "X_train, X_test, y_train, y_test = train_test_split(filtered_text.toarray(), y, stratify=y, random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Print out the model's accuracy\n",
        "print(nb.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "Xtt5qvtjN4FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using PCA\n",
        "In this exercise, you'll apply PCA to the wine dataset, to see if you can increase the model's accuracy.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Instantiate a PCA object.\n",
        "Define the features (X) and labels (y) from wine, using the labels in the \"Type\" column.\n",
        "Apply PCA to X_train and X_test, ensuring no data leakage, and store the transformed values as pca_X_train and pca_X_test.\n",
        "Print out the .explained_variance_ratio_ attribute of pca to check how much variance is explained by each component."
      ],
      "metadata": {
        "id": "4PGcGbNFRYVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate a PCA object\n",
        "pca = PCA()\n",
        "\n",
        "# Define the features and labels from the wine dataset\n",
        "X = wine.drop(\"Type\", axis=1)\n",
        "y = wine[\"Type\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
        "\n",
        "# Apply PCA to the wine dataset X vector\n",
        "pca_X_train = pca.fit_transform(X_train)\n",
        "pca_X_test = pca.transform(X_test)\n",
        "\n",
        "# Look at the percentage of variance explained by the different components\n",
        "print(pca.explained_variance_ratio_)"
      ],
      "metadata": {
        "id": "5a4kYfkGRYtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training a model with PCA\n",
        "Now that you have run PCA on the wine dataset, you'll finally train a KNN model using the transformed data.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Fit the knn model to the PCA-transformed features, pca_X_train, and training labels, y_train.\n",
        "Print the test set accuracy of the knn model using pca_X_test and y_test."
      ],
      "metadata": {
        "id": "b2FnMYk2SmpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit knn to the training data\n",
        "knn.fit(pca_X_train, y_train)\n",
        "\n",
        "# Score knn on the test data and print it out\n",
        "print(knn.score(pca_X_test, y_test))"
      ],
      "metadata": {
        "id": "UjC1-xkOSnUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking column types\n",
        "Take a look at the UFO dataset's column types using the .info() method. Two columns jump out for transformation: the seconds column, which is a numeric column but is being read in as object, and the date column, which can be transformed into the datetime type. That will make our feature engineering efforts easier later on.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Call the .info() method on the ufo dataset.\n",
        "Convert the type of the seconds column to the float data type.\n",
        "Convert the type of the date column to the datetime data type.\n",
        "Call .info() on ufo again to see if the changes worked."
      ],
      "metadata": {
        "id": "6vxhwunwUfDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the DataFrame info\n",
        "print(ufo.info())\n",
        "\n",
        "# Change the type of seconds to float\n",
        "ufo[\"seconds\"] = ufo[\"seconds\"].astype(\"float\")\n",
        "\n",
        "# Change the date column to type datetime\n",
        "ufo[\"date\"] = pd.to_datetime(ufo[\"date\"])\n",
        "\n",
        "# Check the column types\n",
        "print(ufo.info())"
      ],
      "metadata": {
        "id": "ACSW-lgFUfaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dropping missing data\n",
        "In this exercise, you'll remove some of the rows where certain columns have missing values. You're going to look at the length_of_time column, the state column, and the type column. You'll drop any row that contains a missing value in at least one of these three columns.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Print out the number of missing values in the length_of_time, state, and type columns, in that order, using .isna() and .sum().\n",
        "Drop rows that have missing values in at least one of these columns.\n",
        "Print out the shape of the new ufo_no_missing dataset."
      ],
      "metadata": {
        "id": "EjVv7MVTWqek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the missing values in the length_of_time, state, and type columns, in that order\n",
        "print(ufo[[\"length_of_time\", \"state\", \"type\"]].isna().sum())\n",
        "\n",
        "# Drop rows where length_of_time, state, or type are missing\n",
        "ufo_no_missing = ufo.dropna(subset=[\"length_of_time\", \"state\", \"type\"])\n",
        "\n",
        "# Print out the shape of the new dataset\n",
        "print(ufo_no_missing.shape)"
      ],
      "metadata": {
        "id": "KHgTVf_iWq_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting numbers from strings\n",
        "The length_of_time field in the UFO dataset is a text field that has the number of minutes within the string. Here, you'll extract that number from that text field using regular expressions.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Search time_string for numbers using an appropriate RegEx pattern.\n",
        "Use the .apply() method to call the return_minutes() on every row of the length_of_time column.\n",
        "Print out the .head() of both the length_of_time and minutes columns to compare."
      ],
      "metadata": {
        "id": "nKozhMnmYuJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def return_minutes(time_string):\n",
        "\n",
        "    # Search for numbers in time_string\n",
        "    num = re.search(\"\\d+\", time_string)\n",
        "    if num is not None:\n",
        "        return int(num.group(0))\n",
        "\n",
        "# Apply the extraction to the length_of_time column\n",
        "ufo[\"minutes\"] = ufo[\"length_of_time\"].apply(return_minutes)\n",
        "\n",
        "# Take a look at the head of both of the columns\n",
        "print(ufo[[\"length_of_time\", \"minutes\"]].head())"
      ],
      "metadata": {
        "id": "27MEpOVBYuiS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}