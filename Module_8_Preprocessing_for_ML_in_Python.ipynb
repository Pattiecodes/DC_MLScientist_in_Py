{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfw9I8WUHfJVji062HEVuY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pattiecodes/DC_MLScientist_in_Py/blob/main/Module_8_Preprocessing_for_ML_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Module Start ---"
      ],
      "metadata": {
        "id": "0vA4LOQYyv67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring missing data\n",
        "You've been given a dataset comprised of volunteer information from New York City, stored in the volunteer DataFrame. Explore the dataset using the plethora of methods and attributes pandas has to offer to answer the following question.\n",
        "\n",
        "How many missing values are in the locality column?\n",
        "\n",
        "Instructions\n",
        "50 XP\n",
        "Possible answers\n",
        "\n",
        "\n",
        "665\n",
        "\n",
        "595\n",
        "\n",
        "**70**\n",
        "\n",
        "35"
      ],
      "metadata": {
        "id": "jbQ7L9X54mLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dropping missing data\n",
        "Now that you've explored the volunteer dataset and understand its structure and contents, it's time to begin dropping missing values.\n",
        "\n",
        "In this exercise, you'll drop both columns and rows to create a subset of the volunteer dataset.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Drop the Latitude and Longitude columns from volunteer, storing as volunteer_cols.\n",
        "Subset volunteer_cols by dropping rows containing missing values in the category_desc, and store in a new variable called volunteer_subset.\n",
        "Take a look at the .shape attribute of volunteer_subset, to verify it worked correctly."
      ],
      "metadata": {
        "id": "OaNFOozt5frC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_5DrNDyyoEY"
      },
      "outputs": [],
      "source": [
        "# Drop the Latitude and Longitude columns from volunteer\n",
        "volunteer_cols = volunteer.drop([\"Latitude\", \"Longitude\"], axis=1)\n",
        "\n",
        "# Drop rows with missing category_desc values from volunteer_cols\n",
        "volunteer_subset = volunteer_cols.dropna(subset=[\"category_desc\"])\n",
        "\n",
        "# Print out the shape of the subset\n",
        "print(volunteer_subset.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting a column type\n",
        "If you take a look at the volunteer dataset types, you'll see that the column hits is type object. But, if you actually look at the column, you'll see that it consists of integers. Let's convert that column to type int.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Take a look at the .head() of the hits column.\n",
        "Convert the hits column to type int.\n",
        "Take a look at the .dtypes of the dataset again, and notice that the column type has changed."
      ],
      "metadata": {
        "id": "vmo6kv71_ZuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the head of the hits column\n",
        "print(volunteer[\"hits\"].head())\n",
        "\n",
        "# Convert the hits column to type int\n",
        "volunteer[\"hits\"] = volunteer[\"hits\"].astype(\"int\")\n",
        "\n",
        "# Look at the dtypes of the dataset\n",
        "print(volunteer.dtypes)"
      ],
      "metadata": {
        "id": "tXMaZ8PC_bkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stratified sampling\n",
        "You now know that the distribution of class labels in the category_desc column of the volunteer dataset is uneven. If you wanted to train a model to predict category_desc, you'll need to ensure that the model is trained on a sample of data that is representative of the entire dataset. Stratified sampling is a way to achieve this!\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a DataFrame of features, X, with all of the columns except category_desc.\n",
        "Create a DataFrame of labels, y from the category_desc column.\n",
        "Split X and y into training and test sets, ensuring that the class distribution in the labels is the same in both sets\n",
        "Print the labels and counts in y_train using .value_counts()."
      ],
      "metadata": {
        "id": "1449RVKE-TyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame with all columns except category_desc\n",
        "X = volunteer.drop(\"category_desc\", axis=1)\n",
        "\n",
        "# Create a category_desc labels dataset\n",
        "y = volunteer[[\"category_desc\"]]\n",
        "\n",
        "# Use stratified sampling to split up the dataset according to the y dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)\n",
        "\n",
        "# Print the category_desc counts from y_train\n",
        "print(y_train[\"category_desc\"].value_counts())"
      ],
      "metadata": {
        "id": "0dmP9cRq-UYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking the variance\n",
        "Check the variance of the columns in the wine dataset. Out of the four columns listed, which column is the most appropriate candidate for normalization?\n",
        "\n",
        "Instructions\n",
        "50 XP\n",
        "Possible answers\n",
        "\n",
        "\n",
        "Alcohol\n",
        "\n",
        "**Proline**\n",
        "\n",
        "Proanthocyanins\n",
        "\n",
        "Ash"
      ],
      "metadata": {
        "id": "_JDbgGUTYfSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(wine.var())"
      ],
      "metadata": {
        "id": "gWF8T_NWYfo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Log normalization in Python\n",
        "Now that we know that the Proline column in our wine dataset has a large amount of variance, let's log normalize it.\n",
        "\n",
        "numpy has been imported as np.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Print out the variance of the Proline column for reference.\n",
        "Use the np.log() function on the Proline column to create a new, log-normalized column named Proline_log.\n",
        "Print out the variance of the Proline_log column to see the difference."
      ],
      "metadata": {
        "id": "MdprSm9XYkp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out the variance of the Proline column\n",
        "print(wine[\"Proline\"].var())\n",
        "\n",
        "# Apply the log normalization function to the Proline column\n",
        "wine[\"Proline_log\"] = np.log(wine[\"Proline\"])\n",
        "\n",
        "# Check the variance of the normalized Proline column\n",
        "print(wine[\"Proline_log\"].var())"
      ],
      "metadata": {
        "id": "0Qj_N8Y_ZiE0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}