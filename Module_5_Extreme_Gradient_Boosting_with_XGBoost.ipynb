{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMh6MhS0ud8YeIZxZttT/b8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pattiecodes/DC_MLScientist_in_Py/blob/main/Module_5_Extreme_Gradient_Boosting_with_XGBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Module Start ---"
      ],
      "metadata": {
        "id": "FxtTMb5AqyWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost: Fit/Predict\n",
        "It's time to create your first XGBoost model! As Sergey showed you in the video, you can use the scikit-learn .fit() / .predict() paradigm that you are already familiar to build your XGBoost models, as the xgboost library has a scikit-learn compatible API!\n",
        "\n",
        "Here, you'll be working with churn data. This dataset contains imaginary data from a ride-sharing app with user behaviors over their first month of app usage in a set of imaginary cities as well as whether they used the service 5 months after sign-up. It has been pre-loaded for you into a DataFrame called churn_data - explore it in the Shell!\n",
        "\n",
        "Your goal is to use the first month's worth of data to predict whether the app's users will remain users of the service at the 5 month mark. This is a typical setup for a churn prediction problem. To do this, you'll split the data into training and test sets, fit a small xgboost model on the training set, and evaluate its performance on the test set by computing its accuracy.\n",
        "\n",
        "pandas and numpy have been imported as pd and np, and train_test_split has been imported from sklearn.model_selection. Additionally, the arrays for the features and the target have been created as X and y.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import xgboost as xgb.\n",
        "Create training and test sets such that 20% of the data is used for testing. Use a random_state of 123.\n",
        "Instantiate an XGBoostClassifier as xg_cl using xgb.XGBClassifier(). Specify n_estimators to be 10 estimators and an objective of 'binary:logistic'. Do not worry about what this means just yet, you will learn about these parameters later in this course.\n",
        "Fit xg_cl to the training set (X_train, y_train) using the .fit() method.\n",
        "Predict the labels of the test set (X_test) using the .predict() method and hit 'Submit Answer' to print the accuracy."
      ],
      "metadata": {
        "id": "QWP7SsZUfDAy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rb5rxaaXqu40"
      },
      "outputs": [],
      "source": [
        "# Import xgboost\n",
        "import xgboost as xgb\n",
        "\n",
        "# Create arrays for the features and the target: X, y\n",
        "X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1]\n",
        "\n",
        "# Create the training and test sets\n",
        "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "\n",
        "# Instantiate the XGBClassifier: xg_cl\n",
        "xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=123)\n",
        "\n",
        "# Fit the classifier to the training set\n",
        "xg_cl.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels of the test set: preds\n",
        "preds = xg_cl.predict(X_test)\n",
        "\n",
        "# Compute the accuracy: accuracy\n",
        "accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
        "print(\"accuracy: %f\" % (accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision trees\n",
        "Your task in this exercise is to make a simple decision tree using scikit-learn's DecisionTreeClassifier on the breast cancer dataset that comes pre-loaded with scikit-learn.\n",
        "\n",
        "This dataset contains numeric measurements of various dimensions of individual tumors (such as perimeter and texture) from breast biopsies and a single outcome value (the tumor is either malignant, or benign).\n",
        "\n",
        "We've preloaded the dataset of samples (measurements) into X and the target values per tumor into y. Now, you have to split the complete dataset into training and testing sets, and then train a DecisionTreeClassifier. You'll specify a parameter called max_depth. Many other parameters can be modified within this model, and you can check all of them out here.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import:\n",
        "train_test_split from sklearn.model_selection.\n",
        "DecisionTreeClassifier from sklearn.tree.\n",
        "Create training and test sets such that 20% of the data is used for testing. Use a random_state of 123.\n",
        "Instantiate a DecisionTreeClassifier called dt_clf_4 with a max_depth of 4. This parameter specifies the maximum number of successive split points you can have before reaching a leaf node.\n",
        "Fit the classifier to the training set and predict the labels of the test set."
      ],
      "metadata": {
        "id": "VfOK4w32F-e8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary modules\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Create the training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "\n",
        "# Instantiate the classifier: dt_clf_4\n",
        "dt_clf_4 = DecisionTreeClassifier(max_depth = 4)\n",
        "\n",
        "# Fit the classifier to the training set\n",
        "dt_clf_4.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels of the test set: y_pred_4\n",
        "y_pred_4 = dt_clf_4.predict(X_test)\n",
        "\n",
        "# Compute the accuracy of the predictions: accuracy\n",
        "accuracy = float(np.sum(y_pred_4==y_test))/y_test.shape[0]\n",
        "print(\"accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "hv6WWKUWF-1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Measuring accuracy\n",
        "You'll now practice using XGBoost's learning API through its baked in cross-validation capabilities. As Sergey discussed in the previous video, XGBoost gets its lauded performance and efficiency gains by utilizing its own optimized data structure for datasets called a DMatrix.\n",
        "\n",
        "In the previous exercise, the input datasets were converted into DMatrix data on the fly, but when you use the xgboost cv object, you have to first explicitly convert your data into a DMatrix. So, that's what you will do here before running cross-validation on churn_data.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a DMatrix called churn_dmatrix from churn_data using xgb.DMatrix(). The features are available in X and the labels in y.\n",
        "Perform 3-fold cross-validation by calling xgb.cv(). dtrain is your churn_dmatrix, params is your parameter dictionary, nfold is the number of cross-validation folds (3), num_boost_round is the number of trees we want to build (5), metrics is the metric you want to compute (this will be \"error\", which we will convert to an accuracy)."
      ],
      "metadata": {
        "id": "CH-Xj_0wMAgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create arrays for the features and the target: X, y\n",
        "X, y = churn_data.iloc[:,:-1], churn_data.iloc[:,-1]\n",
        "\n",
        "# Create the DMatrix from X and y: churn_dmatrix\n",
        "churn_dmatrix = xgb.DMatrix(data=X, label=y)\n",
        "\n",
        "# Create the parameter dictionary: params\n",
        "params = {\"objective\":\"reg:logistic\", \"max_depth\":3}\n",
        "\n",
        "# Perform cross-validation: cv_results\n",
        "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params,\n",
        "                  nfold=3, num_boost_round=5,\n",
        "                  metrics=\"error\", as_pandas=True, seed=123)\n",
        "\n",
        "# Print cv_results\n",
        "print(cv_results)\n",
        "\n",
        "# Print the accuracy\n",
        "print(((1-cv_results[\"test-error-mean\"]).iloc[-1]))"
      ],
      "metadata": {
        "id": "qEMhr-cRMAxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Measuring AUC\n",
        "Now that you've used cross-validation to compute average out-of-sample accuracy (after converting from an error), it's very easy to compute any other metric you might be interested in. All you have to do is pass it (or a list of metrics) in as an argument to the metrics parameter of xgb.cv().\n",
        "\n",
        "Your job in this exercise is to compute another common metric used in binary classification - the area under the curve (\"auc\"). As before, churn_data is available in your workspace, along with the DMatrix churn_dmatrix and parameter dictionary params.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Perform 3-fold cross-validation with 5 boosting rounds and \"auc\" as your metric.\n",
        "Print the \"test-auc-mean\" column of cv_results."
      ],
      "metadata": {
        "id": "2vMuDZpxIylB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform cross_validation: cv_results\n",
        "cv_results = xgb.cv(dtrain=churn_dmatrix, params=params,\n",
        "                  nfold=3, num_boost_round=5,\n",
        "                  metrics=\"auc\", as_pandas=True, seed=123)\n",
        "\n",
        "# Print cv_results\n",
        "print(cv_results)\n",
        "\n",
        "# Print the AUC\n",
        "print((cv_results[\"test-auc-mean\"]).iloc[-1])"
      ],
      "metadata": {
        "id": "clGWFj8RIy5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision trees as base learners\n",
        "It's now time to build an XGBoost model to predict house prices - not in Boston, Massachusetts, as you saw in the video, but in Ames, Iowa! This dataset of housing prices has been pre-loaded into a DataFrame called df. If you explore it in the Shell, you'll see that there are a variety of features about the house and its location in the city.\n",
        "\n",
        "In this exercise, your goal is to use trees as base learners. By default, XGBoost uses trees as base learners, so you don't have to specify that you want to use trees here with booster=\"gbtree\".\n",
        "\n",
        "xgboost has been imported as xgb and the arrays for the features and the target are available in X and y, respectively.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Split df into training and testing sets, holding out 20% for testing. Use a random_state of 123.\n",
        "Instantiate the XGBRegressor as xg_reg, using a seed of 123. Specify an objective of \"reg:squarederror\" and use 10 trees. Note: You don't have to specify booster=\"gbtree\" as this is the default.\n",
        "Fit xg_reg to the training data and predict the labels of the test set. Save the predictions in a variable called preds.\n",
        "Compute the rmse using np.sqrt() and the mean_squared_error() function from sklearn.metrics, which has been pre-imported."
      ],
      "metadata": {
        "id": "4e8RHS1Ui2cI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "\n",
        "# Instantiate the XGBRegressor: xg_reg\n",
        "xg_reg = xgb.XGBRegressor(objective=\"reg:squarederror\", n_estimators=10, seed=123)\n",
        "\n",
        "# Fit the regressor to the training set\n",
        "xg_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict the labels of the test set: preds\n",
        "preds = xg_reg.predict(X_test)\n",
        "\n",
        "# Compute the rmse: rmse\n",
        "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
        "print(\"RMSE: %f\" % (rmse))"
      ],
      "metadata": {
        "id": "DhGkhakVi2ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear base learners\n",
        "Now that you've used trees as base models in XGBoost, let's use the other kind of base model that can be used with XGBoost - a linear learner. This model, although not as commonly used in XGBoost, allows you to create a regularized linear regression using XGBoost's powerful learning API. However, because it's uncommon, you have to use XGBoost's own non-scikit-learn compatible functions to build the model, such as xgb.train().\n",
        "\n",
        "In order to do this you must create the parameter dictionary that describes the kind of booster you want to use (similarly to how you created the dictionary in Chapter 1 when you used xgb.cv()). The key-value pair that defines the booster type (base model) you need is \"booster\":\"gblinear\".\n",
        "\n",
        "Once you've created the model, you can use the .train() and .predict() methods of the model just like you've done in the past.\n",
        "\n",
        "Here, the data has already been split into training and testing sets, so you can dive right into creating the DMatrix objects required by the XGBoost learning API.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create two DMatrix objects - DM_train for the training set (X_train and y_train), and DM_test (X_test and y_test) for the test set.\n",
        "Create a parameter dictionary that defines the \"booster\" type you will use (\"gblinear\") as well as the \"objective\" you will minimize (\"reg:squarederror\").\n",
        "Train the model using xgb.train(). You have to specify arguments for the following parameters: params, dtrain, and num_boost_round. Use 5 boosting rounds.\n",
        "Predict the labels on the test set using xg_reg.predict(), passing it DM_test. Assign to preds.\n",
        "Hit 'Submit Answer' to view the RMSE!"
      ],
      "metadata": {
        "id": "a48sNKlCjmY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the training and testing sets into DMatrixes: DM_train, DM_test\n",
        "DM_train = xgb.DMatrix(data = X_train, label = y_train)\n",
        "DM_test =  xgb.DMatrix(data = X_test, label = y_test)\n",
        "\n",
        "# Create the parameter dictionary: params\n",
        "params = {\"booster\":\"gblinear\", \"objective\":\"reg:squarederror\"}\n",
        "\n",
        "# Train the model: xg_reg\n",
        "xg_reg = xgb.train(params = params, dtrain=DM_train, num_boost_round=5)\n",
        "\n",
        "# Predict the labels of the test set: preds\n",
        "preds = xg_reg.predict(DM_test)\n",
        "\n",
        "# Compute and print the RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
        "print(\"RMSE: %f\" % (rmse))"
      ],
      "metadata": {
        "id": "I8QetmZPjmot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating model quality\n",
        "It's now time to begin evaluating model quality.\n",
        "\n",
        "Here, you will compare the RMSE and MAE of a cross-validated XGBoost model on the Ames housing data. As in previous exercises, all necessary modules have been pre-loaded and the data is available in the DataFrame df.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "Perform 4-fold cross-validation with 5 boosting rounds and \"rmse\" as the metric.\n",
        "Extract and print the final boosting round RMSE."
      ],
      "metadata": {
        "id": "sS6pfOv7PLwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the DMatrix: housing_dmatrix\n",
        "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
        "\n",
        "# Create the parameter dictionary: params\n",
        "params = {\"objective\":\"reg:squarederror\", \"max_depth\":4}\n",
        "\n",
        "# Perform cross-validation: cv_results\n",
        "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)\n",
        "\n",
        "# Print cv_results\n",
        "print(cv_results)\n",
        "\n",
        "# Extract and print final boosting round metric\n",
        "print((cv_results[\"test-rmse-mean\"]).tail(1))"
      ],
      "metadata": {
        "id": "nugUR9hrPPJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "Now, adapt your code to compute the \"mae\" instead of the \"rmse\"."
      ],
      "metadata": {
        "id": "mZRlSlfgPwUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the DMatrix: housing_dmatrix\n",
        "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
        "\n",
        "# Create the parameter dictionary: params\n",
        "params = {\"objective\":\"reg:squarederror\", \"max_depth\":4}\n",
        "\n",
        "# Perform cross-validation: cv_results\n",
        "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics=\"mae\", as_pandas=True, seed=123)\n",
        "\n",
        "# Print cv_results\n",
        "print(cv_results)\n",
        "\n",
        "# Extract and print final boosting round metric\n",
        "print((cv_results[\"test-mae-mean\"]).tail(1))"
      ],
      "metadata": {
        "id": "zcNcJuivPyaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using regularization in XGBoost\n",
        "Having seen an example of l1 regularization in the video, you'll now vary the l2 regularization penalty - also known as \"lambda\" - and see its effect on overall model performance on the Ames housing dataset.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create your DMatrix from X and y as before.\n",
        "Create an initial parameter dictionary specifying an \"objective\" of \"reg:squarederror\" and \"max_depth\" of 3.\n",
        "Use xgb.cv() inside of a for loop and systematically vary the \"lambda\" value by passing in the current l2 value (reg).\n",
        "Append the \"test-rmse-mean\" from the last boosting round for each cross-validated xgboost model.\n",
        "Hit 'Submit Answer' to view the results. What do you notice?"
      ],
      "metadata": {
        "id": "ezBENX4CacMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the DMatrix: housing_dmatrix\n",
        "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
        "\n",
        "reg_params = [1, 10, 100]\n",
        "\n",
        "# Create the initial parameter dictionary for varying l2 strength: params\n",
        "params = {\"objective\":\"reg:squarederror\",\"max_depth\":3}\n",
        "\n",
        "# Create an empty list for storing rmses as a function of l2 complexity\n",
        "rmses_l2 = []\n",
        "\n",
        "# Iterate over reg_params\n",
        "for reg in reg_params:\n",
        "\n",
        "    # Update l2 strength\n",
        "    params[\"lambda\"] = reg\n",
        "\n",
        "    # Pass this updated param dictionary into cv\n",
        "    cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2, num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)\n",
        "\n",
        "    # Append best rmse (final round) to rmses_l2\n",
        "    rmses_l2.append(cv_results_rmse[\"test-rmse-mean\"].tail(1).values[0])\n",
        "\n",
        "# Look at best rmse per l2 param\n",
        "print(\"Best rmse as a function of l2:\")\n",
        "print(pd.DataFrame(list(zip(reg_params, rmses_l2)), columns=[\"l2\", \"rmse\"]))"
      ],
      "metadata": {
        "id": "RlG-61krahWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing individual XGBoost trees\n",
        "Now that you've used XGBoost to both build and evaluate regression as well as classification models, you should get a handle on how to visually explore your models. Here, you will visualize individual trees from the fully boosted model that XGBoost creates using the entire housing dataset.\n",
        "\n",
        "XGBoost has a plot_tree() function that makes this type of visualization easy. Once you train a model using the XGBoost learning API, you can pass it to the plot_tree() function along with the number of trees you want to plot using the num_trees argument.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a parameter dictionary with an \"objective\" of \"reg:squarederror\" and a \"max_depth\" of 2.\n",
        "Train the model using 10 boosting rounds and the parameter dictionary you created. Save the result in xg_reg.\n",
        "Plot the first tree using xgb.plot_tree(). It takes in two arguments - the model (in this case, xg_reg), and num_trees, which is 0-indexed. So to plot the first tree, specify num_trees=0.\n",
        "Plot the fifth tree.\n",
        "Plot the last (tenth) tree sideways. To do this, specify the additional keyword argument rankdir=\"LR\"."
      ],
      "metadata": {
        "id": "Zbqw_tWGiJd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the DMatrix: housing_dmatrix\n",
        "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
        "\n",
        "# Create the parameter dictionary: params\n",
        "params = {\"objective\":\"reg:squarederror\", \"max_depth\":2}\n",
        "\n",
        "# Train the model: xg_reg\n",
        "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
        "\n",
        "# Plot the first tree\n",
        "xgb.plot_tree(xg_reg, num_trees=0)\n",
        "plt.show()\n",
        "\n",
        "# Plot the fifth tree\n",
        "xgb.plot_tree(xg_reg, num_trees=4)\n",
        "plt.show()\n",
        "\n",
        "# Plot the last tree sideways\n",
        "xgb.plot_tree(xg_reg, num_trees=9, rankdir=\"LR\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1BcYkgGTiJyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing feature importances: What features are most important in my dataset\n",
        "Another way to visualize your XGBoost models is to examine the importance of each feature column in the original dataset within the model.\n",
        "\n",
        "One simple way of doing this involves counting the number of times each feature is split on across all boosting rounds (trees) in the model, and then visualizing the result as a bar graph, with the features ordered according to how many times they appear. XGBoost has a plot_importance() function that allows you to do exactly this, and you'll get a chance to use it in this exercise!\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create your DMatrix from X and y as before.\n",
        "Create a parameter dictionary with appropriate \"objective\" (\"reg:squarederror\") and a \"max_depth\" of 4.\n",
        "Train the model with 10 boosting rounds, exactly as you did in the previous exercise.\n",
        "Use xgb.plot_importance() and pass in the trained model to generate the graph of feature importances."
      ],
      "metadata": {
        "id": "-EHvxk77jSSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the DMatrix: housing_dmatrix\n",
        "housing_dmatrix = xgb.DMatrix(data = X, label = y)\n",
        "\n",
        "# Create the parameter dictionary: params\n",
        "params = {\"objective\":\"reg:squarederror\", \"max_depth\":4}\n",
        "\n",
        "# Train the model: xg_reg\n",
        "xg_reg = xgb.train(params = params, dtrain = housing_dmatrix, num_boost_round = 10)\n",
        "\n",
        "# Plot the feature importances\n",
        "xgb.plot_importance(xg_reg)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IVhKr07jjSlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tuning the number of boosting rounds\n",
        "Let's start with parameter tuning by seeing how the number of boosting rounds (number of trees you build) impacts the out-of-sample performance of your XGBoost model. You'll use xgb.cv() inside a for loop and build one model per num_boost_round parameter.\n",
        "\n",
        "Here, you'll continue working with the Ames housing dataset. The features are available in the array X, and the target vector is contained in y.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a DMatrix called housing_dmatrix from X and y.\n",
        "Create a parameter dictionary called params, passing in the appropriate \"objective\" (\"reg:squarederror\") and \"max_depth\" (set it to 3).\n",
        "Iterate over num_rounds inside a for loop and perform 3-fold cross-validation. In each iteration of the loop, pass in the current number of boosting rounds (curr_num_rounds) to xgb.cv() as the argument to num_boost_round.\n",
        "Append the final boosting round RMSE for each cross-validated XGBoost model to the final_rmse_per_round list.\n",
        "num_rounds and final_rmse_per_round have been zipped and converted into a DataFrame so you can easily see how the model performs with each boosting round. Hit 'Submit Answer' to see the results!"
      ],
      "metadata": {
        "id": "oMFD_ETmsUM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the DMatrix: housing_dmatrix\n",
        "housing_dmatrix = xgb.DMatrix(X, y)\n",
        "\n",
        "# Create the parameter dictionary for each tree: params\n",
        "params = {\"objective\":\"reg:squarederror\", \"max_depth\":3}\n",
        "\n",
        "# Create list of number of boosting rounds\n",
        "num_rounds = [5, 10, 15]\n",
        "\n",
        "# Empty list to store final round rmse per XGBoost model\n",
        "final_rmse_per_round = []\n",
        "\n",
        "# Iterate over num_rounds and build one model per num_boost_round parameter\n",
        "for curr_num_rounds in num_rounds:\n",
        "\n",
        "    # Perform cross-validation: cv_results\n",
        "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=curr_num_rounds, metrics=\"rmse\", as_pandas=True, seed=123)\n",
        "\n",
        "    # Append final round RMSE\n",
        "    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
        "\n",
        "# Print the resultant DataFrame\n",
        "num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\n",
        "print(pd.DataFrame(num_rounds_rmses,columns=[\"num_boosting_rounds\",\"rmse\"]))"
      ],
      "metadata": {
        "id": "fm9rBkJVsUcs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}