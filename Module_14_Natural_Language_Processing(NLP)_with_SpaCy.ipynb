{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZR2b9BhvQzcjcxQco02+S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pattiecodes/DC_MLScientist_in_Py/blob/main/Module_14_Natural_Language_Processing(NLP)_with_SpaCy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Module Start ---"
      ],
      "metadata": {
        "id": "Pe6ciX0rzCHy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Doc container in spaCy\n",
        "The first step of a spaCy text processing pipeline is to convert a given text string into a Doc container, which stores the processed text. In this exercise, you'll practice loading a spaCy model, creating an nlp() object, creating a Doc container and processing a text string that is available for you.\n",
        "\n",
        "en_core_web_sm model is already downloaded.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Load en_core_web_sm and create an nlp object.\n",
        "Create a doc container of the text string.\n",
        "Create a list containing the text of each tokens in the doc container."
      ],
      "metadata": {
        "id": "cQhGeFyfT5P6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuV2_xtjhOc4"
      },
      "outputs": [],
      "source": [
        "# Load en_core_web_sm and create an nlp object\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Create a Doc container for the text object\n",
        "doc = nlp(text)\n",
        "\n",
        "# Create a list containing the text of each token in the Doc container\n",
        "print([token.text for token in doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization with spaCy\n",
        "In this exercise, you'll practice tokenizing text. You'll use the first review from the Amazon Fine Food Reviews dataset for this exercise. You can access this review by using the text object provided.\n",
        "\n",
        "The en_core_web_sm model is already loaded for you. You can access it by calling nlp(). You can use list comprehension to compile output lists.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Store Doc container for the pre-loaded review in a document object.\n",
        "Store and review texts of all the tokens of the document in the variable first_text_tokens."
      ],
      "metadata": {
        "id": "2m3sEa51UDMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Doc container of the given text\n",
        "document = nlp(text)\n",
        "\n",
        "# Store and review the token text values of tokens for the Doc container\n",
        "first_text_tokens = [token.text for token in document]\n",
        "print(\"First text tokens:\\n\", first_text_tokens, \"\\n\")"
      ],
      "metadata": {
        "id": "O1NswbO7UDbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running a spaCy pipeline\n",
        "You've already run a spaCy NLP pipeline on a single piece of text and also extracted tokens of a given list of Doc containers. In this exercise, you'll practice the initial steps of running a spaCy pipeline on texts, which is a list of text strings.\n",
        "\n",
        "You will use the en_core_web_sm model for this purpose. The spaCy package has already been imported for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Load the en_core_web_sm model as nlp.\n",
        "Run an nlp() model on each item of texts, and append each corresponding Doc container to a documents list.\n",
        "Print the token texts for each Doc container of the documents list.\n"
      ],
      "metadata": {
        "id": "zI9I-o39Uyp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load en_core_web_sm model as nlp\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Run an nlp model on each item of texts and append the Doc container to documents\n",
        "documents = []\n",
        "for text in texts:\n",
        "  documents.append(nlp(text))\n",
        "\n",
        "# Print the token texts for each Doc container\n",
        "for doc in documents:\n",
        "  print([token.text for token in doc])"
      ],
      "metadata": {
        "id": "L7869CzSUy3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization with spaCy\n",
        "In this exercise, you will practice lemmatization. Lemmatization can be helpful to generate the root form of derived words. This means that given any sentence, we expect the number of lemmas to be less than or equal to the number of tokens.\n",
        "\n",
        "The first Amazon food review is provided for you in a string called text. en_core_web_sm is loaded as nlp, and has been run on the text to compile document, a Doc container for the text string.\n",
        "\n",
        "tokens, a list containing tokens for the text is also already loaded for your use.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Append the lemma for all tokens in the document, then print the list of lemmas.\n",
        "Print tokens list and observe the differences between tokens and lemmas."
      ],
      "metadata": {
        "id": "cy6LUEUnU2J6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document = nlp(text)\n",
        "tokens = [token.text for token in document]\n",
        "\n",
        "# Append the lemma for all tokens in the document\n",
        "lemmas = [token.lemma_ for token in document]\n",
        "print(\"Lemmas:\\n\", lemmas, \"\\n\")\n",
        "\n",
        "# Print tokens and compare with lemmas list\n",
        "print(\"Tokens:\\n\", tokens)"
      ],
      "metadata": {
        "id": "cpdyQSSgU2bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentence segmentation with spaCy\n",
        "In this exercise, you will practice sentence segmentation. In NLP, segmenting a document into its sentences is a useful basic operation. It is one of the first steps in many NLP tasks that are more elaborate, such as detecting named entities. Additionally, capturing the number of sentences may provide some insight into the amount of information provided by the text.\n",
        "\n",
        "You can access ten food reviews in the list called texts.\n",
        "\n",
        "The en_core_web_sm model has already been loaded for you as nlp and .\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Run the spaCy model on each item in the texts list to compile documents, a list of all Doc containers.\n",
        "Extract sentences of each doc container by iterating through documents list and append them to a list called sentences.\n",
        "Count the number of sentences in each doc container using the sentences list."
      ],
      "metadata": {
        "id": "UJ1JeHVpU9ZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating a documents list of all Doc containers\n",
        "documents = [nlp(text) for text in texts]\n",
        "\n",
        "# Iterate through documents and append sentences in each doc to the sentences list\n",
        "sentences = []\n",
        "for doc in documents:\n",
        "  sentences.append([s for s in doc.sents])\n",
        "\n",
        "# Find number of sentences per each doc container\n",
        "print([len(s) for s in sentences])"
      ],
      "metadata": {
        "id": "jTlQ9kreU9rK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POS tagging with spaCy\n",
        "In this exercise, you will practice POS tagging. POS tagging is a useful tool in NLP as it allows algorithms to understand the grammatical structure of a sentence and to confirm words that have multiple meanings such as watch and play.\n",
        "\n",
        "For this exercise, en_core_web_sm has been loaded for you as nlp. Three comments from the Airline Travel Information System (ATIS) dataset have been provided for you in a list called texts.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Compile documents, a list of all doc containers for each text in texts list using list comprehension.\n",
        "For each doc container, print each token's text and its corresponding POS tag by iterating through documents and tokens of each doc container using a nested for loop."
      ],
      "metadata": {
        "id": "kOv4RW0aVtpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile a list of all Doc containers of texts\n",
        "documents = [nlp(text) for text in texts]\n",
        "\n",
        "# Print token texts and POS tags for each Doc container\n",
        "for doc in documents:\n",
        "    for token in doc:\n",
        "        print(\"Text: \", token.text, \"| POS tag: \", token.pos_)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "hcaA-FKVVt6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NER with spaCy\n",
        "Named entity recognition (NER) helps you to easily identify key elements of a given document, like names of people and places. It helps sort unstructured data and detect important information, which is crucial if you are dealing with large datasets. In this exercise, you will practice Named Entity Recognition.\n",
        "\n",
        "en_core_web_sm has been loaded for you as nlp. Three comments from the Airline Travel Information System (ATIS) dataset have been provided for you in a list called texts.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Compile documents, a list of all Doc containers for each text in the texts using list comprehension.\n",
        "For each doc container, print each entity's text and corresponding label by iterating through doc.ents.\n",
        "Print the sixth token's text, and the entity type of the second Doc container."
      ],
      "metadata": {
        "id": "R4KW_6h1WSiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile a list of all Doc containers of texts\n",
        "documents = [nlp(text) for text in texts]\n",
        "\n",
        "# Print the entity text and label for the entities in each document\n",
        "for doc in documents:\n",
        "    print([(ent.text, ent.label_) for ent in doc.ents])\n",
        "\n",
        "# Print the 6th token's text and entity type of the second document\n",
        "print(\"\\nText:\", documents[1][5].text, \"| Entity type: \", documents[1][5].ent_type_)"
      ],
      "metadata": {
        "id": "d2pFJ0nEWS1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text processing with spaCy\n",
        "Every NLP application consists of several text processing steps. You have already learned some of these steps, including tokenization, lemmatization, sentence segmentation and named entity recognition.\n",
        "\n",
        "spaCy NLP Pipeline\n",
        "\n",
        "In this exercise, you'll continue to practice with text processing steps in spaCy, such as breaking the text into sentences and extracting named entities. You will use the first five reviews from the Amazon Fine Food Reviews dataset for this exercise. You can access these reviews by using the texts object.\n",
        "\n",
        "The en_core_web_sm model has already been loaded for you to use, and you can access it by using nlp. The list of Doc containers for each item in texts is also pre-loaded and accessible at documents.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "2\n",
        "Create sentences, a list of list of all sentences in each doc container in documents using list comprehension.\n",
        "Print num_sentences, a list containing the number of sentences for each doc container by using the len() method."
      ],
      "metadata": {
        "id": "CjfnGjGcWagt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list to store sentences of each Doc container in documents\n",
        "sentences = [[sent for sent in doc.sents] for doc in documents]\n",
        "\n",
        "# Print number of sentences in each Doc container in documents\n",
        "num_sentences = [len(s) for s in sentences]\n",
        "print(\"Number of sentences in documents:\\n\", num_sentences)"
      ],
      "metadata": {
        "id": "9bYkTHVyWa-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instructions 2/2\n",
        "50 XP\n",
        "2\n",
        "Create a list of tuples of format (entity text, entity label) for the third doc container in third_text_entities.\n",
        "Create a list of tuples of format (token text, POS tag) of first ten tokens of third doc container at third_text_10_pos."
      ],
      "metadata": {
        "id": "u24M-LzNWhwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list to store sentences of each Doc container in documents\n",
        "sentences = [[sent for sent in doc.sents] for doc in documents]\n",
        "\n",
        "# Create a list to track number of sentences per Doc container in documents\n",
        "num_sentences = [len([sent for sent in doc.sents]) for doc in documents]\n",
        "print(\"Number of sentences in documents:\\n\", num_sentences, \"\\n\")\n",
        "\n",
        "# Record entities text and corresponding label of the third Doc container\n",
        "third_text_entities = [(ent.text, ent.label_) for ent in documents[2].ents]\n",
        "print(\"Third text entities:\\n\", third_text_entities, \"\\n\")\n",
        "\n",
        "# Record first ten tokens and corresponding POS tag for the third Doc container\n",
        "third_text_10_pos = [(token.text, token.pos_) for token in documents[2]][:10]\n",
        "print(\"First ten tokens of third text:\\n\", third_text_10_pos)"
      ],
      "metadata": {
        "id": "dspl-WmyWiCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word-sense disambiguation with spaCy\n",
        "WSD is a classical problem of deciding in which sense a word is used in a sentence. Determining the sense of the word can be crucial in search engines, machine translation, and question-answering systems. In this exercise, you will practice using POS tagging for word-sense disambiguation.\n",
        "\n",
        "There are two sentences containing the word jam, with two different senses and you are tasked to identify the POS tags to help you determine the corresponding sense of the word in a given sentence.\n",
        "\n",
        "The two sentences are available in the texts list. The en_core_web_sm model is already loaded and available for your use as nlp.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "2\n",
        "Create a documents list containing the Doc containers of each element in the texts list.\n",
        "Print a tuple of the token's text and POS tags per each Doc container only if the word jam is in the token text."
      ],
      "metadata": {
        "id": "ODc8XTBZXgVR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\"This device is used to jam the signal.\",\n",
        "         \"I am stuck in a traffic jam\"]\n",
        "\n",
        "# Create a list of Doc containers in the texts list\n",
        "documents = [nlp(t) for t in texts]\n",
        "\n",
        "# Print a token's text and POS tag if the word jam is in the token's text\n",
        "for i, doc in enumerate(documents):\n",
        "    print(f\"Sentence {i+1}: \", [(token.text, token.pos_) for token in doc if \"jam\" in token.text], \"\\n\")"
      ],
      "metadata": {
        "id": "F7ievXw2XgkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "50 XP\n",
        "Question\n",
        "The word jam has multiple senses. In the sentence \"This device is used to jam the signal.\", what is the sense of the word jam?\n",
        "\n",
        "Possible answers\n",
        "\n",
        "\n",
        "**VERB: become or make unable to move or work due to a part seizing or becoming stuck.**\n",
        "\n",
        "NOUN: an instance of a machine or thing seizing or becoming stuck.\n",
        "\n",
        "NOUN: an awkward situation or predicament."
      ],
      "metadata": {
        "id": "ii_mDyJCXnOV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependency parsing with spaCy\n",
        "Dependency parsing analyzes the grammatical structure in a sentence and finds out related words as well as the type of relationship between them. An application of dependency parsing is to identify a sentence object and subject. In this exercise, you will practice extracting dependency labels for given texts.\n",
        "\n",
        "Three comments from the Airline Travel Information System (ATIS) dataset have been provided for you in a list called texts. en_core_web_sm model is already loaded and available for your use as nlp.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a documents list containing the doc containers of each element in the texts list.\n",
        "Print a tuple of (the token's text, dependency label, and label's explanation) per each doc container."
      ],
      "metadata": {
        "id": "FPBg7phCXsxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of Doc containts of texts list\n",
        "documents = [nlp(t) for t in texts]\n",
        "\n",
        "# Print each token's text, dependency label and its explanation\n",
        "for doc in documents:\n",
        "    print([(token.text, token.dep_, spacy.explain(token.dep_)) for token in doc], \"\\n\")"
      ],
      "metadata": {
        "id": "snZdMg_JXtCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# spaCy vocabulary\n",
        "Word vectors, or word embeddings, are numerical representations of words that allow computers to perform complex tasks using text data. Word vectors are a part of many spaCy models, however, a few of the models do not have word vectors.\n",
        "\n",
        "In this exercise, you will practice accessing spaCy vocabulary information. Some meta information about word vectors are stored in each spaCy model. You can access this information to learn more about the vocabulary size, word vectors dimensions, etc.\n",
        "\n",
        "The spaCy package is already imported for your use. In a spaCy model's metadata, the number of words is stored as an element with the \"vectors\" key and the dimension of word vectors is stored as an element with the \"width\" key.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Load the en_core_web_md model.\n",
        "Print the number of words in the en_core_web_md model's vocabulary.\n",
        "Print the dimensions of word vectors in the en_core_web_md model."
      ],
      "metadata": {
        "id": "k_hHEUPoYYnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the en_core_web_md model\n",
        "md_nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Print the number of words in the model's vocabulary\n",
        "print(\"Number of words: \", md_nlp.meta[\"vectors\"][\"vectors\"], \"\\n\")\n",
        "\n",
        "# Print the dimensions of word vectors in en_core_web_md model\n",
        "print(\"Dimension of word vectors: \", md_nlp.meta[\"vectors\"][\"width\"])"
      ],
      "metadata": {
        "id": "Vkel_FYMYY3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word vectors in spaCy vocabulary\n",
        "The purpose of word vectors is to allow a computer to understand words. In this exercise, you will practice extracting word vectors for a given list of words.\n",
        "\n",
        "A list of words is compiled as words. The en_core_web_md model is already imported and available as nlp.\n",
        "\n",
        "The vocabulary of en_core_web_md model contains 20,000 words. If a word does not exist in the vocabulary, you will not be able to extract its corresponding word vector. In this exercise, for simplicity, it is ensured that all the given words exist in this model's vocabulary.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Extract the IDs of all the given words and store them in an ids list.\n",
        "For each ID from ids, store the first ten elements of the word vector in the word_vectors list.\n",
        "Print the first ten elements of the first word vector from word_vectors."
      ],
      "metadata": {
        "id": "WWE6jN1WYfkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"like\", \"love\"]\n",
        "\n",
        "# IDs of all the given words\n",
        "ids = [nlp.vocab.strings[w] for w in words]\n",
        "\n",
        "# Store the first ten elements of the word vectors for each word\n",
        "word_vectors = [nlp.vocab.vectors[i][:10] for i in ids]\n",
        "\n",
        "# Print the first ten elements of the first word vector\n",
        "print(word_vectors[0])"
      ],
      "metadata": {
        "id": "7lWrWWHyYf0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word vectors projection\n",
        "You can visualize word vectors in a scatter plot to help you understand how the vocabulary words are grouped. In order to visualize word vectors, you need to project them into a two-dimensional space. You can project vectors by extracting the two principal components via Principal Component Analysis (PCA).\n",
        "\n",
        "In this exercise, you will practice how to extract word vectors and project them into two-dimensional space using the PCA library from sklearn.\n",
        "\n",
        "A short list of words that are stored in the words list and the en_core_web_md model are available for use. The model is loaded as nlp. All necessary libraries and packages are already imported for your use (PCA, numpy as np).\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Extract the word IDs from the given words and store them in the word_ids list.\n",
        "Extract the first five elements of the word vectors of the words and then stack them vertically using np.vstack() in word_vectors.\n",
        "Given a pca object, calculate the transformed word vectors using the .fit_transform() function of the pca class.\n",
        "Print the first component of the transformed word vectors using [:, 0] indexing."
      ],
      "metadata": {
        "id": "JHZz2KBuZ0G1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"tiger\", \"bird\"]\n",
        "\n",
        "# Extract word IDs of given words\n",
        "word_ids = [nlp.vocab.strings[w] for w in words]\n",
        "\n",
        "# Extract word vectors and stack the first five elements vertically\n",
        "word_vectors = np.vstack([nlp.vocab.vectors[i][:5] for i in word_ids])\n",
        "\n",
        "# Calculate the transformed word vectors using the pca object\n",
        "pca = PCA(n_components=2)\n",
        "word_vectors_transformed = pca.fit_transform(word_vectors)\n",
        "\n",
        "# Print the first component of the transformed word vectors\n",
        "print(word_vectors_transformed[:, 0])"
      ],
      "metadata": {
        "id": "naqvcpbUZ3eI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Similar words in a vocabulary\n",
        "Finding semantically similar terms has various applications in information retrieval. In this exercise, you will practice finding the most semantically similar term to the word computer from the en_core_web_md model's vocabulary.\n",
        "\n",
        "The computer word vector is already extracted and stored as word_vector. The en_core_web_md model is already loaded as nlp, and NumPy package is loaded as np.\n",
        "\n",
        "You can use the .most_similar() function of the nlp.vocab.vectors object to find the most semantically similar terms. Using [0][0] to index the output of this function will return the word IDs of the semantically similar terms. nlp.vocab.strings[<a given word>] can be used to find the word ID of a given word and it can similarly return the word associated with a given word ID.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Find the most semantically similar term from the en_core_web_md vocabulary.\n",
        "Find the list of similar words given the word IDs of the similar terms."
      ],
      "metadata": {
        "id": "pERDg_QIZ9S1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the most similar word to the word computer\n",
        "most_similar_words = nlp.vocab.vectors.most_similar(np.asarray([word_vector]), n = 1)\n",
        "\n",
        "# Find the list of similar words given the word IDs\n",
        "words = [nlp.vocab.strings[w] for w in most_similar_words[0][0]]\n",
        "print(words)"
      ],
      "metadata": {
        "id": "uQ1tgf3_Z9lD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Doc similarity with spaCy\n",
        "Semantic similarity is the process of analyzing multiple sentences to identify similarities between them. In this exercise, you will practice calculating semantic similarities of documents to a given document. The goal is to categorize a list of given reviews that are relevant to canned dog food.\n",
        "\n",
        "The canned dog food category is stored at category. A sample of five food reviews has been provided for you in a list called texts. en_core_web_md is loaded as nlp.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a documents list containing Doc containers of all texts.\n",
        "Create a Doc container of the category and store it as category_document.\n",
        "Iterate through documents and print the similarity scores of each Doc container and the category_document, rounded to three digits."
      ],
      "metadata": {
        "id": "Gboms1AhbstM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a documents list containing Doc containers\n",
        "documents = [nlp(t) for t in texts]\n",
        "\n",
        "# Create a Doc container of the category\n",
        "category = \"canned dog food\"\n",
        "category_document = nlp(category)\n",
        "\n",
        "# Print similarity scores of each Doc container and the category_document\n",
        "for i, doc in enumerate(documents):\n",
        "  print(f\"Semantic similarity with document {i+1}:\", round(doc.similarity(category_document), 3))"
      ],
      "metadata": {
        "id": "WTxk1BujbtZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Span similarity with spaCy\n",
        "Determining semantic similarity can help you to categorize texts into predefined categories or detect relevant texts, or to flag duplicate content. In this exercise, you will practice calculating the semantic similarities of spans of a document to a given document. The goal is to find the most relevant Span of three tokens that are relevant to canned dog food.\n",
        "\n",
        "The given category of canned dog food is stored at category. A text string is already stored in the text object and the en_core_web_md is loaded as nlp. The Doc container of the text is also already created and stored at document.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a Doc container for the category and store at category_document.\n",
        "Print similarity score of a given Span and the category_document, rounded to three digits."
      ],
      "metadata": {
        "id": "MgF3zfWAb3kB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Doc container for the category\n",
        "category = \"canned dog food\"\n",
        "category_document = nlp(category)\n",
        "\n",
        "# Print similarity score of a given Span and category_document\n",
        "document_span = document[0:3]\n",
        "print(f\"Semantic similarity with\", document_span.text, \":\", round(document_span.similarity(category_document), 3))"
      ],
      "metadata": {
        "id": "xhC6H5Ztb4MA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Semantic similarity for categorizing text\n",
        "The main objective of semantic similarity is to measure the distance between the semantic meanings of a pair of words, phrases, sentences, or documents. For example, the word “car” is more similar to “bus” than it is to “cat”. In this exercise, you will find similar sentences to the word sauce from an example text in Amazon Fine Food Reviews. You can use spacy to calculate the similarity score of the word sauce and any of the sentences in a given texts string and report the most similar sentence's score.\n",
        "\n",
        "A texts string is pre-loaded that contains all reviews' Text data. You'll use en_core_web_md English model for this exercise which is already available as nlp.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Use nlp to generate Doc containers for the word sauce and for texts and store them at key and sentences respectively.\n",
        "Calculate similarity scores of the word sauce with each sentence in the texts string (rounded to two digits)."
      ],
      "metadata": {
        "id": "oq6wHvflcQ-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Populate Doc containers for the word \"sauce\" and for \"texts\" string\n",
        "key = nlp(\"sauce\")\n",
        "sentences = nlp(texts)\n",
        "\n",
        "# Calculate similarity score of each sentence and a Doc container for the word sauce\n",
        "semantic_scores = []\n",
        "for sent in sentences.sents:\n",
        "\tsemantic_scores.append({\"score\": round(sent.similarity(key), 2)})\n",
        "print(semantic_scores)"
      ],
      "metadata": {
        "id": "TmMhDKgrcRj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding pipes in spaCy\n",
        "You often use an existing spaCy model for different NLP tasks. However, in some cases, an off-the-shelf pipeline component such as sentence segmentation will take long times to produce expected results. In this exercise, you'll practice adding a pipeline component to a spaCy model (text processing pipeline).\n",
        "\n",
        "You will use the first five reviews from the Amazon Fine Food Reviews dataset for this exercise. You can access these reviews by using the texts string.\n",
        "\n",
        "The spaCy package is already imported for you to use.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Load a blank spaCy English model and add a sentencizer component to the model.\n",
        "Create a Doc container for the texts, create a list to store sentences of the given document and print its number of sentences.\n",
        "Print the list of tokens in the second sentence from the sentences list."
      ],
      "metadata": {
        "id": "JuZfejjcdmIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a blank spaCy English model and add a sentencizer component\n",
        "nlp = spacy.blank(\"en\")\n",
        "nlp.add_pipe(\"sentencizer\")\n",
        "\n",
        "# Create Doc containers, store sentences and print its number of sentences\n",
        "doc = nlp(texts)\n",
        "sentences = [s for s in doc.sents]\n",
        "print(\"Number of sentences: \", len(sentences), \"\\n\")\n",
        "\n",
        "# Print the list of tokens in the second sentence\n",
        "print(\"Second sentence tokens: \", [token for token in sentences[1]])"
      ],
      "metadata": {
        "id": "wYXxJ10ddm5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyzing pipelines in spaCy\n",
        "spaCy allows you to analyze a spaCy pipeline to check whether any required attributes are not set. In this exercise, you'll practice analyzing a spaCy pipeline. Earlier in the video, an existing en_core_web_sm pipeline was analyzed and the result was No problems found., in this instance, you will analyze a blank spaCy English model with few added components and observe results of the analysis.\n",
        "\n",
        "The spaCy package is already imported for you to use.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "2\n",
        "Load a blank spaCy English model as nlp.\n",
        "Add tagger and entity_linker pipeline components to the blank model.\n",
        "Analyze the nlp pipeline."
      ],
      "metadata": {
        "id": "IKsYHRHodyKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a blank spaCy English model\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "# Add tagger and entity_linker pipeline components\n",
        "nlp.add_pipe(\"tagger\")\n",
        "nlp.add_pipe(\"entity_linker\")\n",
        "\n",
        "# Analyze the pipeline\n",
        "analysis = nlp.analyze_pipes(pretty=True)"
      ],
      "metadata": {
        "id": "JrROhVGvdynV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EntityRuler with blank spaCy model\n",
        "EntityRuler lets you to add entities to doc.ents. It can be combined with EntityRecognizer, a spaCy pipeline component for named-entity recognition, to boost accuracy, or used on its own to implement a purely rule-based entity recognition system. In this exercise, you will practice adding an EntityRuler component to a blank spaCy English model and classify named entities of the given text using purely rule-based named-entity recognition.\n",
        "\n",
        "The spaCy package is already imported and a blank spaCy English model is ready for your use as nlp. A list of patterns to classify lower cased OpenAI and Microsoft as ORG is already created for your use.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create and add an EntityRuler component to the pipeline.\n",
        "Add given patterns to the EntityRuler component.\n",
        "Run the model on the given text and create its corresponding Doc container.\n",
        "Print a tuple of (entities text and types) for all entities in the Doc container."
      ],
      "metadata": {
        "id": "SI1Ad2bjejic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "patterns = [{\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"openai\"}]},\n",
        "            {\"label\": \"ORG\", \"pattern\": [{\"LOWER\": \"microsoft\"}]}]\n",
        "text = \"OpenAI has joined forces with Microsoft.\"\n",
        "\n",
        "# Add EntityRuler component to the model\n",
        "entity_ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "\n",
        "# Add given patterns to the EntityRuler component\n",
        "entity_ruler.add_patterns(patterns)\n",
        "\n",
        "# Run the model on a given text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print entities text and type for all entities in the Doc container\n",
        "print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "metadata": {
        "id": "yNKe2hrLel-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EntityRuler for NER\n",
        "EntityRuler can be combined with EntityRecognizer of an existing model to boost its accuracy. In this exercise, you will practice combining an EntityRuler component and an existing NER component of the en_core_web_sm model. The model is already loaded as nlp.\n",
        "\n",
        "When EntityRuler is added before NER component, the entity recognizer will respect the existing entity spans and adjust its predictions based on patterns added to the EntityRuler to improve accuracy of named entity recognition task.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Add an EntityRuler to the nlp before ner component.\n",
        "Define a token entity pattern to classify lower cased new york group as ORG.\n",
        "Add the patterns to the EntityRuler component.\n",
        "Run the model and print the tuple of entities text and type for the Doc container."
      ],
      "metadata": {
        "id": "1M1VRUOiewy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"New York Group was built in 1987.\"\n",
        "\n",
        "# Add an EntityRuler to the nlp before NER component\n",
        "ruler = nlp.add_pipe(\"entity_ruler\", before='ner')\n",
        "\n",
        "# Define a pattern to classify lower cased new york group as ORG\n",
        "patterns = [{\"label\": \"ORG\", \"pattern\": [{\"lower\": \"new york group\"}]}]\n",
        "\n",
        "# Add the patterns to the EntityRuler component\n",
        "ruler.add_patterns(patterns)\n",
        "\n",
        "# Run the model and print entities text and type for all the entities\n",
        "doc = nlp(text)\n",
        "print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "metadata": {
        "id": "jCj5-BNZexMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EntityRuler with multi-patterns in spaCy\n",
        "EntityRuler lets you to add entities to doc.ents and boost its named entity recognition performance. In this exercise, you will practice adding an EntityRuler component to an existing nlp pipeline to ensure multiple entities are correctly being classified.\n",
        "\n",
        "The en_core_web_sm model is already loaded and is available for your use as nlp. You can access an example text in example_text and use nlp and doc to access an spaCy model and Doc container of example_text respectively.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Print a list of tuples of entities text and types in the example_text with the nlp model.\n",
        "Define multiple patterns to match lower cased brother and sisters to PERSON label.\n",
        "Add an EntityRuler component to the nlp pipeline and add the patterns to the EntityRuler.\n",
        "Print a tuple of text and type of entities for the example_text with the nlp model."
      ],
      "metadata": {
        "id": "ChaiYuE2e850"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Print a list of tuples of entities text and types in the example_text\n",
        "print(\"Before EntityRuler: \", [(ent.text, ent.label_) for ent in nlp(example_text).ents], \"\\n\")\n",
        "\n",
        "# Define pattern to add a label PERSON for lower cased sisters and brother entities\n",
        "patterns = [{\"label\": \"PERSON\", \"pattern\": [{\"lower\": \"sisters\"}]},\n",
        "            {\"label\": \"PERSON\", \"pattern\": [{\"lower\": \"brother\"}]}]\n",
        "\n",
        "# Add an EntityRuler component and add the patterns to the ruler\n",
        "ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "ruler.add_patterns(patterns)\n",
        "\n",
        "# Print a list of tuples of entities text and types\n",
        "print(\"After EntityRuler: \", [(ent.text, ent.label_) for ent in nlp(example_text).ents])"
      ],
      "metadata": {
        "id": "vHDhx6GPe9Us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RegEx in Python\n",
        "Rule-based information extraction is useful for many NLP tasks. Certain types of entities, such as dates or phone numbers have distinct formats that can be recognized by a set of rules without needing to train any model. In this exercise, you will practice using re package for RegEx. The goal is to find phone numbers in a given text.\n",
        "\n",
        "re package is already imported for your use. You can use \\d to match string patterns representative of a metacharacter that matches any digit from 0 to 9.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Define a pattern to match phone numbers of the form (111)-111-1111.\n",
        "Find all the matching patterns using re.finditer() method.\n",
        "For each match, print start and end characters and matching section of the given text."
      ],
      "metadata": {
        "id": "etTQjND5fvyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Our phone number is (425)-123-4567.\"\n",
        "\n",
        "# Define a pattern to match phone numbers\n",
        "pattern = r\"\\((\\d){3}\\)-(\\d){3}-(\\d){4}\"\n",
        "\n",
        "# Find all the matching patterns in the text\n",
        "phones = re.finditer(pattern, text)\n",
        "\n",
        "# Print start and end characters and matching section of the text\n",
        "for match in phones:\n",
        "    start_char = match.start()\n",
        "    end_char = match.end()\n",
        "    print(\"Start character: \", start_char, \"| End character: \", end_char, \"| Matching text: \", text[start_char:end_char])"
      ],
      "metadata": {
        "id": "w_fGYO-9fwLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RegEx with EntityRuler in spaCy\n",
        "Regular expressions, or RegEx, are used for rule-based information extraction with complex string matching patterns. RegEx can be used to retrieve patterns or replace matching patterns in a string with some other patterns. In this exercise, you will practice using EntityRuler in spaCy to find email addresses in a given text.\n",
        "\n",
        "spaCy package is already imported for your use. You can use \\d to match string patterns representative of a metacharacter that matches any digit from 0 to 9.\n",
        "\n",
        "A spaCy pattern can use REGEX as an attribute. In this case, a pattern will be of shape [{\"TEXT\": {\"REGEX\": \"<a given pattern>\"}}].\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Define a pattern to match phone numbers of the form 8888888888 to be used by the EntityRuler.\n",
        "Load a blank spaCy English model and add an EntityRuler component to the pipeline.\n",
        "Add the compiled pattern to the EntityRuler component.\n",
        "Run the model and print the tuple of text and type of entities for the given text."
      ],
      "metadata": {
        "id": "9Rco--rJf5Ji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Our phone number is 4251234567.\"\n",
        "\n",
        "# Define a pattern to match phone numbers\n",
        "patterns = [{\"label\": \"PHONE_NUMBERS\", \"pattern\": [{\"TEXT\": {\"REGEX\": \"(\\d){10}\"}}]}]\n",
        "\n",
        "# Load a blank model and add an EntityRuler\n",
        "nlp = spacy.blank(\"en\")\n",
        "ruler = nlp.add_pipe(\"entity_ruler\")\n",
        "\n",
        "# Add the compiled patterns to the EntityRuler\n",
        "ruler.add_patterns(patterns)\n",
        "\n",
        "# Print the tuple of entities texts and types for the given text\n",
        "doc = nlp(text)\n",
        "print([(ent.text, ent.label_) for ent in doc.ents])"
      ],
      "metadata": {
        "id": "Ox1mMyrLf5qC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# /\n",
        "/\n",
        "Daily XP\n",
        "10283\n",
        "Exercise\n",
        "Exercise\n",
        "Matching a single term in spaCy\n",
        "RegEx patterns are not trivial to read, write and debug. But you are not at a loss, spaCy provides a readable and production-level alternative, the Matcher class. The Matcher class can match predefined rules to a sequence of tokens in a given Doc container. In this exercise, you will practice using Matcher to find a single word.\n",
        "\n",
        "You can access the corresponding text in example_text and use nlp and doc to access an spaCy model and Doc container of example_text respectively.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Initialize a Matcher class.\n",
        "Define a pattern to match lower cased witch in the example_text.\n",
        "Add the patterns to the Matcher class and find matches.\n",
        "Iterate through matches and print start and end token indices and span of the matched text."
      ],
      "metadata": {
        "id": "RVH_aiBzgp3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(example_text)\n",
        "\n",
        "# Initialize a Matcher object\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Define a pattern to match lower cased word witch\n",
        "pattern = [{\"lower\": \"witch\"}]\n",
        "\n",
        "# Add the pattern to matcher object and find matches\n",
        "matcher.add(\"CustomMatcher\", [pattern])\n",
        "matches = matcher(doc)\n",
        "\n",
        "# Print start and end token indices and span of the matched text\n",
        "for match_id, start, end in matches:\n",
        "    print(\"Start token: \", start, \" | End token: \", end, \"| Matched text: \", doc[start:end].text)"
      ],
      "metadata": {
        "id": "2LE9c7TagqXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PhraseMatcher in spaCy\n",
        "While processing unstructured text, you often have long lists and dictionaries that you want to scan and match in given texts. The Matcher patterns are handcrafted and each token needs to be coded individually. If you have a long list of phrases, Matcher is no longer the best option. In this instance, PhraseMatcher class helps us match long dictionaries. In this exercise, you will practice to retrieve patterns with matching shapes to multiple terms using PhraseMatcher class.\n",
        "\n",
        "en_core_web_sm model is already loaded and ready for you to use as nlp. PhraseMatcher class is imported. A text string and a list of terms are available for your use.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Initialize a PhraseMatcher class with an attr to match to shape of given terms.\n",
        "Create patterns to add to the PhraseMatcher object.\n",
        "Find matches to the given patterns and print start and end token indices and matching section of the given text."
      ],
      "metadata": {
        "id": "5uMimS_hgy65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"There are only a few acceptable IP addresse: (1) 127.100.0.1, (2) 123.4.1.0.\"\n",
        "terms = [\"110.0.0.0\", \"101.243.0.0\"]\n",
        "\n",
        "# Initialize a PhraseMatcher class to match to shapes of given terms\n",
        "matcher = PhraseMatcher(nlp.vocab, attr = \"SHAPE\")\n",
        "\n",
        "# Create patterns to add to the PhraseMatcher object\n",
        "patterns = [nlp.make_doc(term) for term in terms]\n",
        "matcher.add(\"IPAddresses\", patterns)\n",
        "\n",
        "# Find matches to the given patterns and print start and end characters and matches texts\n",
        "doc = nlp(text)\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "    print(\"Start token: \", start, \" | End token: \", end, \"| Matched text: \", doc[start:end].text)"
      ],
      "metadata": {
        "id": "fIXe5aKPgzR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Matching with extended syntax in spaCy\n",
        "Rule-based information extraction is essential for any NLP pipeline. The Matcher class allows patterns to be more expressive by allowing some operators inside the curly brackets. These operators are for extended comparison and look similar to Python's in, not in and comparison operators. In this exercise, you will practice with spaCy's matching functionality, Matcher, to find matches for given terms from an example text.\n",
        "\n",
        "Matcher class is already imported from spacy.matcher library. You will use a Doc container of an example text in this exercise by calling doc. A pre-loaded spaCy model is also accessible at nlp.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Define a matcher object using Matcher and nlp.\n",
        "Use the IN operator to define a pattern to match tiny squares and tiny mouthful.\n",
        "Use this pattern to find matches for doc.\n",
        "Print start and end token indices and text span of the matches."
      ],
      "metadata": {
        "id": "IeEs3flsg_gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(example_text)\n",
        "\n",
        "# Define a matcher object\n",
        "matcher = Matcher(nlp.vocab)\n",
        "# Define a pattern to match tiny squares and tiny mouthful\n",
        "pattern = [{\"lower\": \"tiny\"}, {\"lower\": {\"IN\": [\"squares\", \"mouthful\"]}}]\n",
        "\n",
        "# Add the pattern to matcher object and find matches\n",
        "matcher.add(\"CustomMatcher\", [pattern])\n",
        "matches = matcher(doc)\n",
        "\n",
        "# Print out start and end token indices and the matched text span per match\n",
        "for match_id, start, end in matches:\n",
        "    print(\"Start token: \", start, \" | End token: \", end, \"| Matched text: \", doc[start:end].text)"
      ],
      "metadata": {
        "id": "nceIus4Fg_53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model performance on your data\n",
        "In this exercise, you will practice evaluating an existing model on your data. In this case, the aim is to examine model performance on a specific entity label, PRODUCT. If a model can accurately classify a large percentage of PRODUCT entities (e.g. more than 75%), you do not need to train the model on examples of PRODUCT entities, otherwise, you should consider training the model to improve its performance on PRODUCT entity prediction.\n",
        "\n",
        "You'll use two reviews from the Amazon Fine Food Reviews dataset for this exercise. You can access these reviews by using the texts list.\n",
        "\n",
        "The en_core_web_sm model is already loaded for you. You can access it by calling nlp(). The model is already ran on the texts list and documents, a list of Doc containers is available for your use.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Compile a target_entities list, of all the entities for each of the documents, and append a tuple of (entities text, entities label) only if Jumbo is in the entity text.\n",
        "For any tuple in the target_entities, append True to a correct_labels list if the entity label (second attribute in the tuple) is PRODUCT, otherwise append False.\n"
      ],
      "metadata": {
        "id": "2z-jgG2xiw6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Append a tuple of (entities text, entities label) if Jumbo is in the entity\n",
        "target_entities = []\n",
        "for doc in documents:\n",
        "  target_entities.extend([(ent.text, ent.label_) for ent in doc.ents if \"Jumbo\" in ent.text])\n",
        "print(target_entities)\n",
        "\n",
        "# Append True to the correct_labels list if the entity label is `PRODUCT`\n",
        "correct_labels = []\n",
        "for ent in target_entities:\n",
        "  if ent[1] == \"PRODUCT\":\n",
        "    correct_labels.append(True)\n",
        "  else:\n",
        "    correct_labels.append(False)\n",
        "print(correct_labels)"
      ],
      "metadata": {
        "id": "ve5x0CtjixYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Annotation and preparing training data\n",
        "After collecting data, you can annotate data in the required format for a spaCy model. In this exercise, you will practice forming the correct annotated data record for an NER task in the medical domain.\n",
        "\n",
        "A sentence and two entities of entity_1 with a text of chest pain and a SYMPTOM type and entity_2 with a text of hyperthyroidism and a DISEASE type are available for you to use.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Complete the annotated_data record in the correct format.\n",
        "Extract start and end characters of each entity and store as the corresponding variables.\n",
        "Store the same input sentence and its entities in the proper training format as training_data."
      ],
      "metadata": {
        "id": "n-i26vpFjnnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"A patient with chest pain had hyperthyroidism.\"\n",
        "entity_1 = \"chest pain\"\n",
        "entity_2 = \"hyperthyroidism\"\n",
        "\n",
        "# Store annotated data information in the correct format\n",
        "annotated_data = {\"sentence\": text, \"entities\": [{\"label\": \"SYMPTOM\", \"value\": entity_1}, {\"label\": \"DISEASE\", \"value\": entity_2}]}\n",
        "\n",
        "# Extract start and end characters of each entity\n",
        "entity_1_start_char = text.find(entity_1)\n",
        "entity_1_end_char = entity_1_start_char + len(entity_1)\n",
        "entity_2_start_char = text.find(entity_2)\n",
        "entity_2_end_char = entity_2_start_char + len(entity_2)\n",
        "\n",
        "# Store the same input information in the proper format for training\n",
        "training_data = [(text, {\"entities\": [(entity_1_start_char, entity_1_end_char, \"SYMPTOM\"),\n",
        "                                      (entity_2_start_char, entity_2_end_char, \"DISEASE\")]})]\n",
        "print(training_data)"
      ],
      "metadata": {
        "id": "KFXXM98IjoDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compatible training data\n",
        "Recall that you cannot feed the raw text directly to spaCy. Instead, you need to create an Example object for each training example. In this exercise, you will practice converting a training_data with a single annotated sentence into a list of Example objects.\n",
        "\n",
        "en_core_web_sm model is already imported and ready for use as nlp. The Example class is also imported for your use.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Iterate through the text and annotations in the training_data, convert the text to a Doc container and store it at doc.\n",
        "Create an Example object using the doc object and the annotations of each training data point, and store it at example_sentence.\n",
        "Append example_sentence to a list of all_examples.\n"
      ],
      "metadata": {
        "id": "2K3wOzMbj0eK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = 'A patient with chest pain had hyperthyroidism.'\n",
        "training_data = [(example_text, {'entities': [(15, 25, 'SYMPTOM'), (30, 45, 'DISEASE')]})]\n",
        "\n",
        "all_examples = []\n",
        "# Iterate through text and annotations and convert text to a Doc container\n",
        "for text, annotations in training_data:\n",
        "  doc = nlp(text)\n",
        "\n",
        "  # Create an Example object from the doc contianer and annotations\n",
        "  example_sentence = Example.from_dict(doc, annotations)\n",
        "  print(example_sentence.to_dict(), \"\\n\")\n",
        "\n",
        "  # Append the Example object to the list of all examples\n",
        "  all_examples.append(example_sentence)\n",
        "\n",
        "print(\"Number of formatted training data: \", len(all_examples))"
      ],
      "metadata": {
        "id": "6ComUzB4j1Af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training preparation steps\n",
        "Before and during training of a spaCy model, you'll need to (1) disable other pipeline components in order to only train the intended component and (2) convert a Doc container of a training data point and its corresponding annotations into an Example class.\n",
        "\n",
        "In this exercise, you will practice these two steps by using a pre-loaded en_core_web_sm model, which is accessible as nlp. Example class is already imported and a text string and related annotations are also available for your use.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Disable all pipeline components of the nlp model except ner.\n",
        "Convert a text string and its annotations to the correct format usable for training."
      ],
      "metadata": {
        "id": "aOi3O4B9kjNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Disable all pipeline components of  except `ner`\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "nlp.disable_pipes(*other_pipes)\n",
        "\n",
        "# Convert a text and its annotations to the correct format usable for training\n",
        "doc = nlp.make_doc(text)\n",
        "example = Example.from_dict(doc, annotations)\n",
        "print(\"Example object for training: \\n\", example.to_dict())"
      ],
      "metadata": {
        "id": "8WPkPvK6kj4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train an existing NER model\n",
        "A spaCy model may not work well on a given data. One solution is to train the model on our data. In this exercise, you will practice training a NER model in order to improve its prediction performance.\n",
        "\n",
        "A spaCy en_core_web_sm model that is accessible as nlp, which is not able to correctly predict house as an entity in a test string.\n",
        "\n",
        "Given a training_data, write the steps to update this model while iterating through the data two times. The other pipelines are already disabled and optimizer is also ready to be used. Number of epochs is already set to 2.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Use the optimizer object and for each epoch, shuffle the dataset using random package and create an Example object.\n",
        "Update the nlp model using .update attribute and set the sgd arguments to use the optimizer."
      ],
      "metadata": {
        "id": "ZXzhpPHUkxPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "print(\"Before training: \", [(ent.text, ent.label_)for ent in nlp(test).ents])\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "nlp.disable_pipes(*other_pipes)\n",
        "optimizer = nlp.create_optimizer()\n",
        "\n",
        "# Shuffle training data and the dataset using random package per epoch\n",
        "for i in range(epochs):\n",
        "  random.shuffle(training_data)\n",
        "  for text, annotations in training_data:\n",
        "    doc = nlp.make_doc(text)\n",
        "    # Update nlp model after setting sgd argument to optimizer\n",
        "    example = Example.from_dict(doc, annotations)\n",
        "    nlp.update([example], sgd = optimizer)\n",
        "print(\"After training: \", [(ent.text, ent.label_)for ent in nlp(test).ents])"
      ],
      "metadata": {
        "id": "mUxu-356kxqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training a spaCy model from scratch\n",
        "spaCy provides a very clean and efficient approach to train your own models. In this exercise, you will train a NER model from scratch on a real-world corpus (CORD-19 data).\n",
        "\n",
        "Training data is available in the right format as training_data. In this exercise, you will use a given list of labels (\"Pathogen\", \"MedicalCondition\", \"Medicine\") stored in labels using a blank English model (nlp) with an NER component. Intended medical labels will be added the NER pipeline and then you can train the model for one epoch. You can use pre-imported Example class to convert the training data to the required format. To track model training you can add a losses list to the .update() method and review training loss.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a blank spaCy model and add an NER component to the model.\n",
        "Disable other pipeline components, use the created optimizer object and update the model weights using converted data to the Example format."
      ],
      "metadata": {
        "id": "xOW287Z0k9eo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a blank English model, add NER component, add given labels to the ner pipeline\n",
        "nlp = spacy.blank(\"en\")\n",
        "ner = nlp.add_pipe(\"ner\")\n",
        "for ent in labels:\n",
        "    ner.add_label(ent)\n",
        "\n",
        "# Disable other pipeline components, complete training loop and run training loop\n",
        "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
        "nlp.disable_pipes(*other_pipes)\n",
        "losses = {}\n",
        "optimizer = nlp.begin_training()\n",
        "for text, annotation in training_data:\n",
        "    doc = nlp.make_doc(text)\n",
        "    example = Example.from_dict(doc, annotation)\n",
        "    nlp.update([example], sgd=optimizer, losses=losses)\n",
        "    print(losses)"
      ],
      "metadata": {
        "id": "RpxV-Qegk9_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Module End ---"
      ],
      "metadata": {
        "id": "jYYfwc0tlQPN"
      }
    }
  ]
}