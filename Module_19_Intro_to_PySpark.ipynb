{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMG7dU+ziD9yT514qAOzNTx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pattiecodes/DC_MLScientist_in_Py/blob/main/Module_19_Intro_to_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Module Start ---"
      ],
      "metadata": {
        "id": "PvRaKkw3-gIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a SparkSession\n",
        "Let's start with creating a new SparkSession. In this course, you will be usually provided with one, but creating a new one or getting an existing one is a must-have skill!\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import SparkSession from pyspark.sql.\n",
        "Make a new SparkSession called \"my_spark\" using SparkSession.builder.getOrCreate().\n",
        "Print my_spark to the console to verify it's a SparkSession."
      ],
      "metadata": {
        "id": "VG0HNflK_dqn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oD4zyjfziOAG"
      },
      "outputs": [],
      "source": [
        "# Import SparkSession from pyspark.sql\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create my_spark\n",
        "my_spark = SparkSession.builder.appName(\"my_spark\").getOrCreate()\n",
        "\n",
        "# Print my_spark\n",
        "print(my_spark)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading census data\n",
        "Let's start creating your first PySpark DataFrame! The file adult_reduced.csv contains a grouping of adults based on a variety of demographic categories. These data have been adapted from the US Census. There are a total of 32562 groupings of adults.\n",
        "\n",
        "We should load the csv and see the resulting schema.\n",
        "```\n",
        "Data dictionary:\n",
        "\n",
        "Variable\t    Description\n",
        "age\t            Individual age\n",
        "education_num\tEducation by degree\n",
        "marital_status  Marital status\n",
        "occupation\t    Occupation\n",
        "income\t        Categorical income\n",
        "```\n",
        "Instructions\n",
        "100 XP\n",
        "Create a PySpark DataFrame from the\"adult_reduced.csv\" file using the spark.read.csv() method.\n",
        "Show the resulting DataFrame."
      ],
      "metadata": {
        "id": "D_K2flTS_mga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in the CSV\n",
        "census_adult = spark.read.csv(\"adult_reduced.csv\")\n",
        "\n",
        "# Show the DataFrame\n",
        "census_adult.show()"
      ],
      "metadata": {
        "id": "x97pHxO3_psr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading a CSV and performing aggregations\n",
        "You have a spreadsheet of Data Scientist salaries from companies ranging is size from small to large. You want to see if there is a major difference between average salaries grouped by company size.\n",
        "\n",
        "Remember, there's already a SparkSession called spark in your workspace!\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Load a csv file as a DataFrame and infer the schema.\n",
        "Return the count of the number of rows.\n",
        "Group by the column company_size and calculate the average salary with salary_in_usd."
      ],
      "metadata": {
        "id": "h5xGecOYBBIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV file into a DataFrame\n",
        "salaries_df = spark.read.csv(\"salaries.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Count the total number of rows\n",
        "row_count = salaries_df.count()\n",
        "print(f\"Total rows: {row_count}\")\n",
        "\n",
        "# Group by company size and calculate the average of salaries\n",
        "salaries_df.groupBy(\"company_size\").agg({\"salary_in_usd\": \"avg\"}).show()\n",
        "salaries_df.show()"
      ],
      "metadata": {
        "id": "kaETcwTrBBsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filtering by company\n",
        "Using that same dataset from the last exercise, you realized that you only care about the jobs that are entry level (\"EN\") in Canada (\"CA\"). What does the salaries look like there? Remember, there's already a SparkSession called spark in your workspace!\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Filter to subset the DataFrame to where company_location is \"CA\".\n",
        "Calculate the average of the salary_in_usd column.\n",
        "Show the result!"
      ],
      "metadata": {
        "id": "rD2LQ-wUBNGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Average salary for entry level in Canada\n",
        "CA_jobs = ca_salaries_df.filter(ca_salaries_df['company_location'] == \"CA\").filter(ca_salaries_df['experience_level']\n",
        " == \"EN\").groupBy().avg(\"salary_in_usd\")\n",
        "\n",
        "# Show the result\n",
        "CA_jobs.show()"
      ],
      "metadata": {
        "id": "SZQVa3_rBNa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Infer and filter\n",
        "Imagine you have a census dataset that you know has a header and a schema. Let's load that dataset and let PySpark infer the schema. What do you see if you filter on adults over 40?\n",
        "\n",
        "Remember, there's already a SparkSession called spark in your workspace!\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Load a JSON file adults.json.\n",
        "Filter the data to include adults over the age of 40.\n",
        "Show the results."
      ],
      "metadata": {
        "id": "dVXU8OQlB-9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataframe\n",
        "census_df = spark.read.json(\"adults.json\")\n",
        "\n",
        "# Filter rows based on age condition\n",
        "salary_filtered_census = census_df.filter(census_df[\"age\"] > 40)\n",
        "\n",
        "# Show the result\n",
        "salary_filtered_census.show()"
      ],
      "metadata": {
        "id": "oOXSlWfqB_Ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Schema writeout\n",
        "We've loaded Schemas multiple ways now. So lets define a schema directly. We'll use a Data dictionary:\n",
        "\n",
        "```\n",
        "Data dictionary:\n",
        "\n",
        "Variable\t    Description\n",
        "age\t            Individual age\n",
        "education_num\tEducation by degree\n",
        "marital_status  Marital status\n",
        "occupation\t    Occupation\n",
        "income\t        Categorical income\n",
        "```\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Specify the data schema, giving columns names (age,education_num,marital_status,occupation, and income) and column types, setting a comma for the sep= argument.\n",
        "Read data from a comma-delimited file called adult_reduced_100.csv.\n",
        "Print the schema for the resulting DataFrame."
      ],
      "metadata": {
        "id": "54tUabwUCNd9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "# Fill in the schema with the columns you need from the exercise instructions\n",
        "schema = StructType([StructField(\"age\",IntegerType()),\n",
        "                     StructField(\"education_num\",IntegerType()),\n",
        "                     StructField(\"marital_status\",StringType()),\n",
        "                     StructField(\"occupation\",StringType()),\n",
        "                     StructField(\"income\",StringType())\n",
        "                    ])\n",
        "\n",
        "# Read in the CSV, using the schema you defined above\n",
        "census_adult = spark.read.csv(\"adult_reduced_100.csv\", sep=',', header=False, schema=schema)\n",
        "\n",
        "# Print out the schema\n",
        "census_adult.printSchema()"
      ],
      "metadata": {
        "id": "aznOmT5bCSaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling missing data with fill and drop\n",
        "Oh my… You have a lot of missing values in this dataset! Let's clean it up! With the loaded CSV file, drop rows with any null values, and show the results!\n",
        "\n",
        "Remember, there's already a SparkSession called spark in your workspace!\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Drop any rows with null values in the census_df DataFrame.\n",
        "Show the resulting DataFrame."
      ],
      "metadata": {
        "id": "koPiVxQcDC8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with any nulls\n",
        "census_cleaned = census_df.na.drop()\n",
        "\n",
        "# Show the result\n",
        "census_cleaned.show()"
      ],
      "metadata": {
        "id": "ELQ9y1DBDDNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Column operations - creating and renaming columns\n",
        "The census dataset is still not quite showing everything you want it to. Let's make a new synthetic column by adding a new column based on existing columns, and rename it for clarity.\n",
        "\n",
        "Remember, there's already a SparkSession called spark in your workspace!\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a new column, \"weekly_salary\", by dividing the \"income\" column by 52.\n",
        "Rename the \"age\" column to \"years\".\n",
        "Show the resulting DataFrame."
      ],
      "metadata": {
        "id": "6ZEVOsa1DLbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new column 'weekly_salary'\n",
        "census_df_weekly = census_df.withColumn(\"weekly_salary\", census_df.income / 52)\n",
        "\n",
        "# Rename the 'age' column to 'years'\n",
        "census_df_weekly = census_df_weekly.withColumnRenamed(\"age\", \"years\")\n",
        "\n",
        "# Show the result\n",
        "census_df_weekly.show()"
      ],
      "metadata": {
        "id": "vKtCdlCaDLst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Joining flights with their destination airports\n",
        "You've been hired as a data engineer for a global travel company. Your first task is to help the company improve its operations by analyzing flight data. You have two datasets in your workspace: one containing details about flights (flights) and another with information about destination airports (airports), both are already available in your workspace..\n",
        "\n",
        "Your goal? Combine these datasets to create a powerful dataset that links each flight to its destination airport.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Examine the airports DataFrame. Note which key column will let you join airports to the flights table.\n",
        "Join the flights with the airports DataFrame on the \"dest\" column. Save the result as flights_with_airports.\n",
        "Examine flights_with_airports again. Note the new information that has been added."
      ],
      "metadata": {
        "id": "v93dtLEOD6qf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Examine the data\n",
        "airports.show()\n",
        "\n",
        "# .withColumnRenamed() renames the \"faa\" column to \"dest\"\n",
        "airports = airports.withColumnRenamed(\"faa\", \"dest\")\n",
        "\n",
        "# Join the DataFrames\n",
        "flights_with_airports = flights.join(airports, on=\"dest\", how=\"leftouter\")\n",
        "\n",
        "# Examine the new DataFrame\n",
        "flights_with_airports.show()"
      ],
      "metadata": {
        "id": "EdHg34PvD67r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Integers in PySpark UDFs\n",
        "This exercise covers UDFs, allowing you to understand function creation in PySpark! As you work through this exercise, think about what this would replace in a data cleaning workflow.\n",
        "\n",
        "Remember, there's already a SparkSession called spark in your workspace!\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Register the function age_category as a UDF called age_category_udf.\n",
        "Add a new column to the DataFrame df called \"category\" that applies the UDF to categorize people based on their age. The argument for age_category_udf() is provided for you.\n",
        "Show the resulting DataFrame."
      ],
      "metadata": {
        "id": "U7nLKTJ6Ep2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the function age_category as a UDF\n",
        "age_category_udf = udf(age_category, StringType())\n",
        "\n",
        "# Apply your udf to the DataFrame\n",
        "age_category_df_2 = age_category_df.withColumn(\"category\", age_category_udf(age_category_df[\"age\"]))\n",
        "\n",
        "# Show df\n",
        "age_category_df_2.show()"
      ],
      "metadata": {
        "id": "SmC6pEEfEqH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pandas UDFs\n",
        "This exercise covers Pandas UDFs, so that you can practice their syntax! As you work through this exercise, notice the differences between the Pyspark UDF from the last exercise and this type of UDF.\n",
        "\n",
        "Remember, there's already a SparkSession called spark in your workspace!\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Define the add_ten_pandas() function as a pandas UDF.\n",
        "Add a new column to the DataFrame called \"10_plus\" that applies the pandas UDF to the df column \"value\".\n",
        "Show the resulting DataFrame."
      ],
      "metadata": {
        "id": "SH6z71E6EyUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a Pandas UDF that adds 10 to each element in a vectorized way\n",
        "@pandas_udf(DoubleType())\n",
        "def add_ten_pandas(column):\n",
        "    return column + 10\n",
        "\n",
        "# Apply the UDF and show the result\n",
        "df.withColumn(\"10_plus\", add_ten_pandas(df[\"value\"]))\n",
        "df.show()"
      ],
      "metadata": {
        "id": "IqmmfxZ6Eyjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating RDDs\n",
        "In PySpark, you can create an RDD (Resilient Distributed Dataset) in a few different ways. Since you are already familiar with DataFrames, you will set this up using a DataFrame. Remember, there's already a SparkSession called spark in your workspace!\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a DataFrame from the provided list called df.\n",
        "Convert the DataFrame to an RDD.\n",
        "Collect and print the resulting RDD."
      ],
      "metadata": {
        "id": "sGMPK7N0b380"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame\n",
        "df = spark.read.csv(\"salaries.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Convert DataFrame to RDD\n",
        "rdd = df.rdd\n",
        "\n",
        "# Show the RDD's contents\n",
        "rdd.collect()\n",
        "print(rdd)"
      ],
      "metadata": {
        "id": "-h-b3bMdb4Pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collecting RDDs\n",
        "For this exercise, you’ll work with both RDDs and DataFrames in PySpark. The goal is to group data and perform aggregation using both RDD operations and DataFrame methods.\n",
        "\n",
        "You will load a CSV file containing employee salary data into PySpark as an RDD. You'll then group by the experience level data and calculate the maximum salary for each experience level from a DataFrame. By doing this, you'll see the relative strengths of both data formats.\n",
        "\n",
        "The dataset you're using is related to Data Scientist Salaries, so finding market trends are in your best interests! We've already loaded and normalized the data for you! Remember, there's already a SparkSession called spark in your workspace!\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create an RDD from a DataFrame.\n",
        "Collect and display the results of the RDD and DataFrame.\n",
        "Group by the \"experience_level\" and calculate the maximum salary for each."
      ],
      "metadata": {
        "id": "o20mtT8NcD63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an RDD from the df_salaries\n",
        "rdd_salaries = df_salaries.rdd\n",
        "\n",
        "# Collect and print the results\n",
        "print(rdd_salaries.collect())\n",
        "\n",
        "# Group by the experience level and calculate the maximum salary\n",
        "dataframe_results = df_salaries.groupby(\"experience_level\").agg({\"salary_in_usd\": 'max'})\n",
        "\n",
        "# Show the results\n",
        "dataframe_results.show()"
      ],
      "metadata": {
        "id": "1rEQtsnlcESF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Querying on a temp view\n",
        "In this exercise, you'll practice registering a DataFrame as a temporary SQL view in PySpark. Temporary views are powerful tools that allow you to query data using SQL syntax, making complex data manipulations easier and more intuitive. Your goal is to create a view from a provided DataFrame and run SQL queries against it, a common task for ETL and ELT work.\n",
        "\n",
        "You already have a SparkContext, spark, and a PySpark DataFrame, df, available in your workspace.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Register a new view called \"data_view\" from the DataFrame df.\n",
        "Run the provided SQL query to calculate total salary by position."
      ],
      "metadata": {
        "id": "Pf82klFvcsrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Register as a view\n",
        "df.createOrReplaceTempView(\"data_view\")\n",
        "\n",
        "# Advanced SQL query: Calculate total salary by Position\n",
        "result = spark.sql(\"\"\"\n",
        "    SELECT Position, SUM(Salary) AS Total_Salary\n",
        "    FROM data_view\n",
        "    GROUP BY Position\n",
        "    ORDER BY Total_Salary DESC\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "result.show()"
      ],
      "metadata": {
        "id": "iBvtpoqZcs-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Running SQL on DataFrames\n",
        "DataFrames can be easily manipulated using SQL queries in PySpark. The .sql() method in a SparkSession enables applications to run SQL queries programmatically and returns the result as another DataFrame. In this exercise, you'll create a temporary table of a DataFrame that you have created previously, then construct a query to select the names of the people from the temporary table and assign the result to a new DataFrame.\n",
        "\n",
        "Remember, you already have a SparkSession spark and a DataFrame df available in your workspace.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a temporary table named \"people\" from the df DataFrame.\n",
        "Construct a query to select the names of the people from the temporary table people.\n",
        "Assign the result of Spark's query to a new DataFrame called people_df_names.\n",
        "Print the top 10 names of the people from people_df_names DataFrame."
      ],
      "metadata": {
        "id": "BAFgGp4DdCuz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a temporary table \"people\"\n",
        "df.createOrReplaceTempView(\"people\")\n",
        "\n",
        "# Select the names from the temporary table people\n",
        "query = \"\"\"SELECT name FROM people\"\"\"\n",
        "\n",
        "# Assign the result of Spark's query to people_df_names\n",
        "people_df_names = spark.sql(query)\n",
        "\n",
        "# Print the top 10 names of the people\n",
        "people_df_names.show(10)"
      ],
      "metadata": {
        "id": "fwgrsdB7dFZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analytics with SQL on DataFrames\n",
        "SQL queries are concise and easy to run compared to DataFrame operations. But in order to apply SQL queries on a DataFrame first, you need to create a temporary view of the DataFrame as a table and then apply SQL queries on the created table.\n",
        "\n",
        "You already have a SparkContext spark and salaries_df available in your workspace.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create temporary table \"salaries_table\" from salaries_df DataFrame.\n",
        "Construct a query to extract the \"job_title\" column from company_location in Canada (\"CA\").\n",
        "Apply the SQL query and create a new DataFrame canada_titles.\n",
        "Get a summary of the table."
      ],
      "metadata": {
        "id": "BbjJafJxdOn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a temporary view of salaries_table\n",
        "salaries_df.createOrReplaceTempView('salaries_table')\n",
        "\n",
        "# Construct the \"query\"\n",
        "query = '''SELECT job_title, salary_in_usd FROM salaries_table WHERE company_location == \"CA\"'''\n",
        "\n",
        "# Apply the SQL \"query\"\n",
        "canada_titles = spark.sql(query)\n",
        "\n",
        "# Generate basic statistics\n",
        "canada_titles.describe().show()"
      ],
      "metadata": {
        "id": "x0MHtm-JdPBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aggregating in PySpark\n",
        "Now you're ready to do some aggregating of your own! You're going to use a salary dataset that you have already used. Let's see what aggregations you can create! A SparkSession called spark is already in your workspace, along with the Spark DataFrame salaries_df.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Find the minimum salary at a US, Small company - performing the filtering by referencing the column directly (\"salary_in_usd\"), not passing a SQL string.\n",
        "Find the maximum salary at a US, Large company, denoted by a \"L\" - performing the filtering by referencing the column directly (\"salary_in_usd\"), not passing a SQL string."
      ],
      "metadata": {
        "id": "yvLt7QeGeKar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the minimum salaries for small companies\n",
        "salaries_df.filter(salaries_df.company_size == \"S\").groupBy().min(\"salary_in_usd\").show()\n",
        "\n",
        "# Find the maximum salaries for large companies\n",
        "salaries_df.filter(salaries_df.company_size == \"L\").groupBy().max(\"salary_in_usd\").show()"
      ],
      "metadata": {
        "id": "uxDpwJoneKqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aggregating in RDDs\n",
        "Now that you have conducted analytics with DataFrames in PySpark, let's briefly do a similar task with an RDD. Using the provided code, get the sum of the values of an RDD in PySpark.\n",
        "\n",
        "A Spark session called spark has already been made for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create an RDD from the provided DataFrame.\n",
        "Apply the provided Lambda Function to the keys of the RDD.\n",
        "Collect the results of the aggregation."
      ],
      "metadata": {
        "id": "ktvo18YyeUm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DataFrame Creation\n",
        "data = [(\"HR\", \"3000\"), (\"IT\", \"4000\"), (\"Finance\", \"3500\")]\n",
        "columns = [\"Department\", \"Salary\"]\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "# Map the DataFrame to an RDD\n",
        "rdd = df.rdd.map(lambda row: (row[\"Department\"], row[\"Salary\"]))\n",
        "\n",
        "# Apply a lambda function to get the sum of the DataFrame\n",
        "rdd_aggregated = rdd.reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "# Show the collected Results\n",
        "print(rdd_aggregated.collect())"
      ],
      "metadata": {
        "id": "z014cookeVPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Complex Aggregations\n",
        "To get you familiar with more of the built in aggregation methods, let's do a slightly more complex aggregation! The goal is to merge all these commands into a single line.\n",
        "\n",
        "Remember, a SparkSession called spark is already in your workspace, along the Spark DataFrame salaries_df.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Calculate the average salaries of large US companies using the \"salary_in_usd\" column.\n",
        "Calculate the total salaries of large US companies."
      ],
      "metadata": {
        "id": "dkTloz2tefr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Average salaries at large US companies\n",
        "salaries_df.filter(salaries_df.company_size == \"L\").filter(salaries_df.company_location == \"US\").groupBy().avg(\"salary_in_usd\").show()\n",
        "\n",
        "# Set a large companies variable for other analytics\n",
        "large_companies=salaries_df.filter(salaries_df.company_size == \"L\").filter(salaries_df.company_location == \"US\")\n",
        "\n",
        "# Total salaries in usd\n",
        "large_companies.groupBy().sum(\"salary_in_usd\").show()"
      ],
      "metadata": {
        "id": "B80e_Z-2ef-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bringing it all together I\n",
        "You've built a solid foundation in PySpark, explored its core components, and worked through practical scenarios involving Spark SQL, DataFrames, and advanced operations. Now it’s time to bring it all together. Over the next two exercises, you're going to make a SparkSession, a Dataframe, cache that Dataframe, conduct analytics and explain the outcome!\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import SparkSession from pyspark.sql.\n",
        "Make a new SparkSession called final_spark using SparkSession.builder.getOrCreate().\n",
        "Print my_spark to the console to verify it's a SparkSession.\n",
        "Create a new DataFrame from a preloaded schema and column definition."
      ],
      "metadata": {
        "id": "GqqWj6yYfZcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import SparkSession from pyspark.sql\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create my_spark\n",
        "my_spark = SparkSession.builder.appName(\"final_spark\").getOrCreate()\n",
        "\n",
        "# Print my_spark\n",
        "print(my_spark)\n",
        "\n",
        "# Load dataset into a DataFrame\n",
        "df = my_spark.createDataFrame(data, schema=columns)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "id": "mL69MiJ4fZ4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bringing it all together II\n",
        "Create a DataFrame, apply transformations, cache it, and check if it’s cached. Then, uncache it to release memory. For this exercise a spark session has been made for you! Look carefully at the outcome of the .explain() method to understand what the outcome is!\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Cache the df DataFrame.\n",
        "Explain the processing of the agg_result DataFrame.\n",
        "Unpersist the cached df DataFrame after processing."
      ],
      "metadata": {
        "id": "sUV8GnK_flce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cache the DataFrame\n",
        "df.cache()\n",
        "\n",
        "# Perform aggregation\n",
        "agg_result = df.groupBy(\"Department\").sum(\"Salary\")\n",
        "agg_result.show()\n",
        "\n",
        "# Analyze the execution plan\n",
        "agg_result.explain()\n",
        "\n",
        "# Uncache the DataFrame\n",
        "df.unpersist()"
      ],
      "metadata": {
        "id": "-xLKVBiXflxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Module End ---"
      ],
      "metadata": {
        "id": "8vNOClkLfvwN"
      }
    }
  ]
}