{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXtGGQ8A9uvVf4lh1eI8eh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pattiecodes/DC_MLScientist_in_Py/blob/main/Module_21_Winning_a_Kaggle_Competition_in_Py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Module Start ---"
      ],
      "metadata": {
        "id": "DvDcnUuM1JFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore train data\n",
        "You will work with another Kaggle competition called \"Store Item Demand Forecasting Challenge\". In this competition, you are given 5 years of store-item sales data, and asked to predict 3 months of sales for 50 different items in 10 different stores.\n",
        "\n",
        "To begin, let's explore the train data for this competition. For the faster performance, you will work with a subset of the train data containing only a single month history.\n",
        "\n",
        "Your initial goal is to read the input data and take the first look at it.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import pandas as pd.\n",
        "Read train data using pandas' read_csv() method.\n",
        "Print the head of the train data (using head() method) to see the data sample."
      ],
      "metadata": {
        "id": "SxTTTEcm1K24"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6hD-JzaikTq"
      },
      "outputs": [],
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Read train data\n",
        "train = pd.read_csv('train.csv')\n",
        "\n",
        "# Look at the shape of the data\n",
        "print('Train shape:', train.shape)\n",
        "\n",
        "# Look at the head() of the data\n",
        "print(train.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore test data\n",
        "Having looked at the train data, let's explore the test data in the \"Store Item Demand Forecasting Challenge\". Remember, that the test dataset generally contains one column less than the train one.\n",
        "\n",
        "This column, together with the output format, is presented in the sample submission file. Before making any progress in the competition, you should get familiar with the expected output.\n",
        "\n",
        "That is why, let's look at the columns of the test dataset and compare it to the train columns. Additionally, let's explore the format of the sample submission. The train DataFrame is available in your workspace.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "2\n",
        "Read the test dataset.\n",
        "Print the column names of the train and test datasets."
      ],
      "metadata": {
        "id": "XQ3j2xlB1Rap"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the test data\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "# Print train and test columns\n",
        "print('Train columns:', train.columns.tolist())\n",
        "print('Test columns:', test.columns.tolist())"
      ],
      "metadata": {
        "id": "nvSqxTQQ1RsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "50 XP\n",
        "2\n",
        "Notice that test columns do not have the target \"sales\" column. Now, read the sample submission file.\n",
        "Look at the head of the submission file to get the output format."
      ],
      "metadata": {
        "id": "u2rUwTJO1VjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the test data\n",
        "test = pd.read_csv('test.csv')\n",
        "# Print train and test columns\n",
        "print('Train columns:', train.columns.tolist())\n",
        "print('Test columns:', test.columns.tolist())\n",
        "\n",
        "# Read the sample submission file\n",
        "sample_submission = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "# Look at the head() of the sample submission\n",
        "print(sample_submission.head())"
      ],
      "metadata": {
        "id": "kqxPiAKh1Vpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train a simple model\n",
        "As you determined, you are dealing with a regression problem. So, now you're ready to build a model for a subsequent submission. But now, instead of building the simplest Linear Regression model as in the slides, let's build an out-of-box Random Forest model.\n",
        "\n",
        "You will use the RandomForestRegressor class from the scikit-learn library.\n",
        "\n",
        "Your objective is to train a Random Forest model with default parameters on the \"store\" and \"item\" features.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Read the train data using pandas.\n",
        "Create a Random Forest object.\n",
        "Train the Random Forest model on the \"store\" and \"item\" features with \"sales\" as a target.\n"
      ],
      "metadata": {
        "id": "jNioZsrB10L9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Read the train data\n",
        "train = pd.read_csv('train.csv')\n",
        "\n",
        "# Create a Random Forest object\n",
        "rf = RandomForestRegressor()\n",
        "\n",
        "# Train a model\n",
        "rf.fit(X=train[['store', 'item']], y=train['sales'])"
      ],
      "metadata": {
        "id": "RCQyfH0h10b3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare a submission\n",
        "You've already built a model on the training data from the Kaggle Store Item Demand Forecasting Challenge. Now, it's time to make predictions on the test data and create a submission file in the specified format.\n",
        "\n",
        "Your goal is to read the test data, make predictions, and save these in the format specified in the \"sample_submission.csv\" file. The rf object you created in the previous exercise is available in your workspace.\n",
        "\n",
        "Note that starting from now and for the rest of the course, pandas library will be always imported for you and could be accessed as pd.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "2\n",
        "Read \"test.csv\" and \"sample_submission.csv\" files using pandas.\n",
        "Look at the head of the sample submission to determine the format."
      ],
      "metadata": {
        "id": "w0V_DRNd18gv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read test and sample submission data\n",
        "test = pd.read_csv('test.csv')\n",
        "sample_submission = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "# Show the head() of the sample_submission\n",
        "print(sample_submission.head())"
      ],
      "metadata": {
        "id": "SkWz4xbW18wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "50 XP\n",
        "2\n",
        "Note that sample submission has id and sales columns. Now, make predictions on the test data using the rf model, that you fitted on the train data.\n",
        "Using the format given in the sample submission, write your results to a new file."
      ],
      "metadata": {
        "id": "B_Oj1gBE2ABb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read test and sample submission data\n",
        "test = pd.read_csv('test.csv')\n",
        "sample_submission = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "# Show the head() of the sample_submission\n",
        "print(sample_submission.head())\n",
        "\n",
        "# Get predictions for the test set\n",
        "test['sales'] = rf.predict(test[['store', 'item']])\n",
        "\n",
        "# Write test predictions using the sample_submission format\n",
        "test[['id', 'sales']].to_csv('kaggle_submission.csv', index=False)"
      ],
      "metadata": {
        "id": "qESDBQ8t2AKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train XGBoost models\n",
        "Every Machine Learning method could potentially overfit. You will see it on this example with XGBoost. Again, you are working with the Store Item Demand Forecasting Challenge. The train DataFrame is available in your workspace.\n",
        "\n",
        "Firstly, let's train multiple XGBoost models with different sets of hyperparameters using XGBoost's learning API. The single hyperparameter you will change is:\n",
        "\n",
        "max_depth - maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.\n",
        "Instructions 1/3\n",
        "35 XP\n",
        "1\n",
        "Set the maximum depth to 2. Then hit Submit Answer button to train the first model."
      ],
      "metadata": {
        "id": "YJDPCBJf2kpG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Create DMatrix on train data\n",
        "dtrain = xgb.DMatrix(data=train[['store', 'item']],\n",
        "                     label=train['sales'])\n",
        "\n",
        "# Define xgboost parameters\n",
        "params = {'objective': 'reg:linear',\n",
        "          'max_depth': 2,\n",
        "          'verbosity': 0}\n",
        "\n",
        "# Train xgboost model\n",
        "xg_depth_2 = xgb.train(params=params, dtrain=dtrain)"
      ],
      "metadata": {
        "id": "E9MhwQiS2lBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/3\n",
        "\n",
        "Now, set the maximum depth to 8. Then hit Submit Answer button to train the second model.\n"
      ],
      "metadata": {
        "id": "WKp-etuD2o1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Create DMatrix on train data\n",
        "dtrain = xgb.DMatrix(data=train[['store', 'item']],\n",
        "                     label=train['sales'])\n",
        "\n",
        "# Define xgboost parameters\n",
        "params = {'objective': 'reg:linear',\n",
        "          'max_depth': 15,\n",
        "          'verbosity': 0}\n",
        "\n",
        "# Train xgboost model\n",
        "xg_depth_15 = xgb.train(params=params, dtrain=dtrain)import xgboost as xgb\n",
        "\n",
        "# Create DMatrix on train data\n",
        "dtrain = xgb.DMatrix(data=train[['store', 'item']],\n",
        "                     label=train['sales'])\n",
        "\n",
        "# Define xgboost parameters\n",
        "params = {'objective': 'reg:linear',\n",
        "          'max_depth': 8,\n",
        "          'verbosity': 0}\n",
        "\n",
        "# Train xgboost model\n",
        "xg_depth_8 = xgb.train(params=params, dtrain=dtrain)"
      ],
      "metadata": {
        "id": "Dcu3W9tZ2prO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/3\n",
        "\n",
        "\n",
        "Finally, set the maximum depth to 15. Then hit Submit Answer button to train the third model."
      ],
      "metadata": {
        "id": "c4XLlkMj2tI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Create DMatrix on train data\n",
        "dtrain = xgb.DMatrix(data=train[['store', 'item']],\n",
        "                     label=train['sales'])\n",
        "\n",
        "# Define xgboost parameters\n",
        "params = {'objective': 'reg:linear',\n",
        "          'max_depth': 15,\n",
        "          'verbosity': 0}\n",
        "\n",
        "# Train xgboost model\n",
        "xg_depth_15 = xgb.train(params=params, dtrain=dtrain)"
      ],
      "metadata": {
        "id": "aD3Iq3JN2uO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore overfitting XGBoost\n",
        "Having trained 3 XGBoost models with different maximum depths, you will now evaluate their quality. For this purpose, you will measure the quality of each model on both the train data and the test data. As you know by now, the train data is the data models have been trained on. The test data is the next month sales data that models have never seen before.\n",
        "\n",
        "The goal of this exercise is to determine whether any of the models trained is overfitting. To measure the quality of the models you will use Mean Squared Error (MSE). It's available in sklearn.metrics as mean_squared_error() function that takes two arguments: true values and predicted values.\n",
        "\n",
        "train and test DataFrames together with 3 models trained (xg_depth_2, xg_depth_8, xg_depth_15) are available in your workspace.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Make predictions for each model on both the train and test data.\n",
        "Calculate the MSE between the true values and your predictions for both the train and test data."
      ],
      "metadata": {
        "id": "gZ-lq-ni2zXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "dtrain = xgb.DMatrix(data=train[['store', 'item']])\n",
        "dtest = xgb.DMatrix(data=test[['store', 'item']])\n",
        "\n",
        "# For each of 3 trained models\n",
        "for model in [xg_depth_2, xg_depth_8, xg_depth_15]:\n",
        "    # Make predictions\n",
        "    train_pred = model.predict(dtrain)\n",
        "    test_pred = model.predict(dtest)\n",
        "\n",
        "    # Calculate metrics\n",
        "    mse_train = mean_squared_error(train['sales'], train_pred)\n",
        "    mse_test = mean_squared_error(test['sales'], test_pred)\n",
        "    print('MSE Train: {:.3f}. MSE Test: {:.3f}'.format(mse_train, mse_test))"
      ],
      "metadata": {
        "id": "AC9Ynp-32zrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define a competition metric\n",
        "Competition metric is used by Kaggle to evaluate your submissions. Moreover, you also need to measure the performance of different models on a local validation set.\n",
        "\n",
        "For now, your goal is to manually develop a couple of competition metrics in case if they are not available in sklearn.metrics.\n",
        "\n",
        "In particular, you will define:\n",
        "\n",
        "Mean Squared Error (MSE) for the regression problem:\n",
        "\n",
        "\n",
        "\n",
        "Logarithmic Loss (LogLoss) for the binary classification problem:\n",
        "\n",
        "\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "Using numpy, define MSE metric. As a function input, you're given true y_true and predicted y_pred arrays."
      ],
      "metadata": {
        "id": "cjmjUNJx3Kc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Import MSE from sklearn\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Define your own MSE function\n",
        "def own_mse(y_true, y_pred):\n",
        "  \t# Raise differences to the power of 2\n",
        "    squares = np.power(y_true - y_pred, 2)\n",
        "    # Find mean over all observations\n",
        "    err = np.mean(squares)\n",
        "    return err\n",
        "\n",
        "print('Sklearn MSE: {:.5f}'.format(mean_squared_error(y_regression_true, y_regression_pred)))\n",
        "print('Your MSE: {:.5f}'.format(own_mse(y_regression_true, y_regression_pred)))"
      ],
      "metadata": {
        "id": "YezeusFn3LES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "\n",
        "Using numpy, define LogLoss metric. As input, you're given true class y_true and probability predicted prob_pred."
      ],
      "metadata": {
        "id": "AhSrNNt73QmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Import log_loss from sklearn\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Define your own LogLoss function\n",
        "def own_logloss(y_true, prob_pred):\n",
        "  \t# Find loss for each observation\n",
        "    terms = y_true * np.log(prob_pred) + (1 - y_true) * np.log(1 - prob_pred)\n",
        "    # Find mean over all observations\n",
        "    err = np.mean(terms)\n",
        "    return -err\n",
        "\n",
        "print('Sklearn LogLoss: {:.5f}'.format(log_loss(y_classification_true, y_classification_pred)))\n",
        "print('Your LogLoss: {:.5f}'.format(own_logloss(y_classification_true, y_classification_pred)))"
      ],
      "metadata": {
        "id": "GeYXzvn33Rj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA statistics\n",
        "As mentioned in the slides, you'll work with New York City taxi fare prediction data. You'll start with finding some basic statistics about the data. Then you'll move forward to plot some dependencies and generate hypotheses on them.\n",
        "\n",
        "The train and test DataFrames are already available in your workspace.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "2\n",
        "Find the shapes of the train and test data.\n",
        "Look at the head of the train data."
      ],
      "metadata": {
        "id": "D32Cu6nH3i_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shapes of train and test data\n",
        "print('Train shape:', train.shape)\n",
        "print('Test shape:', test.shape)\n",
        "\n",
        "# Train head()\n",
        "print(train.head())"
      ],
      "metadata": {
        "id": "nbDR05ba3jMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "50 XP\n",
        "2\n",
        "Describe the \"fare_amount\" column to get some statistics about the target variable.\n",
        "Find the distribution of the \"passenger_count\" in the train data (using the value_counts() method)."
      ],
      "metadata": {
        "id": "PU61K3nl3nV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shapes of train and test data\n",
        "print('Train shape:', train.shape)\n",
        "print('Test shape:', test.shape)\n",
        "\n",
        "# Train head()\n",
        "print(train.head())\n",
        "\n",
        "# Describe the target variable\n",
        "print(train.fare_amount.describe())\n",
        "\n",
        "# Train distribution of passengers within rides\n",
        "print(train.passenger_count.value_counts())"
      ],
      "metadata": {
        "id": "PmKKPBps3ngV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA plots I\n",
        "After generating a couple of basic statistics, it's time to come up with and validate some ideas about the data dependencies. Again, the train DataFrame from the taxi competition is already available in your workspace.\n",
        "\n",
        "To begin with, let's make a scatterplot plotting the relationship between the fare amount and the distance of the ride. Intuitively, the longer the ride, the higher its price.\n",
        "\n",
        "To get the distance in kilometers between two geo-coordinates, you will use Haversine distance. Its calculation is available with the haversine_distance() function defined for you. The function expects train DataFrame as input.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a new variable \"distance_km\" as Haversine distance between pickup and dropoff points.\n",
        "Plot a scatterplot with \"fare_amount\" on the x axis and \"distance_km\" on the y axis. To draw a scatterplot use matplotlib scatter() method.\n",
        "Set a limit on a ride distance to be between 0 and 50 kilometers to avoid plotting outliers."
      ],
      "metadata": {
        "id": "3DbkWJ_43rxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the ride distance\n",
        "train['distance_km'] = haversine_distance(train)\n",
        "\n",
        "# Draw a scatterplot\n",
        "plt.scatter(x=train['fare_amount'], y=train['distance_km'], alpha=0.5)\n",
        "plt.xlabel('Fare amount')\n",
        "plt.ylabel('Distance, km')\n",
        "plt.title('Fare amount based on the distance')\n",
        "\n",
        "# Limit on the distance\n",
        "plt.ylim(0, 50)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1VyUv-rk3sCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA plots II\n",
        "Another idea that comes to mind is that the price of a ride could change during the day.\n",
        "\n",
        "Your goal is to plot the median fare amount for each hour of the day as a simple line plot. The hour feature is calculated for you. Don't worry if you do not know how to work with the date features. We will explore them in the chapter on Feature Engineering.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Group train DataFrame by \"hour\" and calculate the median for the \"fare_amount\" column.\n",
        "Using hour_price DataFrame obtained, plot a line with \"hour\" on the x axis and \"fare_amount\" on the y axis."
      ],
      "metadata": {
        "id": "EVbh3HNt300l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create hour feature\n",
        "train['pickup_datetime'] = pd.to_datetime(train.pickup_datetime)\n",
        "train['hour'] = train.pickup_datetime.dt.hour\n",
        "\n",
        "# Find median fare_amount for each hour\n",
        "hour_price = train.groupby('hour', as_index=False)['fare_amount'].median()\n",
        "\n",
        "# Plot the line plot\n",
        "plt.plot(hour_price['hour'], hour_price['fare_amount'], marker='o')\n",
        "plt.xlabel('Hour of the day')\n",
        "plt.ylabel('Median fare amount')\n",
        "plt.title('Fare amount based on day time')\n",
        "plt.xticks(range(24))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zXYiJ4sY31Ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-fold cross-validation\n",
        "You will start by getting hands-on experience in the most commonly used K-fold cross-validation.\n",
        "\n",
        "The data you'll be working with is from the \"Two sigma connect: rental listing inquiries\" Kaggle competition. The competition problem is a multi-class classification of the rental listings into 3 classes: low interest, medium interest and high interest. For faster performance, you will work with a subsample consisting of 1,000 observations.\n",
        "\n",
        "You need to implement a K-fold validation strategy and look at the sizes of each fold obtained. train DataFrame is already available in your workspace.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a KFold object with 3 folds.\n",
        "Loop over each split using the kf object.\n",
        "For each split select training and testing folds using train_index and test_index."
      ],
      "metadata": {
        "id": "jwhvbiCj3_0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import KFold\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Create a KFold object\n",
        "kf = KFold(n_splits=3, shuffle=True, random_state=123)\n",
        "\n",
        "# Loop through each split\n",
        "fold = 0\n",
        "for train_index, test_index in kf.split(train):\n",
        "    # Obtain training and testing folds\n",
        "    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
        "    print('Fold: {}'.format(fold))\n",
        "    print('CV train shape: {}'.format(cv_train.shape))\n",
        "    print('Medium interest listings in CV train: {}\\n'.format(sum(cv_train.interest_level == 'medium')))\n",
        "    fold += 1"
      ],
      "metadata": {
        "id": "XztyAFuJ4AD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stratified K-fold\n",
        "As you've just noticed, you have a pretty different target variable distribution among the folds due to the random splits. It's not crucial for this particular competition, but could be an issue for the classification competitions with the highly imbalanced target variable.\n",
        "\n",
        "To overcome this, let's implement the stratified K-fold strategy with the stratification on the target variable. train DataFrame is already available in your workspace.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a StratifiedKFold object with 3 folds and shuffling.\n",
        "Loop over each split using str_kf object. Stratification is based on the \"interest_level\" column.\n",
        "For each split select training and testing folds using train_index and test_index."
      ],
      "metadata": {
        "id": "9zZ_8lRb4Hwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import StratifiedKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Create a StratifiedKFold object\n",
        "str_kf = StratifiedKFold(n_splits=3, shuffle=True, random_state=123)\n",
        "\n",
        "# Loop through each split\n",
        "fold = 0\n",
        "for train_index, test_index in str_kf.split(train, train['interest_level']):\n",
        "    # Obtain training and testing folds\n",
        "    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
        "    print('Fold: {}'.format(fold))\n",
        "    print('CV train shape: {}'.format(cv_train.shape))\n",
        "    print('Medium interest listings in CV train: {}\\n'.format(sum(cv_train.interest_level == 'medium')))\n",
        "    fold += 1"
      ],
      "metadata": {
        "id": "hQMUBza04H-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Time K-fold\n",
        "Remember the \"Store Item Demand Forecasting Challenge\" where you are given store-item sales data, and have to predict future sales?\n",
        "\n",
        "It's a competition with time series data. So, time K-fold cross-validation should be applied. Your goal is to create this cross-validation strategy and make sure that it works as expected.\n",
        "\n",
        "Note that the train DataFrame is already available in your workspace, and that TimeSeriesSplit has been imported from sklearn.model_selection.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a TimeSeriesSplit object with 3 splits.\n",
        "Sort the train data by \"date\" column to apply time K-fold.\n",
        "Loop over each time split using time_kfold object.\n",
        "For each split select training and testing folds using train_index and test_index."
      ],
      "metadata": {
        "id": "uIgACdKs4cRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TimeSeriesSplit object\n",
        "time_kfold = TimeSeriesSplit(n_splits=3)\n",
        "\n",
        "# Sort train data by date\n",
        "train = train.sort_values('date')\n",
        "\n",
        "# Iterate through each split\n",
        "fold = 0\n",
        "for train_index, test_index in time_kfold.split(train):\n",
        "    cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
        "\n",
        "    print('Fold :', fold)\n",
        "    print('Train date range: from {} to {}'.format(cv_train.date.min(), cv_train.date.max()))\n",
        "    print('Test date range: from {} to {}\\n'.format(cv_test.date.min(), cv_test.date.max()))\n",
        "    fold += 1"
      ],
      "metadata": {
        "id": "neiiT6P04cfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overall validation score\n",
        "Now it's time to get the actual model performance using cross-validation! How does our store item demand prediction model perform?\n",
        "\n",
        "Your task is to take the Mean Squared Error (MSE) for each fold separately, and then combine these results into a single number.\n",
        "\n",
        "For simplicity, you're given get_fold_mse() function that for each cross-validation split fits a Random Forest model and returns a list of MSE scores by fold. get_fold_mse() accepts two arguments: train and TimeSeriesSplit object.\n",
        "\n",
        "Instructions 1/3\n",
        "35 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "Create time 3-fold cross-validation.\n",
        "Print the numpy mean of MSE scores by folds."
      ],
      "metadata": {
        "id": "s2sX1MW94ihk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import numpy as np\n",
        "\n",
        "# Sort train data by date\n",
        "train = train.sort_values('date')\n",
        "\n",
        "# Initialize 3-fold time cross-validation\n",
        "kf = TimeSeriesSplit(n_splits=3)\n",
        "\n",
        "# Get MSE scores for each cross-validation split\n",
        "mse_scores = get_fold_mse(train, kf)\n",
        "\n",
        "print('Mean validation MSE: {:.5f}'.format(np.mean(mse_scores)))"
      ],
      "metadata": {
        "id": "vSI2VOFD4i4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/3\n",
        "35 XP\n",
        "2\n",
        "3\n",
        "Print the list of MSEs by fold."
      ],
      "metadata": {
        "id": "U_x83mhv4may"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import numpy as np\n",
        "\n",
        "# Sort train data by date\n",
        "train = train.sort_values('date')\n",
        "\n",
        "# Initialize 3-fold time cross-validation\n",
        "kf = TimeSeriesSplit(n_splits=3)\n",
        "\n",
        "# Get MSE scores for each cross-validation split\n",
        "mse_scores = get_fold_mse(train, kf)\n",
        "\n",
        "print('Mean validation MSE: {:.5f}'.format(np.mean(mse_scores)))\n",
        "print('MSE by fold: {}'.format(mse_scores))"
      ],
      "metadata": {
        "id": "L0J54dfu4msn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/3\n",
        "30 XP\n",
        "3\n",
        "To calculate the overall score, find the sum of MSE mean and standard deviation."
      ],
      "metadata": {
        "id": "jYDi01q14p-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "import numpy as np\n",
        "\n",
        "# Sort train data by date\n",
        "train = train.sort_values('date')\n",
        "\n",
        "# Initialize 3-fold time cross-validation\n",
        "kf = TimeSeriesSplit(n_splits=3)\n",
        "\n",
        "# Get MSE scores for each cross-validation split\n",
        "mse_scores = get_fold_mse(train, kf)\n",
        "\n",
        "print('Mean validation MSE: {:.5f}'.format(np.mean(mse_scores)))\n",
        "print('MSE by fold: {}'.format(mse_scores))\n",
        "print('Overall validation MSE: {:.5f}'.format(np.mean(mse_scores) + np.std(mse_scores)))"
      ],
      "metadata": {
        "id": "o17gYfrx4qM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Arithmetical features\n",
        "To practice creating new features, you will be working with a subsample from the Kaggle competition called \"House Prices: Advanced Regression Techniques\". The goal of this competition is to predict the price of the house based on its properties. It's a regression problem with Root Mean Squared Error as an evaluation metric.\n",
        "\n",
        "Your goal is to create new features and determine whether they improve your validation score. To get the validation score from 5-fold cross-validation, you're given the get_kfold_rmse() function. Use it with the train DataFrame, available in your workspace, as an argument.\n",
        "\n",
        "Instructions 1/3\n",
        "35 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "Create a new feature representing the total area (basement, 1st and 2nd floors) of the house. The columns \"TotalBsmtSF\", \"FirstFlrSF\" and \"SecondFlrSF\" give the areas of the basement, 1st and 2nd floors, respectively."
      ],
      "metadata": {
        "id": "C09IpE9T4wYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at the initial RMSE\n",
        "print('RMSE before feature engineering:', get_kfold_rmse(train))\n",
        "\n",
        "# Find the total area of the house\n",
        "train['TotalArea'] = train['TotalBsmtSF'] + train['FirstFlrSF'] + train['SecondFlrSF']\n",
        "\n",
        "# Look at the updated RMSE\n",
        "print('RMSE with total area:', get_kfold_rmse(train))"
      ],
      "metadata": {
        "id": "SSSJ8X0c4wnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/3\n",
        "35 XP\n",
        "2\n",
        "3\n",
        "Create a new feature representing the area of the garden. It is a difference between the total area of the property (\"LotArea\") and the first floor area (\"FirstFlrSF\")."
      ],
      "metadata": {
        "id": "hQVlfrlp41IA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at the initial RMSE\n",
        "print('RMSE before feature engineering:', get_kfold_rmse(train))\n",
        "\n",
        "# Find the total area of the house\n",
        "train['TotalArea'] = train['TotalBsmtSF'] + train['FirstFlrSF'] + train['SecondFlrSF']\n",
        "print('RMSE with total area:', get_kfold_rmse(train))\n",
        "\n",
        "# Find the area of the garden\n",
        "train['GardenArea'] = train['LotArea'] - train['FirstFlrSF']\n",
        "print('RMSE with garden area:', get_kfold_rmse(train))"
      ],
      "metadata": {
        "id": "pjOQXfHj41QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/3\n",
        "30 XP\n",
        "3\n",
        "Create a new feature representing the total number of bathrooms in the house. It is a sum of full bathrooms (\"FullBath\") and half bathrooms (\"HalfBath\")."
      ],
      "metadata": {
        "id": "nUffnTl944sS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at the initial RMSE\n",
        "print('RMSE before feature engineering:', get_kfold_rmse(train))\n",
        "\n",
        "# Find the total area of the house\n",
        "train['TotalArea'] = train['TotalBsmtSF'] + train['FirstFlrSF'] + train['SecondFlrSF']\n",
        "print('RMSE with total area:', get_kfold_rmse(train))\n",
        "\n",
        "# Find the area of the garden\n",
        "train['GardenArea'] = train['LotArea'] - train['FirstFlrSF']\n",
        "print('RMSE with garden area:', get_kfold_rmse(train))\n",
        "\n",
        "# Find the total number of bathrooms\n",
        "train['TotalBath'] = train['FullBath'] + train['HalfBath']\n",
        "print('RMSE with number of bathrooms:', get_kfold_rmse(train))"
      ],
      "metadata": {
        "id": "3MAo2I0z440V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Date features\n",
        "You've built some basic features using numerical variables. Now, it's time to create features based on date and time. You will practice on a subsample from the Taxi Fare Prediction Kaggle competition data. The data represents information about the taxi rides and the goal is to predict the price for each ride.\n",
        "\n",
        "Your objective is to generate date features from the pickup datetime. Recall that it's better to create new features for train and test data simultaneously. After the features are created, split the data back into the train and test DataFrames. Here it's done using pandas' isin() method.\n",
        "\n",
        "The train and test DataFrames are already available in your workspace.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Concatenate the train and test DataFrames into a single DataFrame taxi.\n",
        "Convert the \"pickup_datetime\" column to a datetime object.\n",
        "Create the day of week (using .dayofweek attribute) and hour (using .hour attribute) features from the \"pickup_datetime\" column."
      ],
      "metadata": {
        "id": "f46YrWPe49jf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate train and test together\n",
        "taxi = pd.concat([train, test])\n",
        "\n",
        "# Convert pickup date to datetime object\n",
        "taxi['pickup_datetime'] = pd.to_datetime(taxi['pickup_datetime'])\n",
        "\n",
        "# Create a day of week feature\n",
        "taxi['dayofweek'] = taxi['pickup_datetime'].dt.dayofweek\n",
        "\n",
        "# Create an hour feature\n",
        "taxi['hour'] = taxi['pickup_datetime'].dt.hour\n",
        "\n",
        "# Split back into train and test\n",
        "new_train = taxi[taxi['id'].isin(train['id'])]\n",
        "new_test = taxi[taxi['id'].isin(test['id'])]"
      ],
      "metadata": {
        "id": "HtRESbM049xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Label encoding\n",
        "Let's work on categorical variables encoding. You will again work with a subsample from the House Prices Kaggle competition.\n",
        "\n",
        "Your objective is to encode categorical features \"RoofStyle\" and \"CentralAir\" using label encoding. The train and test DataFrames are already available in your workspace.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Concatenate train and test DataFrames into a single DataFrame houses.\n",
        "Create a LabelEncoder object without arguments and assign it to le.\n",
        "Create new label-encoded features for \"RoofStyle\" and \"CentralAir\" using the same le object."
      ],
      "metadata": {
        "id": "jT8js66C5Hds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate train and test together\n",
        "houses = pd.concat([train, test])\n",
        "\n",
        "# Label encoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Create new features\n",
        "houses['RoofStyle_enc'] = le.fit_transform(houses['RoofStyle'])\n",
        "houses['CentralAir_enc'] = le.fit_transform(houses['CentralAir'])\n",
        "\n",
        "# Look at new features\n",
        "print(houses[['RoofStyle', 'RoofStyle_enc', 'CentralAir', 'CentralAir_enc']].head())"
      ],
      "metadata": {
        "id": "ldGIGk2i5Hst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-Hot encoding\n",
        "The problem with label encoding is that it implicitly assumes that there is a ranking dependency between the categories. So, let's change the encoding method for the features \"RoofStyle\" and \"CentralAir\" to one-hot encoding. Again, the train and test DataFrames from House Prices Kaggle competition are already available in your workspace.\n",
        "\n",
        "Recall that if you're dealing with binary features (categorical features with only two categories) it is suggested to apply label encoder only.\n",
        "\n",
        "Your goal is to determine which of the mentioned features is not binary, and to apply one-hot encoding only to this one.\n",
        "\n",
        "Instructions 1/4\n",
        "25 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "4\n",
        "Determine the distribution of \"RoofStyle\" and \"CentralAir\" features using pandas' value_counts() method."
      ],
      "metadata": {
        "id": "0fY16n-H5UNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate train and test together\n",
        "houses = pd.concat([train, test])\n",
        "\n",
        "# Look at feature distributions\n",
        "print(houses['RoofStyle'].value_counts(), '\\n')\n",
        "print(houses['CentralAir'].value_counts())"
      ],
      "metadata": {
        "id": "2_khA1g25UhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/4\n",
        "25 XP\n",
        "3\n",
        "4\n",
        "Question\n",
        "Which of the features is binary?\n",
        "Possible answers\n",
        "\n",
        "\n",
        "\"RoofStyle\".\n",
        "\n",
        "***\"CentralAir\".***\n",
        "\n"
      ],
      "metadata": {
        "id": "ZtHU3IAV5a_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/4\n",
        "25 XP\n",
        "3\n",
        "4\n",
        "As long as \"CentralAir\" is a binary feature, encode it with a label encoder (0 - for one class and 1 - for another class)."
      ],
      "metadata": {
        "id": "7mawPjDs5fHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate train and test together\n",
        "houses = pd.concat([train, test])\n",
        "\n",
        "# Label encode binary 'CentralAir' feature\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "houses['CentralAir_enc'] = le.fit_transform(houses['CentralAir'])"
      ],
      "metadata": {
        "id": "i2lbHT615cyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 4/4\n",
        "25 XP\n",
        "4\n",
        "For the categorical feature \"RoofStyle\" let's use the one-hot encoder. Firstly, create one-hot encoded features using the get_dummies() method. Then they are concatenated to the initial houses DataFrame."
      ],
      "metadata": {
        "id": "RJQSrf7z5kdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate train and test together\n",
        "houses = pd.concat([train, test])\n",
        "\n",
        "# Label encode binary 'CentralAir' feature\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "houses['CentralAir_enc'] = le.fit_transform(houses['CentralAir'])\n",
        "\n",
        "# Create One-Hot encoded features\n",
        "ohe = pd.get_dummies(houses['RoofStyle'], prefix='RoofStyle')\n",
        "\n",
        "# Concatenate OHE features to houses\n",
        "houses = pd.concat([houses, ohe], axis=1)\n",
        "\n",
        "# Look at OHE features\n",
        "print(houses[[col for col in houses.columns if 'RoofStyle' in col]].head(3))"
      ],
      "metadata": {
        "id": "h0oBubH85k0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mean target encoding\n",
        "First of all, you will create a function that implements mean target encoding. Remember that you need to develop the two following steps:\n",
        "\n",
        "Calculate the mean on the train, apply to the test\n",
        "Split train into K folds. Calculate the out-of-fold mean for each fold, apply to this particular fold\n",
        "Each of these steps will be implemented in a separate function: test_mean_target_encoding() and train_mean_target_encoding(), respectively.\n",
        "\n",
        "The final function mean_target_encoding() takes as arguments: the train and test DataFrames, the name of the categorical column to be encoded, the name of the target column and a smoothing parameter alpha. It returns two values: a new feature for train and test DataFrames, respectively.\n",
        "\n",
        "Instructions 1/3\n",
        "35 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "You need to add smoothing to avoid overfitting. So, add\n",
        " parameter to the denominator in train_statistics calculations.\n",
        "You need to treat new categories in the test data. So, pass a global mean as an argument to the fillna() method."
      ],
      "metadata": {
        "id": "_SdnK4cr5p24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_mean_target_encoding(train, test, target, categorical, alpha=5):\n",
        "    # Calculate global mean on the train data\n",
        "    global_mean = train[target].mean()\n",
        "\n",
        "    # Group by the categorical feature and calculate its properties\n",
        "    train_groups = train.groupby(categorical)\n",
        "    category_sum = train_groups[target].sum()\n",
        "    category_size = train_groups.size()\n",
        "\n",
        "    # Calculate smoothed mean target statistics\n",
        "    train_statistics = (category_sum + global_mean * alpha) / (category_size + alpha)\n",
        "\n",
        "    # Apply statistics to the test data and fill new categories\n",
        "    test_feature = test[categorical].map(train_statistics).fillna(global_mean)\n",
        "    return test_feature.values"
      ],
      "metadata": {
        "id": "CC2uTL2T5qLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/3\n",
        "35 XP\n",
        "2\n",
        "3\n",
        "To calculate the train mean encoded feature you need to use out-of-fold statistics, splitting train into several folds. Specify the train and test indices for each validation split to access it."
      ],
      "metadata": {
        "id": "1M5ePAiL5v0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_mean_target_encoding(train, target, categorical, alpha=5):\n",
        "    # Create 5-fold cross-validation\n",
        "    kf = KFold(n_splits=5, random_state=123, shuffle=True)\n",
        "    train_feature = pd.Series(index=train.index)\n",
        "\n",
        "    # For each folds split\n",
        "    for train_index, test_index in kf.split(train):\n",
        "        cv_train, cv_test = train.iloc[train_index], train.iloc[test_index]\n",
        "\n",
        "        # Calculate out-of-fold statistics and apply to cv_test\n",
        "        cv_test_feature = test_mean_target_encoding(cv_train, cv_test, target, categorical, alpha)\n",
        "\n",
        "        # Save new feature for this particular fold\n",
        "        train_feature.iloc[test_index] = cv_test_feature\n",
        "    return train_feature.values"
      ],
      "metadata": {
        "id": "eGhjFow75v8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/3\n",
        "30 XP\n",
        "3\n",
        "Finally, you just calculate train and test target mean encoded features and return them from the function. So, return train_feature and test_feature obtained."
      ],
      "metadata": {
        "id": "-E8hlC-r5zFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_target_encoding(train, test, target, categorical, alpha=5):\n",
        "\n",
        "    # Get the train feature\n",
        "    train_feature = train_mean_target_encoding(train, target, categorical, alpha)\n",
        "\n",
        "    # Get the test feature\n",
        "    test_feature = test_mean_target_encoding(train, test, target, categorical, alpha)\n",
        "\n",
        "    # Return new features to add to the model\n",
        "    return train_feature, test_feature"
      ],
      "metadata": {
        "id": "OVKITdBt5zOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-fold cross-validation\n",
        "You will work with a binary classification problem on a subsample from Kaggle playground competition. The objective of this competition is to predict whether a famous basketball player Kobe Bryant scored a basket or missed a particular shot.\n",
        "\n",
        "Train data is available in your workspace as bryant_shots DataFrame. It contains data on 10,000 shots with its properties and a target variable \"shot\\_made\\_flag\" -- whether shot was scored or not.\n",
        "\n",
        "One of the features in the data is \"game_id\" -- a particular game where the shot was made. There are 541 distinct games. So, you deal with a high-cardinality categorical feature. Let's encode it using a target mean!\n",
        "\n",
        "Suppose you're using 5-fold cross-validation and want to evaluate a mean target encoded feature on the local validation.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "To achieve this, you need to repeat encoding procedure for the \"game_id\" categorical feature inside each folds split separately. Your goal is to specify all the missing parameters for the mean_target_encoding() function call inside each folds split.\n",
        "Recall that the train and test parameters expect the train and test DataFrames.\n",
        "While the target and categorical parameters expect names of the target variable and categorical feature to be encoded.\n"
      ],
      "metadata": {
        "id": "YuVeZczN53zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create 5-fold cross-validation\n",
        "kf = KFold(n_splits=5, random_state=123, shuffle=True)\n",
        "\n",
        "# For each folds split\n",
        "for train_index, test_index in kf.split(bryant_shots):\n",
        "    cv_train, cv_test = bryant_shots.iloc[train_index], bryant_shots.iloc[test_index]\n",
        "\n",
        "    # Create mean target encoded feature\n",
        "    cv_train['game_id_enc'], cv_test['game_id_enc'] = mean_target_encoding(train=cv_train,\n",
        "                                                                           test=cv_test,\n",
        "                                                                           target='shot_made_flag',\n",
        "                                                                           categorical='game_id',\n",
        "                                                                           alpha=5)\n",
        "    # Look at the encoding\n",
        "    print(cv_train[['game_id', 'shot_made_flag', 'game_id_enc']].sample(n=1))"
      ],
      "metadata": {
        "id": "2Ch7XO5_54Br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beyond binary classification\n",
        "Of course, binary classification is just a single special case. Target encoding could be applied to any target variable type:\n",
        "\n",
        "For binary classification usually mean target encoding is used\n",
        "For regression mean could be changed to median, quartiles, etc.\n",
        "For multi-class classification with N classes we create N features with target mean for each category in one vs. all fashion\n",
        "The mean_target_encoding() function you've created could be used for any target type specified above. Let's apply it for the regression problem on the example of House Prices Kaggle competition.\n",
        "\n",
        "Your goal is to encode a categorical feature \"RoofStyle\" using mean target encoding. The train and test DataFrames are already available in your workspace.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Specify all the missing parameters for the mean_target_encoding() function call. Target variable name is \"SalePrice\". Set\n",
        " hyperparameter to 10.\n",
        "Recall that the train and test parameters expect the train and test DataFrames.\n",
        "While the target and categorical parameters expect names of the target variable and feature to be encoded."
      ],
      "metadata": {
        "id": "ACdYPJuj6RA7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create mean target encoded feature\n",
        "train['RoofStyle_enc'], test['RoofStyle_enc'] = mean_target_encoding(train=train,\n",
        "                                                                     test=test,\n",
        "                                                                     target='SalePrice',\n",
        "                                                                     categorical='RoofStyle',\n",
        "                                                                     alpha=10)\n",
        "\n",
        "# Look at the encoding\n",
        "print(test[['RoofStyle', 'RoofStyle_enc']].drop_duplicates())"
      ],
      "metadata": {
        "id": "yjB8KIzp6RRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find missing data\n",
        "Let's impute missing data on a real Kaggle dataset. For this purpose, you will be using a data subsample from the Kaggle \"Two sigma connect: rental listing inquiries\" competition.\n",
        "\n",
        "Before proceeding with any imputing you need to know the number of missing values for each of the features. Moreover, if the feature has missing values, you should explore the type of this feature.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "2\n",
        "Read the \"twosigma_train.csv\" file using pandas.\n",
        "Find the number of missing values in each column."
      ],
      "metadata": {
        "id": "M1704_XY6YO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read dataframe\n",
        "twosigma = pd.read_csv('twosigma_train.csv')\n",
        "\n",
        "# Find the number of missing values in each column\n",
        "print(twosigma.isnull().sum())"
      ],
      "metadata": {
        "id": "TmgPNyAr6Yfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "50 XP\n",
        "2\n",
        "Select the columns with the missing values and look at the head of the DataFrame."
      ],
      "metadata": {
        "id": "e_ebhi686dwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read DataFrame\n",
        "twosigma = pd.read_csv('twosigma_train.csv')\n",
        "\n",
        "# Find the number of missing values in each column\n",
        "print(twosigma.isnull().sum())\n",
        "\n",
        "# Look at the columns with the missing values\n",
        "print(twosigma[['building_id', 'price']].head())"
      ],
      "metadata": {
        "id": "mBmdS_z06eC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Impute missing data\n",
        "You've found that \"price\" and \"building_id\" columns have missing values in the Rental Listing Inquiries dataset. So, before passing the data to the models you need to impute these values.\n",
        "\n",
        "Numerical feature \"price\" will be encoded with a mean value of non-missing prices.\n",
        "\n",
        "Imputing categorical feature \"building_id\" with the most frequent category is a bad idea, because it would mean that all the apartments with a missing \"building_id\" are located in the most popular building. The better idea is to impute it with a new category.\n",
        "\n",
        "The DataFrame rental_listings with competition data is read for you.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "Create a SimpleImputer object with \"mean\" strategy.\n",
        "Impute missing prices with the mean value."
      ],
      "metadata": {
        "id": "iIoO6KHn6hcB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import SimpleImputer\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Create mean imputer\n",
        "mean_imputer = SimpleImputer(strategy='mean')\n",
        "\n",
        "# Price imputation\n",
        "rental_listings[['price']] = mean_imputer.fit_transform(rental_listings[['price']])"
      ],
      "metadata": {
        "id": "EZKAt1vL6h5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "\n",
        "\n",
        "Create an imputer with \"constant\" strategy. Use \"MISSING\" as fill_value.\n",
        "Impute missing buildings with a constant value.\n"
      ],
      "metadata": {
        "id": "emDpeJ6A6lUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import SimpleImputer\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Create constant imputer\n",
        "constant_imputer = SimpleImputer(strategy='constant', fill_value='MISSING')\n",
        "\n",
        "# building_id imputation\n",
        "rental_listings[['building_id']] = constant_imputer.fit_transform(rental_listings[['building_id']])"
      ],
      "metadata": {
        "id": "sKeeASH36nTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replicate validation score\n",
        "You've seen both validation and Public Leaderboard scores in the video. However, the code examples are available only for the test data. To get the validation scores you have to repeat the same process on the holdout set.\n",
        "\n",
        "Throughout this chapter, you will work with New York City Taxi competition data. The problem is to predict the fare amount for a taxi ride in New York City. The competition metric is the root mean squared error.\n",
        "\n",
        "The first goal is to evaluate the Baseline model on the validation data. You will replicate the simplest Baseline based on the mean of \"fare_amount\". Recall that as a validation strategy we used a 30% holdout split with validation_train as train and validation_test as holdout DataFrames. Both of them are available in your workspace.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Calculate the mean of \"fare_amount\" over the whole validation_train DataFrame.\n",
        "Assign this naive prediction value to all the holdout predictions. Store them in the \"pred\" column."
      ],
      "metadata": {
        "id": "xfMCea7o6tWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "# Calculate the mean fare_amount on the validation_train data\n",
        "naive_prediction = np.mean(validation_train['fare_amount'])\n",
        "\n",
        "# Assign naive prediction to all the holdout observations\n",
        "validation_test['pred'] = naive_prediction\n",
        "\n",
        "# Measure the local RMSE\n",
        "rmse = sqrt(mean_squared_error(validation_test['fare_amount'], validation_test['pred']))\n",
        "print('Validation RMSE for Baseline I model: {:.3f}'.format(rmse))"
      ],
      "metadata": {
        "id": "rZzVoxhg6tmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline based on the date\n",
        "We've already built 3 different baseline models. To get more practice, let's build a couple more. The first model is based on the grouping variables. It's clear that the ride fare could depend on the part of the day. For example, prices could be higher during the rush hours.\n",
        "\n",
        "Your goal is to build a baseline model that will assign the average \"fare_amount\" for the corresponding hour. For now, you will create the model for the whole train data and make predictions for the test dataset.\n",
        "\n",
        "The train and test DataFrames are available in your workspace. Moreover, the \"pickup_datetime\" column in both DataFrames is already converted to a datetime object for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Get the hour from the \"pickup_datetime\" column for the train and test DataFrames.\n",
        "Calculate the mean \"fare_amount\" for each hour on the train data.\n",
        "Make test predictions using pandas' map() method and the grouping obtained.\n",
        "Write predictions to the file."
      ],
      "metadata": {
        "id": "BKXOqNH56zRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get pickup hour from the pickup_datetime column\n",
        "train['hour'] = train['pickup_datetime'].dt.hour\n",
        "test['hour'] = test['pickup_datetime'].dt.hour\n",
        "\n",
        "# Calculate average fare_amount grouped by pickup hour\n",
        "hour_groups = train.groupby('hour')['fare_amount'].mean()\n",
        "\n",
        "# Make predictions on the test set\n",
        "test['fare_amount'] = test.hour.map(hour_groups)\n",
        "\n",
        "# Write predictions\n",
        "test[['id','fare_amount']].to_csv('hour_mean_sub.csv', index=False)"
      ],
      "metadata": {
        "id": "oMxxgEEm6zkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline based on the gradient boosting\n",
        "Let's build a final baseline based on the Random Forest. You've seen a huge score improvement moving from the grouping baseline to the Gradient Boosting in the video. Now, you will use sklearn's Random Forest to further improve this score.\n",
        "\n",
        "The goal of this exercise is to take numeric features and train a Random Forest model without any tuning. After that, you could make test predictions and validate the result on the Public Leaderboard. Note that you've already got an \"hour\" feature which could also be used as an input to the model.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Add the \"hour\" feature to the list of numeric features.\n",
        "Fit the RandomForestRegressor on the train data with numeric features and \"fare_amount\" as a target.\n",
        "Use the trained Random Forest model to make predictions on the test data."
      ],
      "metadata": {
        "id": "wJKwfidu64xi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Select only numeric features\n",
        "features = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude',\n",
        "            'dropoff_latitude', 'passenger_count', 'hour']\n",
        "\n",
        "# Train a Random Forest model\n",
        "rf = RandomForestRegressor()\n",
        "rf.fit(train[features], train.fare_amount)\n",
        "\n",
        "# Make predictions on the test data\n",
        "test['fare_amount'] = rf.predict(test[features])\n",
        "\n",
        "# Write predictions\n",
        "test[['id','fare_amount']].to_csv('rf_sub.csv', index=False)"
      ],
      "metadata": {
        "id": "nP7ILYHW65B7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grid search\n",
        "Recall that we've created a baseline Gradient Boosting model in the previous lesson. Your goal now is to find the best max_depth hyperparameter value for this Gradient Boosting model. This hyperparameter limits the number of nodes in each individual tree. You will be using K-fold cross-validation to measure the local performance of the model for each hyperparameter value.\n",
        "\n",
        "You're given a function get_cv_score(), which takes the train dataset and dictionary of the model parameters as arguments and returns the overall validation RMSE score over 3-fold cross-validation.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Specify the grid for possible max_depth values with 3, 6, 9, 12 and 15.\n",
        "Pass each hyperparameter candidate in the grid to the model params dictionary."
      ],
      "metadata": {
        "id": "GawshLGZ7Cxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Possible max depth values\n",
        "max_depth_grid = [3, 6, 9, 12, 15]\n",
        "results = {}\n",
        "\n",
        "# For each value in the grid\n",
        "for max_depth_candidate in max_depth_grid:\n",
        "    # Specify parameters for the model\n",
        "    params = {'max_depth': max_depth_candidate}\n",
        "\n",
        "    # Calculate validation score for a particular hyperparameter\n",
        "    validation_score = get_cv_score(train, params)\n",
        "\n",
        "    # Save the results for each max depth value\n",
        "    results[max_depth_candidate] = validation_score\n",
        "print(results)"
      ],
      "metadata": {
        "id": "KCB83rQe7DAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2D grid search\n",
        "The drawback of tuning each hyperparameter independently is a potential dependency between different hyperparameters. The better approach is to try all the possible hyperparameter combinations. However, in such cases, the grid search space is rapidly expanding. For example, if we have 2 parameters with 10 possible values, it will yield 100 experiment runs.\n",
        "\n",
        "Your goal is to find the best hyperparameter couple of max_depth and subsample for the Gradient Boosting model. subsample is a fraction of observations to be used for fitting the individual trees.\n",
        "\n",
        "You're given a function get_cv_score(), which takes the train dataset and dictionary of the model parameters as arguments and returns the overall validation RMSE score over 3-fold cross-validation.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Specify the grids for possible max_depth and subsample values. For max_depth: 3, 5 and 7. For subsample: 0.8, 0.9 and 1.0.\n",
        "Apply the product() function from the itertools package to the hyperparameter grids. It returns all possible combinations for these two grids.\n",
        "Pass each hyperparameters candidate couple to the model params dictionary."
      ],
      "metadata": {
        "id": "Q3syPra07JMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "# Hyperparameter grids\n",
        "max_depth_grid = [3, 5, 7]\n",
        "subsample_grid = [0.8, 0.9, 1.0]\n",
        "results = {}\n",
        "\n",
        "# For each couple in the grid\n",
        "for max_depth_candidate, subsample_candidate in itertools.product(max_depth_grid, subsample_grid):\n",
        "    params = {'max_depth': max_depth_candidate,\n",
        "              'subsample': subsample_candidate}\n",
        "    validation_score = get_cv_score(train, params)\n",
        "    # Save the results for each couple\n",
        "    results[(max_depth_candidate, subsample_candidate)] = validation_score\n",
        "print(results)"
      ],
      "metadata": {
        "id": "UNspMv5o7JkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model blending\n",
        "You will start creating model ensembles with a blending technique.\n",
        "\n",
        "Your goal is to train 2 different models on the New York City Taxi competition data. Make predictions on the test data and then blend them using a simple arithmetic mean.\n",
        "\n",
        "The train and test DataFrames are already available in your workspace. features is a list of columns to be used for training and it is also available in your workspace. The target variable name is \"fare_amount\".\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Train a Gradient Boosting model on the train data using features list, and the \"fare_amount\" column as a target variable.\n",
        "Train a Random Forest model in the same manner.\n",
        "Make predictions on the test data using both Gradient Boosting and Random Forest models.\n",
        "Find the average of both models predictions."
      ],
      "metadata": {
        "id": "spj-j4hz7S9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "\n",
        "# Train a Gradient Boosting model\n",
        "gb = GradientBoostingRegressor().fit(train[features], train.fare_amount)\n",
        "\n",
        "# Train a Random Forest model\n",
        "rf = RandomForestRegressor().fit(train[features], train.fare_amount)\n",
        "\n",
        "# Make predictions on the test data\n",
        "test['gb_pred'] = gb.predict(test[features])\n",
        "test['rf_pred'] = rf.predict(test[features])\n",
        "\n",
        "# Find mean of model predictions\n",
        "test['blend'] = (test['gb_pred'] + test['rf_pred']) / 2\n",
        "print(test[['gb_pred', 'rf_pred', 'blend']].head(3))"
      ],
      "metadata": {
        "id": "urkrwGUj7TOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model stacking I\n",
        "Now it's time for stacking. To implement the stacking approach, you will follow the 6 steps we've discussed in the previous video:\n",
        "\n",
        "Split train data into two parts\n",
        "Train multiple models on Part 1\n",
        "Make predictions on Part 2\n",
        "Make predictions on the test data\n",
        "Train a new model on Part 2 using predictions as features\n",
        "Make predictions on the test data using the 2nd level model\n",
        "train and test DataFrames are already available in your workspace. features is a list of columns to be used for training on the Part 1 data and it is also available in your workspace. Target variable name is \"fare_amount\".\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "2\n",
        "Split the train DataFrame into two equal parts: part_1 and part_2. Use the train_test_split() function with test_size equal to 0.5.\n",
        "Train Gradient Boosting and Random Forest models on the part_1 data."
      ],
      "metadata": {
        "id": "L6qHFv197ZEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
        "\n",
        "# Split train data into two parts\n",
        "part_1, part_2 = train_test_split(train, test_size=0.5, random_state=123)\n",
        "\n",
        "# Train a Gradient Boosting model\n",
        "gb = GradientBoostingRegressor().fit(part_1[features], part_1.fare_amount)\n",
        "\n",
        "# Train a Random Forest model on Part 1\n",
        "rf = RandomForestRegressor().fit(part_1[features], part_1.fare_amount)"
      ],
      "metadata": {
        "id": "fek2UAgH7ZU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "50 XP\n",
        "2\n",
        "Make Gradient Boosting and Random Forest predictions on the part_2 data.\n",
        "Make Gradient Boosting and Random Forest predictions on the test data.\n"
      ],
      "metadata": {
        "id": "rJZy6yKi7eS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the Part 2 data\n",
        "part_2['gb_pred'] = gb.predict(part_2[features])\n",
        "part_2['rf_pred'] = rf.predict(part_2[features])\n",
        "\n",
        "# Make predictions on the test data\n",
        "test['gb_pred'] = gb.predict(test[features])\n",
        "test['rf_pred'] = rf.predict(test[features])"
      ],
      "metadata": {
        "id": "Vu2a8A4A7ebZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model stacking II\n",
        "OK, what you've done so far in the stacking implementation:\n",
        "\n",
        "Split train data into two parts\n",
        "Train multiple models on Part 1\n",
        "Make predictions on Part 2\n",
        "Make predictions on the test data\n",
        "Now, your goal is to create a second level model using predictions from steps 3 and 4 as features. So, this model is trained on Part 2 data and then you can make stacking predictions on the test data.\n",
        "\n",
        "part_2 and test DataFrames are already available in your workspace. Gradient Boosting and Random Forest predictions are stored in these DataFrames under the names \"gb_pred\" and \"rf_pred\", respectively.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Train a Linear Regression model on the Part 2 data using Gradient Boosting and Random Forest models predictions as features.\n",
        "Make predictions on the test data using Gradient Boosting and Random Forest models predictions as features."
      ],
      "metadata": {
        "id": "jvUp1mpo7imZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Create linear regression model without the intercept\n",
        "lr = LinearRegression(fit_intercept=False)\n",
        "\n",
        "# Train 2nd level model on the Part 2 data\n",
        "lr.fit(part_2[['gb_pred', 'rf_pred']], part_2.fare_amount)\n",
        "\n",
        "# Make stacking predictions on the test data\n",
        "test['stacking'] = lr.predict(test[['gb_pred', 'rf_pred']])\n",
        "\n",
        "# Look at the model coefficients\n",
        "print(lr.coef_)"
      ],
      "metadata": {
        "id": "9uhmTnA-7i7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Kaggle forum ideas\n",
        "Unfortunately, not all the Forum posts and Kernels are necessarily useful for your model. So instead of blindly incorporating ideas into your pipeline, you should test them first.\n",
        "\n",
        "You're given a function get_cv_score(), which takes a train dataset as an argument and returns the overall validation root mean squared error over 3-fold cross-validation. The train DataFrame is already available in your workspace.\n",
        "\n",
        "You should try different suggestions from the Kaggle Forum and check whether they improve your validation score.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "Suggestion 1: the passenger_count feature is useless. Let's see! Drop this feature and compare the scores."
      ],
      "metadata": {
        "id": "CrIGxlwy8FHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop passenger_count column\n",
        "new_train_1 = train.drop('passenger_count', axis=1)\n",
        "\n",
        "# Compare validation scores\n",
        "initial_score = get_cv_score(train)\n",
        "new_score = get_cv_score(new_train_1)\n",
        "\n",
        "print('Initial score is {} and the new score is {}'.format(initial_score, new_score))"
      ],
      "metadata": {
        "id": "jPU9lTyW8FW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "\n",
        "This first suggestion worked. Suggestion 2: Sum of pickup_latitude and distance_km is a good feature. Let's try it!"
      ],
      "metadata": {
        "id": "7onS-NST8JtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create copy of the initial train DataFrame\n",
        "new_train_2 = train.copy()\n",
        "\n",
        "# Find sum of pickup latitude and ride distance\n",
        "new_train_2['weird_feature'] = new_train_2['pickup_latitude'] + new_train_2['distance_km']\n",
        "\n",
        "# Compare validation scores\n",
        "initial_score = get_cv_score(train)\n",
        "new_score = get_cv_score(new_train_2)\n",
        "\n",
        "print('Initial score is {} and the new score is {}'.format(initial_score, new_score))"
      ],
      "metadata": {
        "id": "Kfzb6ok48Kno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Module End ---"
      ],
      "metadata": {
        "id": "xGhmIMvi8zqg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Track End ---"
      ],
      "metadata": {
        "id": "OQMSwUAa82cG"
      }
    }
  ]
}