{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfsgTgDgkuhbID0TP+BMmN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pattiecodes/DC_MLScientist_in_Py/blob/main/Module_15_Feature_Engineering_for_NLP_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Module Start ---"
      ],
      "metadata": {
        "id": "WkWSWGcolxt3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-hot encoding\n",
        "In the previous exercise, we encountered a dataframe df1 which contained categorical features and therefore, was unsuitable for applying ML algorithms to.\n",
        "\n",
        "In this exercise, your task is to convert df1 into a format that is suitable for machine learning.\n",
        "\n",
        "Instructions 1/3\n",
        "35 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "Use the columns attribute to print the features of df1."
      ],
      "metadata": {
        "id": "2SjFb5w-m3UU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxT4DvKIhVzJ"
      },
      "outputs": [],
      "source": [
        "# Print the features of df1\n",
        "print(df1.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/3\n",
        "35 XP\n",
        "2\n",
        "3\n",
        "Use the pd.get_dummies() function to perform one-hot encoding on feature 5 of df1."
      ],
      "metadata": {
        "id": "LDcvthCQm8vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the features of df1\n",
        "print(df1.columns)\n",
        "\n",
        "# Perform one-hot encoding\n",
        "df1 = pd.get_dummies(df1, columns=['feature 5'])"
      ],
      "metadata": {
        "id": "SSio5clpm9V_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/3\n",
        "30 XP\n",
        "3\n",
        "Use the columns attribute again to print the new features of df1.\n",
        "Print the first five rows of df1 using head()."
      ],
      "metadata": {
        "id": "-46heGkanF3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the features of df1\n",
        "print(df1.columns)\n",
        "\n",
        "# Perform one-hot encoding\n",
        "df1 = pd.get_dummies(df1, columns=['feature 5'])\n",
        "\n",
        "# Print the new features of df1\n",
        "print(df1.columns)\n",
        "\n",
        "# Print first five rows of df1\n",
        "print(df1.head())"
      ],
      "metadata": {
        "id": "w77ApECInGqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Character count of Russian tweets\n",
        "In this exercise, you have been given a dataframe tweets which contains some tweets associated with Russia's Internet Research Agency and compiled by FiveThirtyEight.\n",
        "\n",
        "Your task is to create a new feature 'char_count' in tweets which computes the number of characters for each tweet. Also, compute the average length of each tweet. The tweets are available in the content feature of tweets.\n",
        "\n",
        "Be aware that this is real data from Twitter and as such there is always a risk that it may contain profanity or other offensive content (in this exercise, and any following exercises that also use real Twitter data).\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a new feature char_count by applying len to the 'content' feature of tweets.\n",
        "Print the average character count of the tweets by computing the mean of the 'char_count' feature."
      ],
      "metadata": {
        "id": "8RN9foGbnukA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a feature char_count\n",
        "tweets['char_count'] = tweets['content'].apply(len)\n",
        "\n",
        "# Print the average character count\n",
        "print(tweets['char_count'].mean())"
      ],
      "metadata": {
        "id": "x-mF8vAQnvQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word count of TED talks\n",
        "ted is a dataframe that contains the transcripts of 500 TED talks. Your job is to compute a new feature word_count which contains the approximate number of words for each talk. Consequently, you also need to compute the average word count of the talks. The transcripts are available as the transcript feature in ted.\n",
        "\n",
        "In order to complete this task, you will need to define a function count_words that takes in a string as an argument and returns the number of words in the string. You will then need to apply this function to the transcript feature of ted to create the new feature word_count and compute its mean.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Split string into a list of words using the split() method.\n",
        "Return the number of elements in words using len().\n",
        "Apply your function to the transcript column of ted to create the new feature word_count.\n",
        "Compute the average word count of the talks using mean()."
      ],
      "metadata": {
        "id": "bj3kxdkan53C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that returns number of words in a string\n",
        "def count_words(string):\n",
        "\t# Split the string into words\n",
        "    words = string.split()\n",
        "\n",
        "    # Return the number of words\n",
        "    return len(words)\n",
        "\n",
        "# Create a new feature word_count\n",
        "ted['word_count'] = ted['transcript'].apply(count_words)\n",
        "\n",
        "# Print the average word count of the talks\n",
        "print(ted['word_count'].mean())"
      ],
      "metadata": {
        "id": "kL2dljN0n6SX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hashtags and mentions in Russian tweets\n",
        "Let's revisit the tweets dataframe containing the Russian tweets. In this exercise, you will compute the number of hashtags and mentions in each tweet by defining two functions count_hashtags() and count_mentions() respectively and applying them to the content feature of tweets.\n",
        "\n",
        "In case you don't recall, the tweets are contained in the content feature of tweets.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "In the list comprehension, use startswith() to check if a particular word starts with '#'."
      ],
      "metadata": {
        "id": "9XgZXOnToEl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that returns number of hashtags in a string\n",
        "def count_hashtags(string):\n",
        "\t# Split the string into words\n",
        "    words = string.split()\n",
        "\n",
        "    # Create a list of words that are hashtags\n",
        "    hashtags = [word for word in words if word.startswith('#')]\n",
        "\n",
        "    # Return number of hashtags\n",
        "    return(len(hashtags))\n",
        "\n",
        "# Create a feature hashtag_count and display distribution\n",
        "tweets['hashtag_count'] = tweets['content'].apply(count_hashtags)\n",
        "tweets['hashtag_count'].hist()\n",
        "plt.title('Hashtag count distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zXh3NutZoFDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "\n",
        "\n",
        "In the list comprehension, use startswith() to check if a particular word starts with '@'."
      ],
      "metadata": {
        "id": "CCiF0v31oNvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function that returns number of mentions in a string\n",
        "def count_mentions(string):\n",
        "\t# Split the string into words\n",
        "    words = string.split()\n",
        "\n",
        "    # Create a list of words that are mentions\n",
        "    mentions = [word for word in words if word.startswith('@')]\n",
        "\n",
        "    # Return number of mentions\n",
        "    return(len(mentions))\n",
        "\n",
        "# Create a feature mention_count and display distribution\n",
        "tweets['mention_count'] = tweets['content'].apply(count_mentions)\n",
        "tweets['mention_count'].hist()\n",
        "plt.title('Mention count distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UmTsRfUpoP6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Readability of 'The Myth of Sisyphus'\n",
        "In this exercise, you will compute the Flesch reading ease score for Albert Camus' famous essay The Myth of Sisyphus. We will then interpret the value of this score as explained in the video and try to determine the reading level of the essay.\n",
        "\n",
        "The entire essay is in the form of a string and is available as sisyphus_essay.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import the Readability class from readability.\n",
        "Compute the readability_scores object for sisyphus_essay using Readability.\n",
        "Print the Flesch reading ease score using the flesch method."
      ],
      "metadata": {
        "id": "1gKQXVXwo5xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Readability\n",
        "from readability import Readability\n",
        "\n",
        "# Compute the readability scores object\n",
        "readability_scores = Readability(sisyphus_essay)\n",
        "\n",
        "# Print the flesch reading ease score\n",
        "flesch = readability_scores.flesch()\n",
        "print(\"The Flesch Reading Ease is %.2f\" % (flesch.score))"
      ],
      "metadata": {
        "id": "eZiWkdIoo6MN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Readability of various publications\n",
        "In this exercise, you have been given excerpts of articles from four publications. Your task is to compute the readability of these excerpts using the Gunning fog score and consequently, determine the relative difficulty of reading these publications.\n",
        "\n",
        "The excerpts are available as the following strings:\n",
        "\n",
        "forbes- An excerpt from an article from Forbes magazine on the Chinese social credit score system.\n",
        "harvard_law- An excerpt from a book review published in Harvard Law Review.\n",
        "r_digest- An excerpt from a Reader's Digest article on flight turbulence.\n",
        "time_kids - An excerpt from an article on the ill effects of salt consumption published in TIME for Kids.\n",
        "Instructions\n",
        "100 XP\n",
        "Import the Readability class from readability.\n",
        "Compute the gf object for each excerpt using the gunning_fog() method on Readability.\n",
        "Compute the Gunning fog score using the the score attribute.\n",
        "Print the list of Gunning fog scores.\n"
      ],
      "metadata": {
        "id": "Z71Eyi7QpJmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Readability\n",
        "from readability import Readability\n",
        "\n",
        "# List of excerpts\n",
        "excerpts = [forbes, harvard_law, r_digest, time_kids]\n",
        "\n",
        "# Loop through excerpts and compute gunning fog index\n",
        "gunning_fog_scores = []\n",
        "for excerpt in excerpts:\n",
        "  gf = Readability(excerpt).gunning_fog()\n",
        "  gf_score = gf.score\n",
        "  gunning_fog_scores.append(gf_score)\n",
        "\n",
        "# Print the gunning fog indices\n",
        "print(gunning_fog_scores)"
      ],
      "metadata": {
        "id": "ty5W_OL0pKYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing the Gettysburg Address\n",
        "In this exercise, you will be tokenizing one of the most famous speeches of all time: the Gettysburg Address delivered by American President Abraham Lincoln during the American Civil War.\n",
        "\n",
        "The entire speech is available as a string named gettysburg.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Load the en_core_web_sm model using spacy.load().\n",
        "Create a Doc object doc for the gettysburg string.\n",
        "Using list comprehension, loop over doc to generate the token texts."
      ],
      "metadata": {
        "id": "yn8KKGRVqFJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the en_core_web_sm model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create a Doc object\n",
        "doc = nlp(gettysburg)\n",
        "\n",
        "# Generate the tokens\n",
        "tokens = [token.text for token in doc]\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "G051cBOAqGB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatizing the Gettysburg address\n",
        "In this exercise, we will perform lemmatization on the same gettysburg address from before.\n",
        "\n",
        "However, this time, we will also take a look at the speech, before and after lemmatization, and try to adjudge the kind of changes that take place to make the piece more machine friendly.\n",
        "\n",
        "Instructions 1/3\n",
        "35 XP\n",
        "1\n",
        "2\n",
        "3\n",
        "Print the gettysburg address to the console."
      ],
      "metadata": {
        "id": "OdUm9LpZqT7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the gettysburg address\n",
        "print(gettysburg)"
      ],
      "metadata": {
        "id": "TxUh6pXSqUY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/3\n",
        "35 XP\n",
        "2\n",
        "3\n",
        "Loop over doc and extract the lemma for each token of gettysburg."
      ],
      "metadata": {
        "id": "AIspq4OVqfw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the en_core_web_sm model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create a Doc object\n",
        "doc = nlp(gettysburg)\n",
        "\n",
        "# Generate lemmas\n",
        "lemmas = [token.lemma_ for token in doc]"
      ],
      "metadata": {
        "id": "fhEgYxSBqgP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/3\n",
        "30 XP\n",
        "3\n",
        "Convert lemmas into a string using join."
      ],
      "metadata": {
        "id": "57gCKXbfqlEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the en_core_web_sm model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create a Doc object\n",
        "doc = nlp(gettysburg)\n",
        "\n",
        "# Generate lemmas\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "\n",
        "# Convert lemmas into a string\n",
        "print(' '.join(lemmas))"
      ],
      "metadata": {
        "id": "MZz7r-6NqlfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning a blog post\n",
        "In this exercise, you have been given an excerpt from a blog post. Your task is to clean this text into a more machine friendly format. This will involve converting to lowercase, lemmatization and removing stopwords, punctuations and non-alphabetic characters.\n",
        "\n",
        "The excerpt is available as a string blog and has been printed to the console. The list of stopwords are available as stopwords.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Using list comprehension, loop through doc to extract the lemma_ of each token.\n",
        "Remove stopwords and non-alphabetic tokens using stopwords and isalpha()"
      ],
      "metadata": {
        "id": "dLEIifyJruNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and create Doc object\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp(blog)\n",
        "\n",
        "# Generate lemmatized tokens\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "\n",
        "# Remove stopwords and non-alphabetic tokens\n",
        "a_lemmas = [lemma for lemma in lemmas\n",
        "            if lemma.isalpha() and lemma not in stopwords]\n",
        "\n",
        "# Print string after text cleaning\n",
        "print(' '.join(a_lemmas))"
      ],
      "metadata": {
        "id": "-ZYBjYw7rvS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning TED talks in a dataframe\n",
        "In this exercise, we will revisit the TED Talks from the first chapter. You have been a given a dataframe ted consisting of 5 TED Talks. Your task is to clean these talks using techniques discussed earlier by writing a function preprocess and applying it to the transcript feature of the dataframe.\n",
        "\n",
        "The stopwords list is available as stopwords.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Generate the Doc object for text. Ignore the disable argument for now.\n",
        "Generate lemmas using list comprehension using the lemma_ attribute.\n",
        "Remove non-alphabetic characters using isalpha() in the if condition."
      ],
      "metadata": {
        "id": "j1naLzywr6r4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess text\n",
        "def preprocess(text):\n",
        "  \t# Create Doc object\n",
        "    doc = nlp(text, disable=['ner', 'parser'])\n",
        "    # Generate lemmas\n",
        "    lemmas = [token.lemma_ for token in doc]\n",
        "    # Remove stopwords and non-alphabetic characters\n",
        "    a_lemmas = [lemma for lemma in lemmas\n",
        "            if lemma.isalpha() and lemma not in stopwords]\n",
        "\n",
        "    return ' '.join(a_lemmas)\n",
        "\n",
        "# Apply preprocess to ted['transcript']\n",
        "ted['transcript'] = ted['transcript'].apply(preprocess)\n",
        "print(ted['transcript'])"
      ],
      "metadata": {
        "id": "unZcFU6Tr7Qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POS tagging in Lord of the Flies\n",
        "In this exercise, you will perform part-of-speech tagging on a famous passage from one of the most well-known novels of all time, Lord of the Flies, authored by William Golding.\n",
        "\n",
        "The passage is available as lotf and has already been printed to the console.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Load the en_core_web_sm model.\n",
        "Create a doc object for lotf using nlp().\n",
        "Using the text and pos_ attributes, generate tokens and their corresponding POS tags."
      ],
      "metadata": {
        "id": "Z6CNUOwUs1hv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the en_core_web_sm model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create a Doc object\n",
        "doc = nlp(lotf)\n",
        "\n",
        "# Generate tokens and pos tags\n",
        "pos = [(token.text, token.pos_) for token in doc]\n",
        "print(pos)"
      ],
      "metadata": {
        "id": "hzjqlmlUs2Bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Counting nouns in a piece of text\n",
        "In this exercise, we will write two functions, nouns() and proper_nouns() that will count the number of other nouns and proper nouns in a piece of text respectively.\n",
        "\n",
        "These functions will take in a piece of text and generate a list containing the POS tags for each word. It will then return the number of proper nouns/other nouns that the text contains. We will use these functions in the next exercise to generate interesting insights about fake news.\n",
        "\n",
        "The en_core_web_sm model has already been loaded as nlp in this exercise.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "Using the list count method, count the number of proper nouns (annotated as PROPN) in the pos list."
      ],
      "metadata": {
        "id": "f8nNilg5s_cO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Returns number of proper nouns\n",
        "def proper_nouns(text, model=nlp):\n",
        "  \t# Create doc object\n",
        "    doc = model(text)\n",
        "    # Generate list of POS tags\n",
        "    pos = [token.pos_ for token in doc]\n",
        "\n",
        "    # Return number of proper nouns\n",
        "    return pos.count('PROPN')\n",
        "\n",
        "print(proper_nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))"
      ],
      "metadata": {
        "id": "B7igN1cGs_9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "\n",
        "Using the list count method, count the number of other nouns (annotated as NOUN) in the pos list."
      ],
      "metadata": {
        "id": "YO-_xBNKtGq4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Returns number of other nouns\n",
        "def nouns(text, model=nlp):\n",
        "  \t# Create doc object\n",
        "    doc = model(text)\n",
        "    # Generate list of POS tags\n",
        "    pos = [token.pos_ for token in doc]\n",
        "\n",
        "    # Return number of other nouns\n",
        "    return pos.count('NOUN')\n",
        "\n",
        "print(nouns(\"Abdul, Bill and Cathy went to the market to buy apples.\", nlp))"
      ],
      "metadata": {
        "id": "JAbXRy2qtLnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Noun usage in fake news\n",
        "In this exercise, you have been given a dataframe headlines that contains news headlines that are either fake or real. Your task is to generate two new features num_propn and num_noun that represent the number of proper nouns and other nouns contained in the title feature of headlines.\n",
        "\n",
        "Next, we will compute the mean number of proper nouns and other nouns used in fake and real news headlines and compare the values. If there is a remarkable difference, then there is a good chance that using the num_propn and num_noun features in fake news detectors will improve its performance.\n",
        "\n",
        "To accomplish this task, the functions proper_nouns and nouns that you had built in the previous exercise have already been made available to you.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "Create a new feature num_propn by applying proper_nouns to headlines['title'].\n",
        "Filter headlines to compute the mean number of proper nouns in fake news using the mean method.\n"
      ],
      "metadata": {
        "id": "RmTKF-R7tbVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "headlines['num_propn'] = headlines['title'].apply(proper_nouns)\n",
        "\n",
        "# Compute mean of proper nouns\n",
        "real_propn = headlines[headlines['label'] == 'REAL']['num_propn'].mean()\n",
        "fake_propn = headlines[headlines['label'] == 'FAKE']['num_propn'].mean()\n",
        "\n",
        "# Print results\n",
        "print(\"Mean no. of proper nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_propn, fake_propn))"
      ],
      "metadata": {
        "id": "paBxQyk7tbyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "\n",
        "Repeat the process for other nous: create a feature 'num_noun' using nouns and compute the mean of other nouns"
      ],
      "metadata": {
        "id": "pK2af0dEtkQf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "headlines['num_noun'] = headlines['title'].apply(nouns)\n",
        "\n",
        "# Compute mean of other nouns\n",
        "real_noun = headlines[headlines['label'] == 'REAL']['num_noun'].mean()\n",
        "fake_noun = headlines[headlines['label'] == 'FAKE']['num_noun'].mean()\n",
        "\n",
        "# Print results\n",
        "print(\"Mean no. of other nouns in real and fake headlines are %.2f and %.2f respectively\"%(real_noun, fake_noun))"
      ],
      "metadata": {
        "id": "15iXjpretlti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Named entities in a sentence\n",
        "In this exercise, we will identify and classify the labels of various named entities in a body of text using one of spaCy's statistical models. We will also verify the veracity of these labels.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Use spacy.load() to load the en_core_web_sm model.\n",
        "Create a Doc instance doc using text and nlp.\n",
        "Loop over doc.ents to print all the named entities and their corresponding labels."
      ],
      "metadata": {
        "id": "6RcLGOEsxO6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the required model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create a Doc instance\n",
        "text = 'Sundar Pichai is the CEO of Google. Its headquarters is in Mountain View.'\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print all named entities and their labels\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "id": "iHhUL_AGxPOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Identifying people mentioned in a news article\n",
        "In this exercise, you have been given an excerpt from a news article published in TechCrunch. Your task is to write a function find_people that identifies the names of people that have been mentioned in a particular piece of text. You will then use find_people to identify the people of interest in the article.\n",
        "\n",
        "The article is available as the string tc and has been printed to the console. The required spacy model has also been already loaded as nlp.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a Doc object for text.\n",
        "Using list comprehension, loop through doc.ents and create a list of named entities whose label is PERSON.\n",
        "Using find_persons(), print the people mentioned in tc."
      ],
      "metadata": {
        "id": "skGfivzuxXZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_persons(text):\n",
        "  # Create Doc object\n",
        "  doc = nlp(text)\n",
        "\n",
        "  # Identify the persons\n",
        "  persons = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
        "\n",
        "  # Return persons\n",
        "  return persons\n",
        "\n",
        "print(find_persons(tc))"
      ],
      "metadata": {
        "id": "mNGlcWxExXrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BoW model for movie taglines\n",
        "In this exercise, you have been provided with a corpus of more than 7000 movie tag lines. Your job is to generate the bag of words representation bow_matrix for these taglines. For this exercise, we will ignore the text preprocessing step and generate bow_matrix directly.\n",
        "\n",
        "We will also investigate the shape of the resultant bow_matrix. The first five taglines in corpus have been printed to the console for you to examine.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import the CountVectorizer class from sklearn.\n",
        "Instantiate a CountVectorizer object. Name it vectorizer.\n",
        "Using fit_transform(), generate bow_matrix for corpus."
      ],
      "metadata": {
        "id": "243OqFReyHgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create CountVectorizer object\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Generate matrix of word vectors\n",
        "bow_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Print the shape of bow_matrix\n",
        "print(bow_matrix.shape)"
      ],
      "metadata": {
        "id": "iA-dWFHSyHy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyzing dimensionality and preprocessing\n",
        "In this exercise, you have been provided with a lem_corpus which contains the pre-processed versions of the movie taglines from the previous exercise. In other words, the taglines have been lowercased and lemmatized, and stopwords have been removed.\n",
        "\n",
        "Your job is to generate the bag of words representation bow_lem_matrix for these lemmatized taglines and compare its shape with that of bow_matrix obtained in the previous exercise. The first five lemmatized taglines in lem_corpus have been printed to the console for you to examine.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import the CountVectorizer class from sklearn.\n",
        "Instantiate a CountVectorizer object. Name it vectorizer.\n",
        "Using fit_transform(), generate bow_lem_matrix for lem_corpus."
      ],
      "metadata": {
        "id": "-tl96mQsyPhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create CountVectorizer object\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Generate matrix of word vectors\n",
        "bow_lem_matrix = vectorizer.fit_transform(lem_corpus)\n",
        "\n",
        "# Print the shape of bow_lem_matrix\n",
        "print(bow_lem_matrix.shape)"
      ],
      "metadata": {
        "id": "QWwnI6uTyPya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mapping feature indices with feature names\n",
        "In the lesson video, we had seen that CountVectorizer doesn't necessarily index the vocabulary in alphabetical order. In this exercise, we will learn to map each feature index to its corresponding feature name from the vocabulary.\n",
        "\n",
        "We will use the same three sentences on lions from the video. The sentences are available in a list named corpus and has already been printed to the console.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Instantiate a CountVectorizer object. Name it vectorizer.\n",
        "Using fit_transform(), generate bow_matrix for corpus.\n",
        "Using the get_feature_names() method, map the column names to the corresponding word in the vocabulary."
      ],
      "metadata": {
        "id": "tTJQx0PCyWvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create CountVectorizer object\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Generate matrix of word vectors\n",
        "bow_matrix = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Convert bow_matrix into a DataFrame\n",
        "bow_df = pd.DataFrame(bow_matrix.toarray())\n",
        "\n",
        "# Map the column names to vocabulary\n",
        "bow_df.columns = vectorizer.get_feature_names()\n",
        "\n",
        "# Print bow_df\n",
        "print(bow_df)"
      ],
      "metadata": {
        "id": "wcRjUIhoyXBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BoW vectors for movie reviews\n",
        "In this exercise, you have been given two pandas Series, X_train and X_test, which consist of movie reviews. They represent the training and the test review data respectively. Your task is to preprocess the reviews and generate BoW vectors for these two sets using CountVectorizer.\n",
        "\n",
        "Once we have generated the BoW vector matrices X_train_bow and X_test_bow, we will be in a very good position to apply a machine learning model to it and conduct sentiment analysis.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import CountVectorizer from the sklearn library.\n",
        "Instantiate a CountVectorizer object named vectorizer. Ensure that all words are converted to lowercase and english stopwords are removed.\n",
        "Using X_train, fit vectorizer and then use it to transform X_train to generate the set of BoW vectors X_train_bow.\n",
        "Transform X_test using vectorizer to generate the set of BoW vectors X_test_bow.\n"
      ],
      "metadata": {
        "id": "P14r8ZY1zGfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create a CountVectorizer object\n",
        "vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
        "\n",
        "# Fit and transform X_train\n",
        "X_train_bow = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform X_test\n",
        "X_test_bow = vectorizer.transform(X_test)\n",
        "\n",
        "# Print shape of X_train_bow and X_test_bow\n",
        "print(X_train_bow.shape)\n",
        "print(X_test_bow.shape)"
      ],
      "metadata": {
        "id": "vlp8nedvzGwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting the sentiment of a movie review\n",
        "In the previous exercise, you generated the bag-of-words representations for the training and test movie review data. In this exercise, we will use this model to train a Naive Bayes classifier that can detect the sentiment of a movie review and compute its accuracy. Note that since this is a binary classification problem, the model is only capable of classifying a review as either positive (1) or negative (0). It is incapable of detecting neutral reviews.\n",
        "\n",
        "In case you don't recall, the training and test BoW vectors are available as X_train_bow and X_test_bow respectively. The corresponding labels are available as y_train and y_test respectively. Also, for you reference, the original movie review dataset is available as df.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Instantiate an object of MultinomialNB. Name it clf.\n",
        "Fit clf using X_train_bow and y_train.\n",
        "Measure the accuracy of clf using X_test_bow and y_test."
      ],
      "metadata": {
        "id": "N6QWw8-rzOej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a MultinomialNB object\n",
        "clf = MultinomialNB()\n",
        "\n",
        "# Fit the classifier\n",
        "clf.fit(X_train_bow, y_train)\n",
        "\n",
        "# Measure the accuracy\n",
        "accuracy = clf.score(X_test_bow, y_test)\n",
        "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
        "\n",
        "# Predict the sentiment of a negative review\n",
        "review = \"The movie was terrible. The music was underwhelming and the acting mediocre.\"\n",
        "prediction = clf.predict(vectorizer.transform([review]))[0]\n",
        "print(\"The sentiment predicted by the classifier is %i\" % (prediction))"
      ],
      "metadata": {
        "id": "0tA4HE5vzOwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# n-gram models for movie tag lines\n",
        "In this exercise, we have been provided with a corpus of more than 9000 movie tag lines. Our job is to generate n-gram models up to n equal to 1, n equal to 2 and n equal to 3 for this data and discover the number of features for each model.\n",
        "\n",
        "We will then compare the number of features generated for each model.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Generate an n-gram model with n-grams up to n=1. Name it ng1\n",
        "Generate an n-gram model with n-grams up to n=2. Name it ng2\n",
        "Generate an n-Gram Model with n-grams up to n=3. Name it ng3\n",
        "Print the number of features for each model."
      ],
      "metadata": {
        "id": "sBCYGhSNzyVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate n-grams upto n=1\n",
        "vectorizer_ng1 = CountVectorizer(ngram_range=(1,1))\n",
        "ng1 = vectorizer_ng1.fit_transform(corpus)\n",
        "\n",
        "# Generate n-grams upto n=2\n",
        "vectorizer_ng2 = CountVectorizer(ngram_range=(1,2))\n",
        "ng2 = vectorizer_ng2.fit_transform(corpus)\n",
        "\n",
        "# Generate n-grams upto n=3\n",
        "vectorizer_ng3 = CountVectorizer(ngram_range=(1,3))\n",
        "ng3 = vectorizer_ng3.fit_transform(corpus)\n",
        "\n",
        "# Print the number of features for each model\n",
        "print(\"ng1, ng2 and ng3 have %i, %i and %i features respectively\" % (ng1.shape[1], ng2.shape[1], ng3.shape[1]))"
      ],
      "metadata": {
        "id": "YAEdKcHuzynk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Higher order n-grams for sentiment analysis\n",
        "Similar to a previous exercise, we are going to build a classifier that can detect if the review of a particular movie is positive or negative. However, this time, we will use n-grams up to n=2 for the task.\n",
        "\n",
        "The n-gram training reviews are available as X_train_ng. The corresponding test reviews are available as X_test_ng. Finally, use y_train and y_test to access the training and test sentiment classes respectively.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Define an instance of MultinomialNB. Name it clf_ng\n",
        "Fit the classifier on X_train_ng and y_train.\n",
        "Measure accuracy on X_test_ng and y_test the using score() method."
      ],
      "metadata": {
        "id": "sMwJgctE0F0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an instance of MultinomialNB\n",
        "clf_ng = MultinomialNB()\n",
        "\n",
        "# Fit the classifier\n",
        "clf_ng.fit(X_train_ng, y_train)\n",
        "\n",
        "# Measure the accuracy\n",
        "accuracy = clf_ng.score(X_test_ng, y_test)\n",
        "print(\"The accuracy of the classifier on the test set is %.3f\" % accuracy)\n",
        "\n",
        "# Predict the sentiment of a negative review\n",
        "review = \"The movie was not good. The plot had several holes and the acting lacked panache.\"\n",
        "prediction = clf_ng.predict(ng_vectorizer.transform([review]))[0]\n",
        "print(\"The sentiment predicted by the classifier is %i\" % (prediction))"
      ],
      "metadata": {
        "id": "6_6ASYwU0GIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing performance of n-gram models\n",
        "You now know how to conduct sentiment analysis by converting text into various n-gram representations and feeding them to a classifier. In this exercise, we will conduct sentiment analysis for the same movie reviews from before using two n-gram models: unigrams and n-grams upto n equal to 3.\n",
        "\n",
        "We will then compare the performance using three criteria: accuracy of the model on the test set, time taken to execute the program and the number of features created when generating the n-gram representation.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "Initialize a CountVectorizer object such that it generates unigrams."
      ],
      "metadata": {
        "id": "93mEDe8V0Nbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "# Splitting the data into training and test sets\n",
        "train_X, test_X, train_y, test_y = train_test_split(df['review'], df['sentiment'], test_size=0.5, random_state=42, stratify=df['sentiment'])\n",
        "\n",
        "# Generating ngrams\n",
        "vectorizer = CountVectorizer()\n",
        "train_X = vectorizer.fit_transform(train_X)\n",
        "test_X = vectorizer.transform(test_X)\n",
        "\n",
        "# Fit classifier\n",
        "clf = MultinomialNB()\n",
        "clf.fit(train_X, train_y)\n",
        "\n",
        "# Print accuracy, time and number of dimensions\n",
        "print(\"The program took %.3f seconds to complete. The accuracy on the test set is %.2f. The ngram representation had %i features.\" % (time.time() - start_time, clf.score(test_X, test_y), train_X.shape[1]))"
      ],
      "metadata": {
        "id": "i5B33KWe0NsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "\n",
        "Initialize a CountVectorizer object such that it generates ngrams upto n=3."
      ],
      "metadata": {
        "id": "zS3akV4E0TY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "# Splitting the data into training and test sets\n",
        "train_X, test_X, train_y, test_y = train_test_split(df['review'], df['sentiment'], test_size=0.5, random_state=42, stratify=df['sentiment'])\n",
        "\n",
        "# Generating ngrams\n",
        "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
        "train_X = vectorizer.fit_transform(train_X)\n",
        "test_X = vectorizer.transform(test_X)\n",
        "\n",
        "# Fit classifier\n",
        "clf = MultinomialNB()\n",
        "clf.fit(train_X, train_y)\n",
        "\n",
        "# Print accuracy, time and number of dimensions\n",
        "print(\"The program took %.3f seconds to complete. The accuracy on the test set is %.2f. The ngram representation had %i features.\" % (time.time() - start_time, clf.score(test_X, test_y), train_X.shape[1]))"
      ],
      "metadata": {
        "id": "fdz2efIS0U-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tf-idf vectors for TED talks\n",
        "In this exercise, you have been given a corpus ted which contains the transcripts of 500 TED Talks. Your task is to generate the tf-idf vectors for these talks.\n",
        "\n",
        "In a later lesson, we will use these vectors to generate recommendations of similar talks based on the transcript.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import TfidfVectorizer from sklearn.\n",
        "Create a TfidfVectorizer object. Name it vectorizer.\n",
        "Generate tfidf_matrix for ted using the fit_transform() method."
      ],
      "metadata": {
        "id": "Veylr-mr0ck9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create TfidfVectorizer object\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Generate matrix of word vectors\n",
        "tfidf_matrix = vectorizer.fit_transform(ted)\n",
        "\n",
        "# Print the shape of tfidf_matrix\n",
        "print(tfidf_matrix.shape)"
      ],
      "metadata": {
        "id": "PDnEMJr91DYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computing dot product\n",
        "In this exercise, we will learn to compute the dot product between two vectors, A = (1, 3) and B = (-2, 2), using the numpy library. More specifically, we will use the np.dot() function to compute the dot product of two numpy arrays.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Initialize A (1,3) and B (-2,2) as numpy arrays using np.array().\n",
        "Compute the dot product using np.dot() and passing A and B as arguments."
      ],
      "metadata": {
        "id": "WEZkzkJ23hmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize numpy vectors\n",
        "A = np.array([1,3])\n",
        "B = np.array([-2, 2])\n",
        "\n",
        "# Compute dot product\n",
        "dot_prod = np.dot(A, B)\n",
        "\n",
        "# Print dot product\n",
        "print(dot_prod)"
      ],
      "metadata": {
        "id": "GgIrbCoi3h1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cosine similarity matrix of a corpus\n",
        "In this exercise, you have been given a corpus, which is a list containing five sentences. The corpus is printed in the console. You have to compute the cosine similarity matrix which contains the pairwise cosine similarity score for every pair of sentences (vectorized using tf-idf).\n",
        "\n",
        "Remember, the value corresponding to the ith row and jth column of a similarity matrix denotes the similarity score for the ith and jth vector.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Initialize an instance of TfidfVectorizer. Name it tfidf_vectorizer.\n",
        "Using fit_transform(), generate the tf-idf vectors for corpus. Name it tfidf_matrix.\n",
        "Use cosine_similarity() and pass tfidf_matrix to compute the cosine similarity matrix cosine_sim."
      ],
      "metadata": {
        "id": "3X0jpDM930Uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an instance of tf-idf Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Generate the tf-idf vectors for the corpus\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Compute and print the cosine similarity matrix\n",
        "cosine_sim = cosine_similarity(tfidf_matrix,tfidf_matrix)\n",
        "print(cosine_sim)"
      ],
      "metadata": {
        "id": "uHKtGCm-30lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing linear_kernel and cosine_similarity\n",
        "In this exercise, you have been given tfidf_matrix which contains the tf-idf vectors of a thousand documents. Your task is to generate the cosine similarity matrix for these vectors first using cosine_similarity and then, using linear_kernel.\n",
        "\n",
        "We will then compare the computation times for both functions.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "1\n",
        "Compute the cosine similarity matrix for tfidf_matrix using cosine_similarity."
      ],
      "metadata": {
        "id": "Abc4y-mw4d1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Record start time\n",
        "start = time.time()\n",
        "\n",
        "# Compute cosine similarity matrix\n",
        "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "# Print cosine similarity matrix\n",
        "print(cosine_sim)\n",
        "\n",
        "# Print time taken\n",
        "print(\"Time taken: %s seconds\" %(time.time() - start))"
      ],
      "metadata": {
        "id": "839UMvk54eE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "\n",
        "Compute the cosine similarity matrix for tfidf_matrix using linear_kernel."
      ],
      "metadata": {
        "id": "WfdF3vNN4iQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Record start time\n",
        "start = time.time()\n",
        "\n",
        "# Compute cosine similarity matrix\n",
        "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "# Print cosine similarity matrix\n",
        "print(cosine_sim)\n",
        "\n",
        "# Print time taken\n",
        "print(\"Time taken: %s seconds\" %(time.time() - start))"
      ],
      "metadata": {
        "id": "l6I_YWEr4jIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot recommendation engine\n",
        "In this exercise, we will build a recommendation engine that suggests movies based on similarity of plot lines. You have been given a get_recommendations() function that takes in the title of a movie, a similarity matrix and an indices series as its arguments and outputs a list of most similar movies. indices has already been provided to you.\n",
        "\n",
        "You have also been given a movie_plots Series that contains the plot lines of several movies. Your task is to generate a cosine similarity matrix for the tf-idf vectors of these plots.\n",
        "\n",
        "Consequently, we will check the potency of our engine by generating recommendations for one of my favorite movies, The Dark Knight Rises.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Initialize a TfidfVectorizer with English stop_words. Name it tfidf.\n",
        "Construct tfidf_matrix by fitting and transforming the movie plot data using fit_transform().\n",
        "Generate the cosine similarity matrix cosine_sim using tfidf_matrix. Don't use cosine_similarity()!\n",
        "Use get_recommendations() to generate recommendations for 'The Dark Knight Rises'.\n"
      ],
      "metadata": {
        "id": "eE7eT24E4rFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Construct the TF-IDF matrix\n",
        "tfidf_matrix = tfidf.fit_transform(movie_plots)\n",
        "\n",
        "# Generate the cosine similarity matrix\n",
        "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "# Generate recommendations\n",
        "print(get_recommendations('The Dark Knight Rises', cosine_sim, indices))"
      ],
      "metadata": {
        "id": "PYwFuN664rTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The recommender function\n",
        "In this exercise, we will build a recommender function get_recommendations(), as discussed in the lesson and the previous exercise. As we know, it takes in a title, a cosine similarity matrix, and a movie title and index mapping as arguments and outputs a list of 10 titles most similar to the original title (excluding the title itself).\n",
        "\n",
        "You have been given a dataset metadata that consists of the movie titles and overviews. The head of this dataset has been printed to console.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Get index of the movie that matches the title by using the title key of indices.\n",
        "Extract the ten most similar movies from sim_scores and store it back in sim_scores."
      ],
      "metadata": {
        "id": "rNxQjKjq4zQd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate mapping between titles and index\n",
        "indices = pd.Series(metadata.index, index=metadata['title']).drop_duplicates()\n",
        "\n",
        "def get_recommendations(title, cosine_sim, indices):\n",
        "    # Get index of movie that matches title\n",
        "    idx = indices[title]\n",
        "    # Sort the movies based on the similarity scores\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    # Get the scores for 10 most similar movies\n",
        "    sim_scores = sim_scores[1:11]\n",
        "    # Get the movie indices\n",
        "    movie_indices = [i[0] for i in sim_scores]\n",
        "    # Return the top 10 most similar movies\n",
        "    return metadata['title'].iloc[movie_indices]"
      ],
      "metadata": {
        "id": "BvzsfIOr4zfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TED talk recommender\n",
        "In this exercise, we will build a recommendation system that suggests TED Talks based on their transcripts. You have been given a get_recommendations() function that takes in the title of a talk, a similarity matrix and an indices series as its arguments, and outputs a list of most similar talks. indices has already been provided to you.\n",
        "\n",
        "You have also been given a transcripts series that contains the transcripts of around 500 TED talks. Your task is to generate a cosine similarity matrix for the tf-idf vectors of the talk transcripts.\n",
        "\n",
        "Consequently, we will generate recommendations for a talk titled '5 ways to kill your dreams' by Brazilian entrepreneur Bel Pesce.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Initialize a TfidfVectorizer with English stopwords. Name it tfidf.\n",
        "Construct tfidf_matrix by fitting and transforming transcripts.\n",
        "Generate the cosine similarity matrix cosine_sim using tfidf_matrix.\n",
        "Use get_recommendations() to generate recommendations for '5 ways to kill your dreams'."
      ],
      "metadata": {
        "id": "4dsHxM2k48DC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Construct the TF-IDF matrix\n",
        "tfidf_matrix = tfidf.fit_transform(transcripts)\n",
        "\n",
        "# Generate the cosine similarity matrix\n",
        "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "# Generate recommendations\n",
        "print(get_recommendations('5 ways to kill your dreams', cosine_sim, indices))"
      ],
      "metadata": {
        "id": "kZHWg2cy48bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating word vectors\n",
        "In this exercise, we will generate the pairwise similarity scores of all the words in a sentence. The sentence is available as sent and has been printed to the console for your convenience.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a Doc object doc for sent.\n",
        "In the nested loop, compute the similarity between token1 and token2."
      ],
      "metadata": {
        "id": "vTCwMOLj5ggr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the doc object\n",
        "doc = nlp(sent)\n",
        "\n",
        "# Compute pairwise similarity scores\n",
        "for token1 in doc:\n",
        "  for token2 in doc:\n",
        "    print(token1.text, token2.text, token1.similarity(token2))"
      ],
      "metadata": {
        "id": "0jVtmfU25guP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computing similarity of Pink Floyd songs\n",
        "In this final exercise, you have been given lyrics of three songs by the British band Pink Floyd, namely 'High Hopes', 'Hey You' and 'Mother'. The lyrics to these songs are available as hopes, hey and mother respectively.\n",
        "\n",
        "Your task is to compute the pairwise similarity between mother and hopes, and mother and hey.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create Doc objects for mother, hopes and hey.\n",
        "Compute the similarity between mother and hopes.\n",
        "Compute the similarity between mother and hey.\n"
      ],
      "metadata": {
        "id": "Xs_T3BLr5nf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Doc objects\n",
        "mother_doc = nlp(mother)\n",
        "hopes_doc = nlp(hopes)\n",
        "hey_doc = nlp(hey)\n",
        "\n",
        "# Print similarity between mother and hopes\n",
        "print(mother_doc.similarity(hopes_doc))\n",
        "\n",
        "# Print similarity between mother and hey\n",
        "print(mother_doc.similarity(hey_doc))"
      ],
      "metadata": {
        "id": "OXKG955s5n3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Module End ---"
      ],
      "metadata": {
        "id": "wZOP4GLl5vdp"
      }
    }
  ]
}