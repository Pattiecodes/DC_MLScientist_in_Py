{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyME1UnDQuKVE5G1HJ3VkTAd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pattiecodes/DC_MLScientist_in_Py/blob/main/Module_7_Dimensionality_Reduction_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Module Start ---"
      ],
      "metadata": {
        "id": "TPJZjNO1_mAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removing features without variance\n",
        "A sample of the Pokemon dataset has been loaded as pokemon_df. To get an idea of which features have little variance you should use the IPython Shell to calculate summary statistics on this sample. Then adjust the code to create a smaller, easier to understand, dataset.\n",
        "\n",
        "Instructions 1/3\n",
        "35 XP\n",
        "2\n",
        "3\n",
        "Use the .describe() method to find the numeric feature without variance and remove its name from the list assigned to number_cols."
      ],
      "metadata": {
        "id": "IAUcQ5ob_o1r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6E6KPI82mlg"
      },
      "outputs": [],
      "source": [
        "# Remove the feature without variance from this list\n",
        "number_cols = ['HP', 'Attack', 'Defense']\n",
        "\n",
        "pokemon_df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/3\n",
        "35 XP\n",
        "3\n",
        "Combine the two lists of feature names to sub-select the chosen features from pokemon_df."
      ],
      "metadata": {
        "id": "kCMO-mBf_v_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the feature without variance from this list\n",
        "number_cols = ['HP', 'Attack', 'Defense']\n",
        "\n",
        "# Leave this list as is for now\n",
        "non_number_cols = ['Name', 'Type', 'Legendary']\n",
        "\n",
        "# Sub-select by combining the lists with chosen features\n",
        "df_selected = pokemon_df[number_cols + non_number_cols]\n",
        "\n",
        "# Prints the first 5 lines of the new DataFrame\n",
        "print(df_selected.head())"
      ],
      "metadata": {
        "id": "gEiOv0WP_wUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/3\n",
        "30 XP\n",
        "Find the non-numeric feature without variance and remove its name from the list assigned to non_number_cols."
      ],
      "metadata": {
        "id": "gflMZVlsAR7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Leave this list as is\n",
        "number_cols = ['HP', 'Attack', 'Defense']\n",
        "\n",
        "# Remove the feature without variance from this list\n",
        "non_number_cols = ['Name', 'Type']\n",
        "\n",
        "# Create a new DataFrame by subselecting the chosen features\n",
        "df_selected = pokemon_df[number_cols + non_number_cols]\n",
        "\n",
        "# Prints the first 5 lines of the new DataFrame\n",
        "print(df_selected.head())"
      ],
      "metadata": {
        "id": "0O1EAlrxASMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visually detecting redundant features\n",
        "Data visualization is a crucial step in any data exploration. Let's use Seaborn to explore some samples of the US Army ANSUR body measurement dataset.\n",
        "\n",
        "Two data samples have been pre-loaded as ansur_df_1 and ansur_df_2.\n",
        "\n",
        "Seaborn has been imported as sns.\n",
        "\n",
        "Instructions 1/4\n",
        "25 XP\n",
        "2\n",
        "3\n",
        "4\n",
        "Create a pairplot of the ansur_df_1 data sample and color the points using the 'Gender' feature."
      ],
      "metadata": {
        "id": "LKm04_VIDpyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pairplot and color the points using the 'Gender' feature\n",
        "sns.pairplot(ansur_df_1, hue='Gender', diag_kind='hist')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "__msjtcPDqR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/4\n",
        "25 XP\n",
        "3\n",
        "4\n",
        "Two features are basically duplicates, remove one of them from the dataset.\n"
      ],
      "metadata": {
        "id": "oTKIr_YoEPKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove one of the redundant features\n",
        "reduced_df = ansur_df_1.drop('stature_m', axis=1)\n",
        "\n",
        "# Create a pairplot and color the points using the 'Gender' feature\n",
        "sns.pairplot(reduced_df, hue='Gender')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xJY1mp5dEPYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/4\n",
        "25 XP\n",
        "4\n",
        "Now create a pairplot of the ansur_df_2 data sample and color the points using the 'Gender' feature."
      ],
      "metadata": {
        "id": "-5IkUjLvEius"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pairplot and color the points using the 'Gender' feature\n",
        "sns.pairplot(ansur_df_2, hue='Gender', diag_kind='hist')\n",
        "\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BfbpeTHBEi81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 4/4\n",
        "25 XP\n",
        "One feature has no variance, remove it from the dataset."
      ],
      "metadata": {
        "id": "IfThcgFfEtZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the redundant feature\n",
        "reduced_df = ansur_df_2.drop('n_legs', axis=1)\n",
        "\n",
        "# Create a pairplot and color the points using the 'Gender' feature\n",
        "sns.pairplot(reduced_df, hue='Gender', diag_kind='hist')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-dwNHOEaEt80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fitting t-SNE to the ANSUR data\n",
        "t-SNE is a great technique for visual exploration of high dimensional datasets. In this exercise, you'll apply it to the ANSUR dataset. You'll remove non-numeric columns from the pre-loaded dataset df and fit TSNE to this numeric dataset.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Drop the non-numeric columns from the dataset.\n",
        "Create a TSNE model with learning rate 50.\n",
        "Fit and transform the model on the numeric dataset."
      ],
      "metadata": {
        "id": "fOej6jQQgBCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Non-numerical columns in the dataset\n",
        "non_numeric = ['Branch', 'Gender', 'Component']\n",
        "\n",
        "# Drop the non-numerical columns from df\n",
        "df_numeric = df.drop(non_numeric, axis=1)\n",
        "\n",
        "# Create a t-SNE model with learning rate 50\n",
        "m = TSNE(learning_rate = 50)\n",
        "\n",
        "# Fit and transform the t-SNE model on the numeric dataset\n",
        "tsne_features = m.fit_transform(df_numeric)\n",
        "print(tsne_features.shape)"
      ],
      "metadata": {
        "id": "npObLW9mgBV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# t-SNE visualisation of dimensionality\n",
        "Time to look at the results of your hard work. In this exercise, you will visualize the output of t-SNE dimensionality reduction on the combined male and female Ansur dataset. You'll create 3 scatterplots of the 2 t-SNE features ('x' and 'y') which were added to the dataset df. In each scatterplot you'll color the points according to a different categorical variable.\n",
        "\n",
        "seaborn has already been imported as sns and matplotlib.pyplot as plt.\n",
        "\n",
        "Instructions 1/3\n",
        "35 XP\n",
        "Use seaborn's sns.scatterplot to create the plot.\n",
        "Color the points by 'Component'."
      ],
      "metadata": {
        "id": "wwzu7R6hmtXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Color the points according to Army Component\n",
        "sns.scatterplot(x=\"x\", y=\"y\", hue='Component', data=df)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4dgqw-9rm20B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/3\n",
        "Color the points of the scatterplot by 'Branch'."
      ],
      "metadata": {
        "id": "6Zv_Hgn_nBR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Color the points by Army Branch\n",
        "sns.scatterplot(x=\"x\", y=\"y\", hue='Branch', data=df)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EJ34jpKNnFvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/3 Color the points of the scatterplot by 'Gender'."
      ],
      "metadata": {
        "id": "xU9JHxSInMmB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Color the points by Gender\n",
        "sns.scatterplot(x=\"x\", y=\"y\", hue='Gender', data=df)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ox6-Y-jAnN0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train - test split\n",
        "In this chapter, you will keep working with the ANSUR dataset. Before you can build a model on your dataset, you should first decide on which feature you want to predict. In this case, you're trying to predict gender.\n",
        "\n",
        "You need to extract the column holding this feature from the dataset and then split the data into a training and test set. The training set will be used to train the model and the test set will be used to check its performance on unseen data.\n",
        "\n",
        "ansur_df has been pre-loaded for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import the train_test_split function from sklearn.model_selection.\n",
        "Assign the 'Gender' column to y.\n",
        "Remove the 'Gender' column from the DataFrame and assign the result to X.\n",
        "Set the test size to 30% to perform a 70% train and 30% test data split."
      ],
      "metadata": {
        "id": "yx8b2_rhp4jj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import train_test_split()\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Select the Gender column as the feature to be predicted (y)\n",
        "y = ansur_df['Gender']\n",
        "\n",
        "# Remove the Gender column to create the training data\n",
        "X = ansur_df.drop('Gender', axis=1)\n",
        "\n",
        "# Perform a 70% train and 30% test data split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "print(f\"{X_test.shape[0]} rows in test set vs. {X_train.shape[0]} in training set, {X_test.shape[1]} Features.\")"
      ],
      "metadata": {
        "id": "l4e2jHiLp43Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fitting and testing the model\n",
        "In the previous exercise, you split the dataset into X_train, X_test, y_train, and y_test. These datasets have been pre-loaded for you. You'll now create a support vector machine classifier model (SVC()) and fit that to the training data. You'll then calculate the accuracy on both the test and training set to detect overfitting.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import SVC from sklearn.svm and accuracy_score from sklearn.metrics\n",
        "Create an instance of the Support Vector Classification class (SVC()).\n",
        "Fit the model to the training data.\n",
        "Calculate accuracy scores on both train and test data."
      ],
      "metadata": {
        "id": "WoZI2ValuLt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import SVC from sklearn.svm and accuracy_score from sklearn.metrics\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create an instance of the Support Vector Classification class\n",
        "svc = SVC()\n",
        "\n",
        "# Fit the model to the training data\n",
        "svc.fit(X_train, y_train)\n",
        "\n",
        "# Calculate accuracy scores on both train and test data\n",
        "accuracy_train = accuracy_score(y_train, svc.predict(X_train))\n",
        "accuracy_test = accuracy_score(y_test, svc.predict(X_test))\n",
        "\n",
        "print(f\"{accuracy_test:.1%} accuracy on test set vs. {accuracy_train:.1%} on training set\")"
      ],
      "metadata": {
        "id": "fCGKoZjCuMOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy after dimensionality reduction\n",
        "You'll reduce the overfit with the help of dimensionality reduction. In this case, you'll apply a rather drastic form of dimensionality reduction by only selecting a single column that has some good information to distinguish between genders. You'll repeat the train-test split, model fit and prediction steps to compare the accuracy on test versus training data.\n",
        "\n",
        "All relevant packages and y have been pre-loaded.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Select just the neck circumference ('neckcircumferencebase') column from ansur_df.\n",
        "Split the data, instantiate a classifier and fit the data. This has been done for you.\n",
        "Once again calculate the accuracy scores on both training and test set."
      ],
      "metadata": {
        "id": "WzNzGxMZu8Pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign just the 'neckcircumferencebase' column from ansur_df to X\n",
        "X = ansur_df[['neckcircumferencebase']]\n",
        "\n",
        "# Split the data, instantiate a classifier and fit the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "svc = SVC()\n",
        "svc.fit(X_train, y_train)\n",
        "\n",
        "# Calculate accuracy scores on both train and test data\n",
        "accuracy_train = accuracy_score(y_train, svc.predict(X_train))\n",
        "accuracy_test = accuracy_score(y_test, svc.predict(X_test))\n",
        "\n",
        "print(f\"{accuracy_test:.1%} accuracy on test set vs. {accuracy_train:.1%} on training set\")"
      ],
      "metadata": {
        "id": "RCQXkE0Ru8ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding a good variance threshold\n",
        "You'll be working on a slightly modified subsample of the ANSUR dataset with just head measurements pre-loaded as head_df.\n",
        "\n",
        "Instructions 1/4\n",
        "25 XP\n",
        "2\n",
        "3\n",
        "4\n",
        "Create a boxplot on head_df."
      ],
      "metadata": {
        "id": "acrX2sJVucN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the boxplot\n",
        "head_df.boxplot()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VTSpnX7WucmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/4\n",
        "25 XP\n",
        "3\n",
        "4\n",
        "Normalize the data by dividing the DataFrame with its mean values."
      ],
      "metadata": {
        "id": "uohXEIEwuomH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the data\n",
        "normalized_df = head_df / head_df.mean()\n",
        "\n",
        "normalized_df.boxplot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XmfeTRQfuo06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/4\n",
        "25 XP\n",
        "4\n",
        "Print the variances of the normalized data."
      ],
      "metadata": {
        "id": "iBATK06Qu2G5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the data\n",
        "normalized_df = head_df / head_df.mean()\n",
        "\n",
        "# Print the variances of the normalized data\n",
        "print(normalized_df.var())"
      ],
      "metadata": {
        "id": "47gqiv3Eu2UK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Features with low variance\n",
        "In the previous exercise you established that 0.001 is a good threshold to filter out low variance features in head_df after normalization. Now use the VarianceThreshold feature selector to remove these features.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create the variance threshold selector with a threshold of 0.001.\n",
        "Normalize the head_df DataFrame by dividing it by its mean values and fit the selector.\n",
        "Create a boolean mask from the selector using .get_support().\n",
        "Create a reduced DataFrame by passing the mask to the .loc[] method."
      ],
      "metadata": {
        "id": "JZDlvjyEvlhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "# Create a VarianceThreshold feature selector\n",
        "sel = VarianceThreshold(threshold=0.001)\n",
        "\n",
        "# Fit the selector to normalized head_df\n",
        "sel.fit(head_df / head_df.mean())\n",
        "\n",
        "# Create a boolean mask\n",
        "mask = sel.get_support()\n",
        "\n",
        "# Apply the mask to create a reduced DataFrame\n",
        "reduced_df = head_df.loc[:, mask]\n",
        "\n",
        "print(f\"Dimensionality reduced from {head_df.shape[1]} to {reduced_df.shape[1]}.\")"
      ],
      "metadata": {
        "id": "me46hbRQvlzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removing features with many missing values\n",
        "You'll apply feature selection on the Boston Public Schools dataset which has been pre-loaded as school_df. Calculate the missing value ratio per feature and then create a mask to remove features with many missing values.\n",
        "\n",
        "Instructions 2/2\n",
        "50 XP\n",
        "Create a boolean mask on whether each feature has less than 50% missing values.\n",
        "Apply the mask to school_df to select columns without many missing values."
      ],
      "metadata": {
        "id": "0rCaDHQDwX9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a boolean mask on whether each feature less than 50% missing values.\n",
        "mask = school_df.isna().sum() / len(school_df) < 0.5\n",
        "\n",
        "# Create a reduced dataset by applying the mask\n",
        "reduced_df = school_df.loc[:, mask]\n",
        "\n",
        "print(school_df.shape)\n",
        "print(reduced_df.shape)"
      ],
      "metadata": {
        "id": "LfJETpn1wYS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing the correlation matrix\n",
        "Reading the correlation matrix of ansur_df in its raw, numeric format doesn't allow us to get a quick overview. Let's improve this by removing redundant values and visualizing the matrix using seaborn.\n",
        "\n",
        "Seaborn has been pre-loaded as sns, matplotlib.pyplot as plt, NumPy as np and pandas as pd.\n",
        "\n",
        "Instructions 1/4\n",
        "25 XP\n",
        "2\n",
        "3\n",
        "4\n",
        "Create the correlation matrix.\n",
        "Visualize it using Seaborn's heatmap function."
      ],
      "metadata": {
        "id": "Csf327Bkmck2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the correlation matrix\n",
        "corr = ansur_df.corr()\n",
        "\n",
        "# Draw a heatmap of the correlation matrix\n",
        "sns.heatmap(corr,  cmap=cmap, center=0, linewidths=1, annot=True, fmt=\".2f\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VtzBLSXXmc3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/4\n",
        "25 XP\n",
        "3\n",
        "4\n",
        "Create a boolean mask for the upper triangle of the plot.\\"
      ],
      "metadata": {
        "id": "05ii33grmg04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the correlation matrix\n",
        "corr = ansur_df.corr()\n",
        "\n",
        "# Generate a mask for the upper triangle\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "\n",
        "print(mask)"
      ],
      "metadata": {
        "id": "ZGDROgkGmhAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/4\n",
        "25 XP\n",
        "4\n",
        "Add the mask to the heatmap."
      ],
      "metadata": {
        "id": "GLrk0bZZmn0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the correlation matrix\n",
        "corr = ansur_df.corr()\n",
        "\n",
        "# Generate a mask for the upper triangle\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "\n",
        "# Add the mask to the heatmap\n",
        "sns.heatmap(corr, mask=mask, cmap=cmap, center=0, linewidths=1, annot=True, fmt=\".2f\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3Uw9Ej9Kmoax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filtering out highly correlated features\n",
        "You're going to automate the removal of highly correlated features in the numeric ANSUR dataset. You'll calculate the correlation matrix and filter out columns that have a correlation coefficient of more than 0.95 or less than -0.95.\n",
        "\n",
        "Since each correlation coefficient occurs twice in the matrix (correlation of A to B equals correlation of B to A) you'll want to ignore half of the correlation matrix so that only one of the two correlated features is removed. Use a mask trick for this purpose.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Calculate the correlation matrix of ansur_df and take the absolute value of this matrix.\n",
        "Create a boolean mask with True values in the upper right triangle and apply it to the correlation matrix.\n",
        "Set the correlation coefficient threshold to 0.95.\n",
        "Drop all the columns listed in to_drop from the DataFrame."
      ],
      "metadata": {
        "id": "c0CAXclXTSj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the correlation matrix and take the absolute value\n",
        "corr_df = ansur_df.corr().abs()\n",
        "\n",
        "# Create a True/False mask and apply it\n",
        "mask = np.triu(np.ones_like(corr_df, dtype=bool))\n",
        "tri_df = corr_df.mask(mask)\n",
        "\n",
        "# List column names of highly correlated features (r > 0.95)\n",
        "to_drop = [c for c in tri_df.columns if any(tri_df[c] >  0.95)]\n",
        "\n",
        "# Drop the features in the to_drop list\n",
        "reduced_df = ansur_df.drop(to_drop, axis=1)\n",
        "\n",
        "print(f\"The reduced_df DataFrame has {reduced_df.shape[1]} columns.\")"
      ],
      "metadata": {
        "id": "C4qp4hqgTS3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nuclear energy and pool drownings\n",
        "The dataset that has been pre-loaded for you as weird_df contains actual data provided by the US Centers for Disease Control & Prevention and Department of Energy.\n",
        "\n",
        "Let's see if we can find a pattern.\n",
        "\n",
        "Seaborn has been pre-loaded as sns and matplotlib.pyplot as plt.\n",
        "\n",
        "Instructions 1/4\n",
        "25 XP\n",
        "2\n",
        "3\n",
        "4\n",
        "Print the first five lines of weird_df."
      ],
      "metadata": {
        "id": "S9xiRZnUTpvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first five lines of weird_df\n",
        "print(weird_df.head())"
      ],
      "metadata": {
        "id": "yEvwvVFrTp_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/4\n",
        "25 XP\n",
        "3\n",
        "4\n",
        "Create a scatterplot with nuclear energy production on the x-axis and the number of pool drownings on the y-axis."
      ],
      "metadata": {
        "id": "ta2iUmJUT_W9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Put nuclear energy production on the x-axis and the number of pool drownings on the y-axis\n",
        "sns.scatterplot(x='nuclear_energy', y='pool_drownings', data=weird_df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "14V7NG00T_h7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/4\n",
        "25 XP\n",
        "4\n",
        "Print out the correlation matrix of weird_df."
      ],
      "metadata": {
        "id": "hhllMxeBUI7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out the correlation matrix of weird_df\n",
        "print(weird_df.corr())"
      ],
      "metadata": {
        "id": "k874cDPjUJRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a diabetes classifier\n",
        "You'll be using the Pima Indians diabetes dataset to predict whether a person has diabetes using logistic regression. There are 8 features and one target in this dataset. The data has been split into a training and test set and pre-loaded for you as X_train, y_train, X_test, and y_test.\n",
        "\n",
        "A StandardScaler() instance has been predefined as scaler and a LogisticRegression() one as lr.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Fit the scaler on the training features and transform these features in one go.\n",
        "Fit the logistic regression model on the scaled training data.\n",
        "Scale the test features.\n",
        "Predict diabetes presence on the scaled test set."
      ],
      "metadata": {
        "id": "MD-gWn9bWs_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the scaler on the training features and transform these in one go\n",
        "X_train_std = scaler.fit_transform(X_train)\n",
        "\n",
        "# Fit the logistic regression model on the scaled training data\n",
        "lr.fit(X_train_std, y_train)\n",
        "\n",
        "# Scale the test features\n",
        "X_test_std = scaler.transform(X_test)\n",
        "\n",
        "# Predict diabetes presence on the scaled test set\n",
        "y_pred = lr.predict(X_test_std)\n",
        "\n",
        "# Prints accuracy metrics and feature coefficients\n",
        "print(f\"{accuracy_score(y_test, y_pred):.1%} accuracy on test set.\")\n",
        "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
      ],
      "metadata": {
        "id": "JP5iFP2GWtmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manual Recursive Feature Elimination\n",
        "Now that we've created a diabetes classifier, let's see if we can reduce the number of features without hurting the model accuracy too much.\n",
        "\n",
        "On the second line of code the features are selected from the original DataFrame. Adjust this selection.\n",
        "\n",
        "A StandardScaler() instance has been predefined as scaler and a LogisticRegression() one as lr.\n",
        "\n",
        "All necessary functions and packages have been pre-loaded too.\n",
        "\n",
        "Instructions 1/3\n",
        "35 XP\n",
        "First, run the given code, then remove the feature with the lowest model coefficient from X."
      ],
      "metadata": {
        "id": "kuBPFIRZXdc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the feature with the lowest model coefficient\n",
        "X = diabetes_df[['pregnant', 'glucose', 'triceps', 'insulin', 'bmi', 'family', 'age']]\n",
        "\n",
        "# Performs a 25-75% train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
        "\n",
        "# Scales features and fits the logistic regression model\n",
        "lr.fit(scaler.fit_transform(X_train), y_train)\n",
        "\n",
        "# Calculates the accuracy on the test set and prints coefficients\n",
        "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
        "print(f\"{acc:.1%} accuracy on test set.\")\n",
        "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
      ],
      "metadata": {
        "id": "0dA6QB4BXdvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/3\n",
        "\n",
        "Run the code and remove 2 more features with the lowest model coefficients."
      ],
      "metadata": {
        "id": "gcClaf7TX3MH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the 2 features with the lowest model coefficients\n",
        "X = diabetes_df[['glucose', 'triceps', 'bmi', 'family', 'age']]\n",
        "\n",
        "# Performs a 25-75% train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
        "\n",
        "# Scales features and fits the logistic regression model\n",
        "lr.fit(scaler.fit_transform(X_train), y_train)\n",
        "\n",
        "# Calculates the accuracy on the test set and prints coefficients\n",
        "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
        "print(f\"{acc:.1%} accuracy on test set.\")\n",
        "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
      ],
      "metadata": {
        "id": "k2VjsPCvX4JY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/3\n",
        "\n",
        "Run the code and only keep the feature with the highest coefficient.\n"
      ],
      "metadata": {
        "id": "cqtl1783YCeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Only keep the feature with the highest coefficient\n",
        "X = diabetes_df[['glucose']]\n",
        "\n",
        "# Performs a 25-75% train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
        "\n",
        "# Scales features and fits the logistic regression model to the data\n",
        "lr.fit(scaler.fit_transform(X_train), y_train)\n",
        "\n",
        "# Calculates the accuracy on the test set and prints coefficients\n",
        "acc = accuracy_score(y_test, lr.predict(scaler.transform(X_test)))\n",
        "print(f\"{acc:.1%} accuracy on test set.\")\n",
        "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))"
      ],
      "metadata": {
        "id": "Woo1vpTKYDWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automatic Recursive Feature Elimination\n",
        "Now let's automate this recursive process. Wrap a Recursive Feature Eliminator (RFE) around our logistic regression estimator and pass it the desired number of features.\n",
        "\n",
        "All the necessary functions and packages have been pre-loaded and the features have been scaled for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create the RFE with a LogisticRegression() estimator and 3 features to select.\n",
        "Print the features and their ranking.\n",
        "Print the features that are not eliminated."
      ],
      "metadata": {
        "id": "N1q71s1ep-tZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the RFE with a LogisticRegression estimator and 3 features to select\n",
        "rfe = RFE(estimator=LogisticRegression(), n_features_to_select=3, verbose=1)\n",
        "\n",
        "# Fits the eliminator to the data\n",
        "rfe.fit(X_train, y_train)\n",
        "\n",
        "# Print the features and their ranking (high = dropped early on)\n",
        "print(dict(zip(X.columns, rfe.ranking_)))\n",
        "\n",
        "# Print the features that are not eliminated\n",
        "print(X.columns[rfe.support_])\n",
        "\n",
        "# Calculates the test set accuracy\n",
        "acc = accuracy_score(y_test, rfe.predict(X_test))\n",
        "print(f\"{acc:.1%} accuracy on test set.\")"
      ],
      "metadata": {
        "id": "yMsHvfFXp_tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a random forest model\n",
        "You'll again work on the Pima Indians dataset to predict whether an individual has diabetes. This time using a random forest classifier. You'll fit the model on the training data after performing the train-test split and consult the feature importance values.\n",
        "\n",
        "The feature and target datasets have been pre-loaded for you as X and y. Same goes for the necessary packages and functions.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Set a 25% test size to perform a 75%-25% train-test split.\n",
        "Fit the random forest classifier to the training data.\n",
        "Calculate the accuracy on the test set.\n",
        "Print the feature importances per feature."
      ],
      "metadata": {
        "id": "lH57sSaIvNxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a 75% training and 25% test data split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
        "\n",
        "# Fit the random forest model to the training data\n",
        "rf = RandomForestClassifier(random_state=0)\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Calculate the accuracy\n",
        "acc = accuracy_score(y_test, rf.predict(X_test))\n",
        "\n",
        "# Print the importances per feature\n",
        "print(dict(zip(X.columns, rf.feature_importances_.round(2))))\n",
        "\n",
        "# Print accuracy\n",
        "print(f\"{acc:.1%} accuracy on test set.\")"
      ],
      "metadata": {
        "id": "yVg92CGNvOEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random forest for feature selection\n",
        "Now lets use the fitted random model to select the most important features from our input dataset X.\n",
        "\n",
        "The trained model from the previous exercise has been pre-loaded for you as rf.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "2\n",
        "Create a mask for features with an importance higher than 0.15."
      ],
      "metadata": {
        "id": "rcMYcF2YxtZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a mask for features importances above the threshold\n",
        "mask = rf.feature_importances_ > 0.15\n",
        "\n",
        "# Prints out the mask\n",
        "print(mask)"
      ],
      "metadata": {
        "id": "viZj-48bxtp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "50 XP\n",
        "2\n",
        "Sub-select the most important features by applying the mask to X."
      ],
      "metadata": {
        "id": "ag4hqF9txwCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a mask for features importances above the threshold\n",
        "mask = rf.feature_importances_ > 0.15\n",
        "\n",
        "# Apply the mask to the feature dataset X\n",
        "reduced_X = X.loc[:, mask]\n",
        "\n",
        "# prints out the selected column names\n",
        "print(reduced_X.columns)"
      ],
      "metadata": {
        "id": "VDckRO6rxwYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recursive Feature Elimination with random forests\n",
        "You'll wrap a Recursive Feature Eliminator around a random forest model to remove features step by step. This method is more conservative compared to selecting features after applying a single importance threshold. Since dropping one feature can influence the relative importances of the others.\n",
        "\n",
        "You'll need these pre-loaded datasets: X, X_train, y_train.\n",
        "\n",
        "Functions and classes that have been pre-loaded for you are: RandomForestClassifier(), RFE(), train_test_split().\n",
        "\n",
        "Instructions 1/4\n",
        "25 XP\n",
        "2\n",
        "3\n",
        "4\n",
        "Create a recursive feature eliminator that will select the 2 most important features using a random forest model."
      ],
      "metadata": {
        "id": "A-Op8dBTyKWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap the feature eliminator around the random forest model\n",
        "rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, verbose=1)"
      ],
      "metadata": {
        "id": "hwwlBT30yK5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/4\n",
        "25 XP\n",
        "3\n",
        "4\n",
        "Fit the recursive feature eliminator to the training data."
      ],
      "metadata": {
        "id": "b1IqFK50yWOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap the feature eliminator around the random forest model\n",
        "rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, verbose=1)\n",
        "\n",
        "# Fit the model to the training data\n",
        "rfe.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "Q9MI3DjwyWbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/4\n",
        "25 XP\n",
        "3\n",
        "4\n",
        "Create a mask using the fitted eliminator's support_ attribute, then apply it to the feature dataset X."
      ],
      "metadata": {
        "id": "t-b09UxJyeSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Wrap the feature eliminator around the random forest model\n",
        "rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, verbose=1)\n",
        "\n",
        "# Fit the model to the training data\n",
        "rfe.fit(X_train, y_train)\n",
        "\n",
        "# Create a mask using the support_ attribute of rfe\n",
        "mask = rfe.support_\n",
        "\n",
        "# Apply the mask to the feature dataset X and print the result\n",
        "reduced_X = X.loc[:, mask]\n",
        "print(reduced_X.columns)"
      ],
      "metadata": {
        "id": "M75WM5RQyfmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 4/4\n",
        "25 XP\n",
        "Change the settings of RFE() to eliminate 2 features at each step.\n"
      ],
      "metadata": {
        "id": "R_Je9sKryu1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the feature eliminator to remove 2 features on each step\n",
        "rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, step=2, verbose=1)\n",
        "\n",
        "# Fit the model to the training data\n",
        "rfe.fit(X_train, y_train)\n",
        "\n",
        "# Create a mask\n",
        "mask = rfe.support_\n",
        "\n",
        "# Apply the mask to the feature dataset X and print the result\n",
        "reduced_X = X.loc[:, mask]\n",
        "print(reduced_X.columns)"
      ],
      "metadata": {
        "id": "MpdBlxc3yvR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a LASSO regressor\n",
        "You'll be working on the numeric ANSUR body measurements dataset to predict a persons Body Mass Index (BMI) using the pre-imported Lasso() regressor. BMI is a metric derived from body height and weight but those two features have been removed from the dataset to give the model a challenge.\n",
        "\n",
        "You'll standardize the data first using the StandardScaler() that has been instantiated for you as scaler to make sure all coefficients face a comparable regularizing force trying to bring them down.\n",
        "\n",
        "All necessary functions and classes plus the input datasets X and y have been pre-loaded.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Set the test size to 30% to get a 70-30% train test split.\n",
        "Fit the scaler on the training features and transform these in one go.\n",
        "Create the Lasso model.\n",
        "Fit it to the scaled training data."
      ],
      "metadata": {
        "id": "8kNOiMBjmAcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the test size to 30% to get a 70-30% train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "# Fit the scaler on the training features and transform these in one go\n",
        "X_train_std = scaler.fit_transform(X_train)\n",
        "\n",
        "# Create the Lasso model\n",
        "la = Lasso()\n",
        "\n",
        "# Fit it to the standardized training data\n",
        "la.fit(X_train_std, y_train)"
      ],
      "metadata": {
        "id": "kX8pbC-LmA7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lasso model results\n",
        "Now that you've trained the Lasso model, you'll score its predictive capacity (\n",
        ") on the test set and count how many features are ignored because their coefficient is reduced to zero.\n",
        "\n",
        "The X_test and y_test datasets have been pre-loaded for you.\n",
        "\n",
        "The Lasso() model and StandardScaler() have been instantiated as la and scaler respectively and both were fitted to the training data.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Transform the test set with the pre-fitted scaler.\n",
        "Calculate the\n",
        " value on the scaled test data.\n",
        "Create a list that has True values when coefficients equal 0.\n",
        "Calculate the total number of features with a coefficient of 0."
      ],
      "metadata": {
        "id": "wtWlNPL4nVmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the test set with the pre-fitted scaler\n",
        "X_test_std = scaler.transform(X_test)\n",
        "\n",
        "# Calculate the coefficient of determination (R squared) on X_test_std\n",
        "r_squared = la.score(X_test_std, y_test)\n",
        "print(f\"The model can predict {r_squared:.1%} of the variance in the test set.\")\n",
        "\n",
        "# Create a list that has True values when coefficients equal 0\n",
        "zero_coef = la.coef_ == 0\n",
        "\n",
        "# Calculate how many features have a zero coefficient\n",
        "n_ignored = sum(zero_coef)\n",
        "print(f\"The model has ignored {n_ignored} out of {len(la.coef_)} features.\")"
      ],
      "metadata": {
        "id": "q8BjLaD0nV4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adjusting the regularization strength\n",
        "Your current Lasso model has an\n",
        " score of 84.7%. When a model applies overly powerful regularization it can suffer from high bias, hurting its predictive power.\n",
        "\n",
        "Let's improve the balance between predictive power and model simplicity by tweaking the alpha parameter.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Find the highest value for alpha that gives an\n",
        " value above 98% from the options: 1, 0.5, 0.1, and 0.01."
      ],
      "metadata": {
        "id": "grItd_yioE4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the highest alpha value with R-squared above 98%\n",
        "la = Lasso(alpha = 0.1, random_state=0)\n",
        "\n",
        "# Fits the model and calculates performance stats\n",
        "la.fit(X_train_std, y_train)\n",
        "r_squared = la.score(X_test_std, y_test)\n",
        "n_ignored_features = sum(la.coef_ == 0)\n",
        "\n",
        "# Print peformance stats\n",
        "print(f\"The model can predict {r_squared:.1%} of the variance in the test set.\")\n",
        "print(f\"{n_ignored_features} out of {len(la.coef_)} features were ignored.\")"
      ],
      "metadata": {
        "id": "OQIH1EdXoFKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a LassoCV regressor\n",
        "You'll be predicting biceps circumference on a subsample of the male ANSUR dataset using the LassoCV() regressor that automatically tunes the regularization strength (alpha value) using Cross-Validation.\n",
        "\n",
        "The standardized training and test data has been pre-loaded for you as X_train, X_test, y_train, and y_test.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create and fit the LassoCV model on the training set.\n",
        "Calculate\n",
        " on the test set.\n",
        "Create a mask for coefficients not equal to zero."
      ],
      "metadata": {
        "id": "tfJSGEhrzgFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LassoCV\n",
        "\n",
        "# Create and fit the LassoCV model on the training set\n",
        "lcv = LassoCV()\n",
        "lcv.fit(X_train, y_train)\n",
        "print(f'Optimal alpha = {lcv.alpha_:.3f}')\n",
        "\n",
        "# Calculate R squared on the test set\n",
        "r_squared = lcv.score(X_test, y_test)\n",
        "print(f'The model explains {r_squared:.1%} of the test set variance')\n",
        "\n",
        "# Create a mask for coefficients not equal to zero\n",
        "lcv_mask = lcv.coef_ != 0\n",
        "print(f'{sum(lcv_mask)} features out of {len(lcv_mask)} selected')"
      ],
      "metadata": {
        "id": "MPfQJgZzzg_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble models for extra votes\n",
        "The LassoCV() model selected 22 out of 32 features. Not bad, but not a spectacular dimensionality reduction either. Let's use two more models to select the 10 features they consider most important using the Recursive Feature Eliminator (RFE).\n",
        "\n",
        "The standardized training and test data has been pre-loaded for you as X_train, X_test, y_train, and y_test.\n",
        "\n",
        "Instructions 1/4\n",
        "25 XP\n",
        "2\n",
        "3\n",
        "4\n",
        "Select 10 features with RFE on a GradientBoostingRegressor and drop 3 features on each step."
      ],
      "metadata": {
        "id": "DmmsUStvz9CJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Select 10 features with RFE on a GradientBoostingRegressor, drop 3 features on each step\n",
        "rfe_gb = RFE(estimator=GradientBoostingRegressor(),\n",
        "             n_features_to_select=10, step=3, verbose=1)\n",
        "rfe_gb.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "jgCUJU0rz9Yu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/4\n",
        "25 XP\n",
        "3\n",
        "4\n",
        "Calculate the\n",
        " on the test set."
      ],
      "metadata": {
        "id": "eIl9DH6h0Fb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Select 10 features with RFE on a GradientBoostingRegressor, drop 3 features on each step\n",
        "rfe_gb = RFE(estimator=GradientBoostingRegressor(),\n",
        "             n_features_to_select=10, step=3, verbose=1)\n",
        "rfe_gb.fit(X_train, y_train)\n",
        "\n",
        "# Calculate the R squared on the test set\n",
        "r_squared = rfe_gb.score(X_test, y_test)\n",
        "print(f'The model can explain {r_squared:.1%} of the variance in the test set')"
      ],
      "metadata": {
        "id": "Umm9QvxT0Fmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/4\n",
        "25 XP\n",
        "4\n",
        "Assign the support array of the fitted model to gb_mask."
      ],
      "metadata": {
        "id": "Z8-3GJLn0T4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Select 10 features with RFE on a GradientBoostingRegressor, drop 3 features on each step\n",
        "rfe_gb = RFE(estimator=GradientBoostingRegressor(),\n",
        "             n_features_to_select=10, step=3, verbose=1)\n",
        "rfe_gb.fit(X_train, y_train)\n",
        "\n",
        "# Calculate the R squared on the test set\n",
        "r_squared = rfe_gb.score(X_test, y_test)\n",
        "print(f'The model can explain {r_squared:.1%} of the variance in the test set')\n",
        "\n",
        "# Assign the support array to gb_mask\n",
        "gb_mask = rfe_gb.support_"
      ],
      "metadata": {
        "id": "3DeMC5Aa0UKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 4/4\n",
        "25 XP\n",
        "Modify the first step to select 10 features with RFE on a RandomForestRegressor() and drop 3 features on each step."
      ],
      "metadata": {
        "id": "Ir9gU19j0dWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# Select 10 features with RFE on a RandomForestRegressor, drop 3 features on each step\n",
        "rfe_rf = RFE(estimator=RandomForestRegressor(),\n",
        "             n_features_to_select=10, step=3, verbose=1)\n",
        "rfe_rf.fit(X_train, y_train)\n",
        "\n",
        "# Calculate the R squared on the test set\n",
        "r_squared = rfe_rf.score(X_test, y_test)\n",
        "print(f'The model can explain {r_squared:.1%} of the variance in the test set')\n",
        "\n",
        "# Assign the support array to rf_mask\n",
        "rf_mask = rfe_rf.support_"
      ],
      "metadata": {
        "id": "SUhAqpLK0dkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combining 3 feature selectors\n",
        "We'll combine the votes of the 3 models you built in the previous exercises, to decide which features are important into a meta mask. We'll then use this mask to reduce dimensionality and see how a simple linear regressor performs on the reduced dataset.\n",
        "\n",
        "The per model votes have been pre-loaded as lcv_mask, rf_mask, and gb_mask and the feature and target datasets as X and y.\n",
        "\n",
        "Instructions 1/4\n",
        "25 XP\n",
        "2\n",
        "3\n",
        "4\n",
        "Sum the votes of the three models using np.sum()."
      ],
      "metadata": {
        "id": "pPu0rLtEDFfw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sum the votes of the three models\n",
        "votes = np.sum([lcv_mask, rf_mask, gb_mask], axis = 0)\n",
        "print(votes)"
      ],
      "metadata": {
        "id": "5RfszEzyDG8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/4\n",
        "25 XP\n",
        "3\n",
        "4\n",
        "Create a mask for features selected by all 3 models."
      ],
      "metadata": {
        "id": "pzxWnJCJDb_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sum the votes of the three models\n",
        "votes = np.sum([lcv_mask, rf_mask, gb_mask], axis=0)\n",
        "\n",
        "# Create a mask for features selected by all 3 models\n",
        "meta_mask = votes >= 3\n",
        "print(meta_mask)"
      ],
      "metadata": {
        "id": "IDU2SnS4DcKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/4\n",
        "25 XP\n",
        "4\n",
        "Apply the dimensionality reduction on X and print which features were selected."
      ],
      "metadata": {
        "id": "_EW7F8PLDlo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sum the votes of the three models\n",
        "votes = np.sum([lcv_mask, rf_mask, gb_mask], axis=0)\n",
        "\n",
        "# Create a mask for features selected by all 3 models\n",
        "meta_mask = votes == 3\n",
        "\n",
        "# Apply the dimensionality reduction on X\n",
        "X_reduced = X.loc[:, meta_mask]\n",
        "print(X_reduced.columns)"
      ],
      "metadata": {
        "id": "yexVkMsADl2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 4/4\n",
        "25 XP\n",
        "Plug the reduced dataset into the code for simple linear regression that has been written for you."
      ],
      "metadata": {
        "id": "ZoutpnP4DyPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sum the votes of the three models\n",
        "votes = np.sum([lcv_mask, rf_mask, gb_mask], axis=0)\n",
        "\n",
        "# Create a mask for features selected by all 3 models\n",
        "meta_mask = votes == 3\n",
        "\n",
        "# Apply the dimensionality reduction on X\n",
        "X_reduced = X.loc[:, meta_mask]\n",
        "\n",
        "# Plug the reduced dataset into a linear regression pipeline\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=0)\n",
        "lm.fit(scaler.fit_transform(X_train), y_train)\n",
        "r_squared = lm.score(scaler.transform(X_test), y_test)\n",
        "print(f'The model can explain {r_squared:.1%} of the variance in the test set using {len(lm.coef_)} features.')"
      ],
      "metadata": {
        "id": "nH34xKGnD0qi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manual feature extraction I\n",
        "You want to compare prices for specific products between stores. The features in the pre-loaded dataset sales_df are: storeID, product, quantity and revenue. The quantity and revenue features tell you how many items of a particular product were sold in a store and what the total revenue was. For the purpose of your analysis it's more interesting to know the average price per product.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Calculate the product price from the quantity sold and total revenue.\n",
        "Drop the quantity and revenue features from the dataset."
      ],
      "metadata": {
        "id": "4fZExMgGHjvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the price from the quantity sold and revenue\n",
        "sales_df['price'] = sales_df['revenue'] / sales_df['quantity']\n",
        "\n",
        "# Drop the quantity and revenue features\n",
        "reduced_df = sales_df.drop(['quantity', 'revenue'], axis=1)\n",
        "\n",
        "print(reduced_df.head())"
      ],
      "metadata": {
        "id": "zhT4jPGHHkAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Manual feature extraction II\n",
        "You're working on a variant of the ANSUR dataset, height_df, where a person's height was measured 3 times: height_1, height_2, height_3. Add a feature with the mean height to the dataset, then drop the 3 original features.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Add a feature with the mean height to the dataset. Use the .mean() method with axis=1.\n",
        "Drop the 3 original height features from the dataset."
      ],
      "metadata": {
        "id": "u6JIH6SjIHLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the mean height\n",
        "height_df['height'] = height_df[['height_1', 'height_2', 'height_3']].mean(axis=1)\n",
        "\n",
        "# Drop the 3 original height features\n",
        "reduced_df = height_df.drop(['height_1', 'height_2', 'height_3'], axis=1)\n",
        "\n",
        "print(reduced_df.head())"
      ],
      "metadata": {
        "id": "vFhFgIeEIHon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating Principal Components\n",
        "You'll visually inspect a 4 feature sample of the ANSUR dataset before and after PCA using Seaborn's pairplot(). This will allow you to inspect the pairwise correlations between the features.\n",
        "\n",
        "The data has been pre-loaded for you as ansur_df.\n",
        "\n",
        "Instructions 1/4\n",
        "25 XP\n",
        "2\n",
        "3\n",
        "4\n",
        "Create a Seaborn pairplot to inspect ansur_df."
      ],
      "metadata": {
        "id": "emt6JTMZRWXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pairplot to inspect ansur_df\n",
        "sns.pairplot(ansur_df)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G8mE9eYxRg_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/4\n",
        "25 XP\n",
        "3\n",
        "4\n",
        "Create the scaler and standardize the data."
      ],
      "metadata": {
        "id": "JUpxkrkYUd2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create the scaler and standardize the data\n",
        "scaler = StandardScaler()\n",
        "ansur_std = scaler.fit_transform(ansur_df)"
      ],
      "metadata": {
        "id": "xahLSnQsUeF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/4\n",
        "25 XP\n",
        "4\n",
        "Create the PCA() instance and fit and transform the standardized data."
      ],
      "metadata": {
        "id": "r3VEaxooUqPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Create the scaler and standardize the data\n",
        "scaler = StandardScaler()\n",
        "ansur_std = scaler.fit_transform(ansur_df)\n",
        "\n",
        "# Create the PCA instance and fit and transform the data with pca\n",
        "pca = PCA()\n",
        "pc = pca.fit_transform(ansur_std)\n",
        "\n",
        "# This changes the numpy array output back to a DataFrame\n",
        "pc_df = pd.DataFrame(pc, columns=['PC 1', 'PC 2', 'PC 3', 'PC 4'])"
      ],
      "metadata": {
        "id": "GYXFWa71UqrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 4/4\n",
        "25 XP\n",
        "4\n",
        "Create a pairplot of the principal component DataFrame."
      ],
      "metadata": {
        "id": "CqtH756FU_ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Create the scaler\n",
        "scaler = StandardScaler()\n",
        "ansur_std = scaler.fit_transform(ansur_df)\n",
        "\n",
        "# Create the PCA instance and fit and transform the data with pca\n",
        "pca = PCA()\n",
        "pc = pca.fit_transform(ansur_std)\n",
        "pc_df = pd.DataFrame(pc, columns=['PC 1', 'PC 2', 'PC 3', 'PC 4'])\n",
        "\n",
        "# Create a pairplot of the principal component DataFrame\n",
        "sns.pairplot(pc_df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B60XjwzaU_59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA on a larger dataset\n",
        "You'll now apply PCA on a somewhat larger ANSUR datasample with 13 dimensions, once again pre-loaded as ansur_df. The fitted model will be used in the next exercise. Since we are not using the principal components themselves there is no need to transform the data, instead, it is sufficient to fit pca to the data.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create the scaler.\n",
        "Standardize the data.\n",
        "Create the PCA() instance.\n",
        "Fit it to the standardized data."
      ],
      "metadata": {
        "id": "V84PlPoyVfaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "ansur_std = scaler.fit_transform(ansur_df)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "pca.fit(ansur_std)"
      ],
      "metadata": {
        "id": "Js5T1ejMVfuX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}