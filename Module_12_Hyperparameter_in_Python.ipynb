{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPrlp0RujaDz7pgN6Nsj9n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pattiecodes/DC_MLScientist_in_Py/blob/main/Module_12_Hyperparameter_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Module Start ---"
      ],
      "metadata": {
        "id": "-VcIrbIjQEkO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting a Logistic Regression parameter\n",
        "You are now going to practice extracting an important parameter of the logistic regression model. The logistic regression has a few other parameters you will not explore here but you can review them in the scikit-learn.org documentation for the LogisticRegression() module under 'Attributes'.\n",
        "\n",
        "This parameter is important for understanding the direction and magnitude of the effect the variables have on the target.\n",
        "\n",
        "In this exercise we will extract the coefficient parameter (found in the coef_ attribute), zip it up with the original column names, and see which variables had the largest positive effect on the target variable.\n",
        "\n",
        "You will have available:\n",
        "\n",
        "A logistic regression model object named log_reg_clf\n",
        "The X_train DataFrame\n",
        "sklearn and pandas have been imported for you.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a list of the original column names used in the training DataFrame.\n",
        "Extract the coefficients of the logistic regression estimator.\n",
        "Create a DataFrame of coefficients and variable names & view it.\n",
        "Print out the top 3 'positive' variables based on the coefficient size."
      ],
      "metadata": {
        "id": "AlRQVUS9QG5t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxXSwXIOt8Oy"
      },
      "outputs": [],
      "source": [
        "# Create a list of original variable names from the training DataFrame\n",
        "original_variables = list(X_train.columns)\n",
        "\n",
        "# Extract the coefficients of the logistic regression estimator\n",
        "model_coefficients = log_reg_clf.coef_[0]\n",
        "\n",
        "# Create a dataframe of the variables and coefficients & print it out\n",
        "coefficient_df = pd.DataFrame({\"Variable\" : original_variables, \"Coefficient\": model_coefficients})\n",
        "print(coefficient_df)\n",
        "\n",
        "# Print out the top 3 positive variables\n",
        "top_three_df = coefficient_df.sort_values(by=\"Coefficient\", axis=0, ascending=False)[0:3]\n",
        "print(top_three_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extracting a Random Forest parameter\n",
        "You will now translate the work previously undertaken on the logistic regression model to a random forest model. A parameter of this model is, for a given tree, how it decided to split at each level.\n",
        "\n",
        "This analysis is not as useful as the coefficients of logistic regression as you will be unlikely to ever explore every split and every tree in a random forest model. However, it is a very useful exercise to peek under the hood at what the model is doing.\n",
        "\n",
        "In this exercise we will extract a single tree from our random forest model, visualize it and programmatically extract one of the splits.\n",
        "\n",
        "You have available:\n",
        "\n",
        "A random forest model object, rf_clf\n",
        "An image of the top of the chosen decision tree, tree_viz_image\n",
        "The X_train DataFrame & the original_variables list\n",
        "Instructions\n",
        "100 XP\n",
        "Extract the 7th tree (6th index) from the random forest model.\n",
        "Visualize this tree (tree_viz_image) to see the split decisions.\n",
        "Extract the feature & level of the top split.\n",
        "Print out the feature and level together."
      ],
      "metadata": {
        "id": "zsZDabeKRFeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the 7th (index 6) tree from the random forest\n",
        "chosen_tree = rf_clf.estimators_[6]\n",
        "\n",
        "# Visualize the graph using the provided image\n",
        "imgplot = plt.imshow(tree_viz_image)\n",
        "plt.show()\n",
        "\n",
        "# Extract the parameters and level of the top (index 0) node\n",
        "split_column = chosen_tree.tree_.feature[0]\n",
        "split_column_name = X_train.columns[split_column]\n",
        "split_value = chosen_tree.tree_.threshold[0]\n",
        "\n",
        "# Print out the feature and level\n",
        "print(\"This node split on feature {}, at a value of {}\".format(split_column_name, split_value))"
      ],
      "metadata": {
        "id": "i3eyjuATRFyx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}