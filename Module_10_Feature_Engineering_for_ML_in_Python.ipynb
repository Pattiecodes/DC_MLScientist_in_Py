{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKtCQSjvXlXPlnQo3Br2Dr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pattiecodes/DC_MLScientist_in_Py/blob/main/Module_10_Feature_Engineering_for_ML_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Module Start ---"
      ],
      "metadata": {
        "id": "XrOijaSIQU8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting to know your data\n",
        "Pandas is one the most popular packages used to work with tabular data in Python. It is generally imported using the alias pd and can be used to load a CSV (or other delimited files) using read_csv().\n",
        "\n",
        "You will be working with a modified subset of the Stackoverflow survey response data in the first three chapters of this course. This dataset records the details, and preferences of thousands of users of the StackOverflow website.\n",
        "\n",
        "Instructions 1/4\n",
        "25 XP\n",
        "2\n",
        "3\n",
        "4\n",
        "Import the pandas library as pd.\n",
        "so_survey_csv contains the URL to a CSV file. Import it using Pandas into so_survey_df."
      ],
      "metadata": {
        "id": "NJAnqrvLOBL9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T18HV5PnQL4K"
      },
      "outputs": [],
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Import so_survey_csv into so_survey_df\n",
        "so_survey_df = pd.read_csv(so_survey_csv)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/4\n",
        "25 XP\n",
        "3\n",
        "4\n",
        "Print the first five rows of so_survey_df."
      ],
      "metadata": {
        "id": "EtrLAMrgOIwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Import so_survey_csv into so_survey_df\n",
        "so_survey_df = pd.read_csv(so_survey_csv)\n",
        "\n",
        "# Print the first five rows of the DataFrame\n",
        "print(so_survey_df.head())"
      ],
      "metadata": {
        "id": "kcKhU41zOI7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/4\n",
        "25 XP\n",
        "4\n",
        "Print the data type of each column in so_survey_df."
      ],
      "metadata": {
        "id": "8ArUxMPNOdoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Import so_survey_csv into so_survey_df\n",
        "so_survey_df = pd.read_csv(so_survey_csv)\n",
        "\n",
        "# Print the first five rows of the DataFrame\n",
        "print(so_survey_df.head())\n",
        "\n",
        "# Print the data type of each column\n",
        "print(so_survey_df.dtypes)"
      ],
      "metadata": {
        "id": "BF9CpsD-Od2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 4/4\n",
        "25 XP\n",
        "Question\n",
        "What type of data is the ConvertedSalary column?\n",
        "\n",
        "Possible answers\n",
        "\n",
        "\n",
        "Datetime\n",
        "\n",
        "**Numeric**\n",
        "\n",
        "String\n",
        "\n",
        "Boolean\n"
      ],
      "metadata": {
        "id": "rwECr0fwO1xo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Selecting specific data types\n",
        "Often a dataset will contain columns with several different data types (like the one you are working with). The majority of machine learning models require you to have a consistent data type across features. Similarly, most feature engineering techniques are applicable to only one type of data at a time. For these reasons among others, you will often want to be able to access just the columns of certain types when working with a DataFrame.\n",
        "\n",
        "The DataFrame (so_survey_df) from the previous exercise is available in your workspace.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a subset of so_survey_df consisting of only the numeric (int and float) columns.\n",
        "Print the column names contained in so_survey_df_num.\n"
      ],
      "metadata": {
        "id": "7pN_aBqcQQY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create subset of only the numeric columns\n",
        "so_numeric_df = so_survey_df.select_dtypes(include=[int, float])\n",
        "\n",
        "# Print the column names contained in so_survey_df_num\n",
        "print(so_numeric_df.columns)"
      ],
      "metadata": {
        "id": "dV6gJiLmQT01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-hot encoding and dummy variables\n",
        "To use categorical variables in a machine learning model, you first need to represent them in a quantitative way. The two most common approaches are to one-hot encode the variables using or to use dummy variables. In this exercise, you will create both types of encoding, and compare the created column sets. We will continue using the same DataFrame from previous lesson loaded as so_survey_df and focusing on its Country column.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "One-hot encode the Country column, adding \"OH\" as a prefix for each column."
      ],
      "metadata": {
        "id": "3FSfcUVD-D25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the Country column to a one hot encoded Data Frame\n",
        "one_hot_encoded = pd.get_dummies(so_survey_df, columns=['Country'], prefix='OH')\n",
        "\n",
        "# Print the columns names\n",
        "print(one_hot_encoded.columns)"
      ],
      "metadata": {
        "id": "0jzhZqrL-Go1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "\n",
        "Create dummy variables for the Country column, adding \"DM\" as a prefix for each column."
      ],
      "metadata": {
        "id": "uMnm1lN_-l7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dummy variables for the Country column\n",
        "dummy = pd.get_dummies(so_survey_df, columns=['Country'], drop_first=True, prefix='DM')\n",
        "\n",
        "# Print the columns names\n",
        "print(dummy.columns)"
      ],
      "metadata": {
        "id": "YzODGMAT-nrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dealing with uncommon categories\n",
        "Some features can have many different categories but a very uneven distribution of their occurrences. Take for example Data Science's favorite languages to code in, some common choices are Python, R, and Julia, but there can be individuals with bespoke choices, like FORTRAN, C etc. In these cases, you may not want to create a feature for each value, but only the more common occurrences.\n",
        "\n",
        "Instructions 1/3\n",
        "35 XP\n",
        "2\n",
        "3\n",
        "Extract the Country column of so_survey_df as a series and assign it to countries.\n",
        "Find the counts of each category in the newly created countries series."
      ],
      "metadata": {
        "id": "xPv0lTQDAEqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a series out of the Country column\n",
        "countries = so_survey_df['Country']\n",
        "\n",
        "# Get the counts of each category\n",
        "country_counts = countries.value_counts()\n",
        "\n",
        "# Print the count values for each category\n",
        "print(country_counts)"
      ],
      "metadata": {
        "id": "xOAfX7kzAGRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/3\n",
        "35 XP\n",
        "3\n",
        "Create a mask for values occurring less than 10 times in country_counts.\n",
        "Print the first 5 rows of the mask."
      ],
      "metadata": {
        "id": "D7mr0L3xAmY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a series out of the Country column\n",
        "countries = so_survey_df['Country']\n",
        "\n",
        "# Get the counts of each category\n",
        "country_counts = countries.value_counts()\n",
        "\n",
        "# Create a mask for only categories that occur less than 10 times\n",
        "mask = countries.isin(country_counts[country_counts < 10].index)\n",
        "\n",
        "# Print the top 5 rows in the mask series\n",
        "print(mask.head())"
      ],
      "metadata": {
        "id": "Fo9kJzFyAmoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/3\n",
        "30 XP\n",
        "Label values occurring less than the mask cutoff as 'Other'.\n",
        "Print the new category counts in countries."
      ],
      "metadata": {
        "id": "ku21UkipA-iZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a series out of the Country column\n",
        "countries = so_survey_df['Country']\n",
        "\n",
        "# Get the counts of each category\n",
        "country_counts = countries.value_counts()\n",
        "\n",
        "# Create a mask for only categories that occur less than 10 times\n",
        "mask = countries.isin(country_counts[country_counts < 10].index)\n",
        "\n",
        "# Label all other categories as Other\n",
        "countries[mask] = 'Other'\n",
        "\n",
        "# Print the updated category counts\n",
        "print(countries.value_counts())"
      ],
      "metadata": {
        "id": "dZo5sIq7A-uM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binarizing columns\n",
        "While numeric values can often be used without any feature engineering, there will be cases when some form of manipulation can be useful. For example on some occasions, you might not care about the magnitude of a value but only care about its direction, or if it exists at all. In these situations, you will want to binarize a column. In the so_survey_df data, you have a large number of survey respondents that are working voluntarily (without pay). You will create a new column titled Paid_Job indicating whether each person is paid (their salary is greater than zero).\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a new column called Paid_Job filled with zeros.\n",
        "Replace all the Paid_Job values with a 1 where the corresponding ConvertedSalary is greater than 0.\n"
      ],
      "metadata": {
        "id": "TR5EinZ3C3Zv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Paid_Job column filled with zeros\n",
        "so_survey_df['Paid_Job'] = 0\n",
        "\n",
        "# Replace all the Paid_Job values where ConvertedSalary is > 0\n",
        "so_survey_df.loc[so_survey_df['ConvertedSalary'] > 0, 'Paid_Job'] = 1\n",
        "\n",
        "# Print the first five rows of the columns\n",
        "print(so_survey_df[['Paid_Job', 'ConvertedSalary']].head())"
      ],
      "metadata": {
        "id": "Y5aVIhTgC3yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binning values\n",
        "For many continuous values you will care less about the exact value of a numeric column, but instead care about the bucket it falls into. This can be useful when plotting values, or simplifying your machine learning models. It is mostly used on continuous variables where accuracy is not the biggest concern e.g. age, height, wages.\n",
        "\n",
        "Bins are created using pd.cut(df['column_name'], bins) where bins can be an integer specifying the number of evenly spaced bins, or a list of bin boundaries.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "Bin the value of the ConvertedSalary column in so_survey_df into 5 equal bins, in a new column called equal_binned."
      ],
      "metadata": {
        "id": "dqtEeHAOEPcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bin the continuous variable ConvertedSalary into 5 bins\n",
        "so_survey_df['equal_binned'] = pd.cut(so_survey_df['ConvertedSalary'],\n",
        "                                        bins = 5)\n",
        "\n",
        "# Print the first 5 rows of the equal_binned column\n",
        "print(so_survey_df[['equal_binned', 'ConvertedSalary']].head())"
      ],
      "metadata": {
        "id": "alTobTtmEPvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "\n",
        "Bin the ConvertedSalary column using the boundaries in the list bins and label the bins using labels."
      ],
      "metadata": {
        "id": "ATs6Uh8fEfAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import numpy\n",
        "import numpy as np\n",
        "\n",
        "# Specify the boundaries of the bins\n",
        "bins = [-np.inf, 10000, 50000, 100000, 150000, np.inf]\n",
        "\n",
        "# Bin labels\n",
        "labels = ['Very low', 'Low', 'Medium', 'High', 'Very high']\n",
        "\n",
        "# Bin the continuous variable ConvertedSalary using these boundaries\n",
        "so_survey_df['boundary_binned'] = pd.cut(so_survey_df['ConvertedSalary'],\n",
        "                                         bins = bins, labels = labels)\n",
        "\n",
        "# Print the first 5 rows of the boundary_binned column\n",
        "print(so_survey_df[['boundary_binned', 'ConvertedSalary']].head())"
      ],
      "metadata": {
        "id": "mQpGKE8xEfNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How sparse is my data?\n",
        "Most datasets contain missing values, often represented as NaN (Not a Number). If you are working with Pandas you can easily check how many missing values exist in each column.\n",
        "\n",
        "Let's find out how many of the developers taking the survey chose to enter their age (found in the Age column of so_survey_df) and their gender (Gender column of so_survey_df).\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "2\n",
        "Subset the DataFrame to only include the 'Age' and 'Gender' columns.\n",
        "Print the number of non-missing values in both columns."
      ],
      "metadata": {
        "id": "w33y2gVkGOyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subset the DataFrame\n",
        "sub_df = so_survey_df[['Age', 'Gender']]\n",
        "\n",
        "# Print the number of non-missing values\n",
        "print(sub_df.notnull().sum())"
      ],
      "metadata": {
        "id": "EkRqpAVcGPFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding the missing values\n",
        "While having a summary of how much of your data is missing can be useful, often you will need to find the exact locations of these missing values. Using the same subset of the StackOverflow data from the last exercise (sub_df), you will show how a value can be flagged as missing.\n",
        "\n",
        "Instructions 1/3\n",
        "35 XP\n",
        "Print the first 10 entries of the DataFrame."
      ],
      "metadata": {
        "id": "BllPX8nqG3dM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the top 10 entries of the DataFrame\n",
        "print(sub_df.head(10))"
      ],
      "metadata": {
        "id": "sykQo8TWG4OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/3\n",
        "\n",
        "Print the locations of the missing values in the first 10 rows."
      ],
      "metadata": {
        "id": "_w_B49OMHBOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the locations of the missing values\n",
        "print(sub_df.head(10).isnull())"
      ],
      "metadata": {
        "id": "HDBD2KVCHCdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/3\n",
        "\n",
        "Print the locations of the non-missing values in the first 10 rows."
      ],
      "metadata": {
        "id": "qqYav0zXHH0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the locations of the non-missing values\n",
        "print(sub_df.head(10).notnull())"
      ],
      "metadata": {
        "id": "mOJy96WhHJVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Listwise deletion\n",
        "The simplest way to deal with missing values in your dataset when they are occurring entirely at random is to remove those rows, also called 'listwise deletion'.\n",
        "\n",
        "Depending on the use case, you will sometimes want to remove all missing values in your data while other times you may want to only remove a particular column if too many values are missing in that column.\n",
        "\n",
        "Instructions 1/4\n",
        "25 XP\n",
        "Print the number of rows and columns in so_survey_df."
      ],
      "metadata": {
        "id": "i1yz1aFqSm_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of rows and columns\n",
        "print(so_survey_df.shape)"
      ],
      "metadata": {
        "id": "ZucuCWzISncv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/4\n",
        "\n",
        "Drop all rows with missing values in so_survey_df."
      ],
      "metadata": {
        "id": "5EyQDwBBS4Xz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new DataFrame dropping all incomplete rows\n",
        "no_missing_values_rows = so_survey_df.dropna()\n",
        "\n",
        "# Print the shape of the new DataFrame\n",
        "print(no_missing_values_rows.shape)"
      ],
      "metadata": {
        "id": "ni8lldrSS6CG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/4\n",
        "\n",
        "Drop all columns with missing values in so_survey_df."
      ],
      "metadata": {
        "id": "vepVs6l2TOZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new DataFrame dropping all columns with incomplete rows\n",
        "no_missing_values_cols = so_survey_df.dropna(how='any', axis=1)\n",
        "\n",
        "# Print the shape of the new DataFrame\n",
        "print(no_missing_values_cols.shape)"
      ],
      "metadata": {
        "id": "j8k-oiv8TPc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 4/4\n",
        "\n",
        "Drop all rows in so_survey_df where 'Gender' is missing."
      ],
      "metadata": {
        "id": "hhbwv3suTkah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop all rows where Gender is missing\n",
        "no_gender = so_survey_df.dropna(subset=['Gender'])\n",
        "\n",
        "# Print the shape of the new DataFrame\n",
        "print(no_gender.shape)"
      ],
      "metadata": {
        "id": "9Xi5Ji33TlP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Replacing missing values with constants\n",
        "While removing missing data entirely maybe a correct approach in many situations, this may result in a lot of information being omitted from your models.\n",
        "\n",
        "You may find categorical columns where the missing value is a valid piece of information in itself, such as someone refusing to answer a question in a survey. In these cases, you can fill all missing values with a new category entirely, for example 'No response given'.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "2\n",
        "Print the count of occurrences of each category in so_survey_df's Gender column."
      ],
      "metadata": {
        "id": "UnWpDJakT4vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the count of occurrences\n",
        "print(so_survey_df['Gender'].value_counts())"
      ],
      "metadata": {
        "id": "UXl3IzUvT5AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "50 XP\n",
        "Replace all missing values in the Gender column with the string 'Not Given'. Make changes to the original DataFrame."
      ],
      "metadata": {
        "id": "7pwwZi7uUIXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace missing values\n",
        "so_survey_df['Gender'].fillna(value = 'Not Given', inplace = True)\n",
        "\n",
        "# Print the count of each value\n",
        "print(so_survey_df['Gender'].value_counts())"
      ],
      "metadata": {
        "id": "rx-MFM-bUIvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filling continuous missing values\n",
        "In the last lesson, you dealt with different methods of removing data missing values and filling in missing values with a fixed string. These approaches are valid in many cases, particularly when dealing with categorical columns but have limited use when working with continuous values. In these cases, it may be most valid to fill the missing values in the column with a value calculated from the entries present in the column.\n",
        "\n",
        "Instructions 1/3\n",
        "35 XP\n",
        "2\n",
        "3\n",
        "Print the first five rows of the StackOverflowJobsRecommend column of so_survey_df."
      ],
      "metadata": {
        "id": "rBZuKrHRVOzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first five rows of StackOverflowJobsRecommend column\n",
        "print(so_survey_df['StackOverflowJobsRecommend'].head())"
      ],
      "metadata": {
        "id": "8_-3-SHnVPOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/3\n",
        "35 XP\n",
        "3\n",
        "Replace the missing values in the StackOverflowJobsRecommend column with its mean. Make changes directly to the original DataFrame."
      ],
      "metadata": {
        "id": "EQ67GK65VXUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values with the mean\n",
        "so_survey_df['StackOverflowJobsRecommend'].fillna(so_survey_df['StackOverflowJobsRecommend'].mean(), inplace=True)\n",
        "\n",
        "# Print the first five rows of StackOverflowJobsRecommend column\n",
        "print(so_survey_df['StackOverflowJobsRecommend'].head())"
      ],
      "metadata": {
        "id": "DWjSsZAQVXs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/3\n",
        "30 XP\n",
        "Round the decimal values that you introduced in the StackOverflowJobsRecommend column."
      ],
      "metadata": {
        "id": "51m74gVMVkEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values with the mean\n",
        "so_survey_df['StackOverflowJobsRecommend'].fillna(so_survey_df['StackOverflowJobsRecommend'].mean(), inplace=True)\n",
        "\n",
        "# Round the StackOverflowJobsRecommend values\n",
        "so_survey_df['StackOverflowJobsRecommend'] = round(so_survey_df['StackOverflowJobsRecommend'])\n",
        "\n",
        "# Print the top 5 rows\n",
        "print(so_survey_df['StackOverflowJobsRecommend'].head())"
      ],
      "metadata": {
        "id": "qpMQrQagVkZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dealing with stray characters (I)\n",
        "In this exercise, you will work with the RawSalary column of so_survey_df which contains the wages of the respondents along with the currency symbols and commas, such as $42,000. When importing data from Microsoft Excel, more often that not you will come across data in this form.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "Remove the commas (,) from the RawSalary column."
      ],
      "metadata": {
        "id": "3kBvsJ2pW17h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the commas in the column\n",
        "so_survey_df['RawSalary'] = so_survey_df['RawSalary'].str.replace(',', '')"
      ],
      "metadata": {
        "id": "JI9aOC0VW2Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "\n",
        "Remove the dollar ($) signs from the RawSalary column."
      ],
      "metadata": {
        "id": "whbFcbDqW9Ea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the dollar signs in the column\n",
        "so_survey_df['RawSalary'] = so_survey_df['RawSalary'].str.replace('$', '')"
      ],
      "metadata": {
        "id": "S10dTgZKW-LB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dealing with stray characters (II)\n",
        "In the last exercise, you could tell quickly based off of the df.head() call which characters were causing an issue. In many cases this will not be so apparent. There will often be values deep within a column that are preventing you from casting a column as a numeric type so that it can be used in a model or further feature engineering.\n",
        "\n",
        "One approach to finding these values is to force the column to the data type desired using pd.to_numeric(), coercing any values causing issues to NaN, Then filtering the DataFrame by just the rows containing the NaN values.\n",
        "\n",
        "Try to cast the RawSalary column as a float and it will fail as an additional character can now be found in it. Find the character and remove it so the column can be cast as a float.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "2\n",
        "Attempt to convert the RawSalary column of so_survey_df to numeric values coercing all failures into null values.\n",
        "Find the indexes of the rows containing NaNs.\n",
        "Print the rows in RawSalary based on these indexes."
      ],
      "metadata": {
        "id": "9ePZw4p0XwJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Attempt to convert the column to numeric values\n",
        "numeric_vals = pd.to_numeric(so_survey_df['RawSalary'], errors='coerce')\n",
        "\n",
        "# Find the indexes of missing values\n",
        "idx = numeric_vals.isna()\n",
        "\n",
        "# Print the relevant rows\n",
        "print(so_survey_df['RawSalary'][idx])"
      ],
      "metadata": {
        "id": "PFwMfazdXwbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "50 XP\n",
        "Did you notice the pound (£) signs in the RawSalary column? Remove these signs like you did in the previous exercise.\n",
        "Convert the RawSalary column to float."
      ],
      "metadata": {
        "id": "acp4IHN5YZi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace the offending characters\n",
        "so_survey_df['RawSalary'] = so_survey_df['RawSalary'].str.replace('£', '')\n",
        "\n",
        "# Convert the column to float\n",
        "so_survey_df['RawSalary'] = so_survey_df['RawSalary'].astype('float')\n",
        "\n",
        "# Print the column\n",
        "print(so_survey_df['RawSalary'])"
      ],
      "metadata": {
        "id": "9UsHf2VIYZuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method chaining\n",
        "When applying multiple operations on the same column (like in the previous exercises), you made the changes in several steps, assigning the results back in each step. However, when applying multiple successive operations on the same column, you can \"chain\" these operations together for clarity and ease of management. This can be achieved by calling multiple methods sequentially:\n",
        "```\n",
        "# Method chaining\n",
        "df['column'] = df['column'].method1().method2().method3()\n",
        "\n",
        "# Same as\n",
        "df['column'] = df['column'].method1()\n",
        "df['column'] = df['column'].method2()\n",
        "df['column'] = df['column'].method3()\n",
        "```\n",
        "In this exercise you will repeat the steps you performed in the last two exercises, but do so using method chaining.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Remove the commas (,) from the RawSalary column of so_survey_df.\n",
        "Remove the dollar ($) signs from the RawSalary column.\n",
        "Remove the pound (£) signs from the RawSalary column.\n",
        "Convert the RawSalary column to float."
      ],
      "metadata": {
        "id": "yz2wNrBaak1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use method chaining\n",
        "so_survey_df['RawSalary'] = so_survey_df['RawSalary']\\\n",
        "                              .str.replace(',', '')\\\n",
        "                              .str.replace('$', '')\\\n",
        "                              .str.replace('£', '')\\\n",
        "                              .astype('float')\n",
        "\n",
        "# Print the RawSalary column\n",
        "print(so_survey_df['RawSalary'])"
      ],
      "metadata": {
        "id": "NH5c_VkXamFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What does your data look like? (I)\n",
        "Up until now you have focused on creating new features and dealing with issues in your data. Feature engineering can also be used to make the most out of the data that you already have and use it more effectively when creating machine learning models.\n",
        "Many algorithms may assume that your data is normally distributed, or at least that all your columns are on the same scale. This will often not be the case, e.g. one feature may be measured in thousands of dollars while another would be number of years. In this exercise, you will create plots to examine the distributions of some numeric columns in the so_survey_df DataFrame, stored in so_numeric_df.\n",
        "\n",
        "Instructions 1/3\n",
        "35 XP\n",
        "2\n",
        "3\n",
        "Generate a histogram of all columns in the so_numeric_df DataFrame."
      ],
      "metadata": {
        "id": "EtgO9Tb3mEgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a histogram\n",
        "so_numeric_df.hist()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUgkhnYdmE1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/3\n",
        "35 XP\n",
        "3\n",
        "Generate box plots of the Age and Years Experience columns in the so_numeric_df DataFrame."
      ],
      "metadata": {
        "id": "_i1Hf1EtmO2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a boxplot of two columns\n",
        "so_numeric_df[['Age', 'Years Experience']].boxplot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oOEsXr2MmPD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/3\n",
        "30 XP\n",
        "Generate a box plot of the ConvertedSalary column in the so_numeric_df DataFrame."
      ],
      "metadata": {
        "id": "-scH-6BCmivZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a boxplot of ConvertedSalary\n",
        "so_numeric_df[['ConvertedSalary']].boxplot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gwgPK8lvmi3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What does your data look like? (II)\n",
        "In the previous exercise you looked at the distribution of individual columns. While this is a good start, a more detailed view of how different features interact with each other may be useful as this can impact your decision on what to transform and how.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "2\n",
        "Import matplotlib's pyplot module as plt.\n",
        "Import seaborn as sns.\n",
        "Plot pairwise relationships in the so_numeric_df dataset.\n",
        "Show the plot."
      ],
      "metadata": {
        "id": "jTjWcW7PnNuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import packages\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Plot pairwise relationships\n",
        "sns.pairplot(so_numeric_df)\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n6jjtFxynOpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "50 XP\n",
        "Print the summary statistics of the so_numeric_df DataFrame."
      ],
      "metadata": {
        "id": "QYck915onauI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print summary statistics\n",
        "print(so_numeric_df.describe())"
      ],
      "metadata": {
        "id": "Cb6G72msna3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Normalization\n",
        "As discussed in the video, in normalization you linearly scale the entire column between 0 and 1, with 0 corresponding with the lowest value in the column, and 1 with the largest.\n",
        "When using scikit-learn (the most commonly used machine learning library in Python) you can use a MinMaxScaler to apply normalization. (It is called this as it scales your values between a minimum and maximum value.)\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import MinMaxScaler from sklearn's preprocessing module.\n",
        "Instantiate the MinMaxScaler() as MM_scaler.\n",
        "Fit the MinMaxScaler on the Age column of so_numeric_df.\n",
        "Transform the same column with the scaler you just fit."
      ],
      "metadata": {
        "id": "-NlN5hu6ofCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import MinMaxScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Instantiate MinMaxScaler\n",
        "MM_scaler = MinMaxScaler()\n",
        "\n",
        "# Fit MM_scaler to the data\n",
        "MM_scaler.fit(so_numeric_df[['Age']])\n",
        "\n",
        "# Transform the data using the fitted scaler\n",
        "so_numeric_df['Age_MM'] = MM_scaler.transform(so_numeric_df[['Age']])\n",
        "\n",
        "# Compare the origional and transformed column\n",
        "print(so_numeric_df[['Age_MM', 'Age']].head())"
      ],
      "metadata": {
        "id": "UoS4tFdjofSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standardization\n",
        "While normalization can be useful for scaling a column between two data points, it is hard to compare two scaled columns if even one of them is overly affected by outliers. One commonly used solution to this is called standardization, where instead of having a strict upper and lower bound, you center the data around its mean, and calculate the number of standard deviations away from mean each data point is.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import StandardScaler from sklearn's preprocessing module.\n",
        "Instantiate the StandardScaler() as SS_scaler.\n",
        "Fit the StandardScaler on the Age column of so_numeric_df.\n",
        "Transform the same column with the scaler you just fit."
      ],
      "metadata": {
        "id": "eBrye-DgowgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import StandardScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Instantiate StandardScaler\n",
        "SS_scaler = StandardScaler()\n",
        "\n",
        "# Fit SS_scaler to the data\n",
        "SS_scaler.fit(so_numeric_df[['Age']])\n",
        "\n",
        "# Transform the data using the fitted scaler\n",
        "so_numeric_df['Age_SS'] = SS_scaler.transform(so_numeric_df[['Age']])\n",
        "\n",
        "# Compare the origional and transformed column\n",
        "print(so_numeric_df[['Age_SS', 'Age']].head())"
      ],
      "metadata": {
        "id": "RUULtLPHowvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Log transformation\n",
        "In the previous exercises you scaled the data linearly, which will not affect the data's shape. This works great if your data is normally distributed (or closely normally distributed), an assumption that a lot of machine learning models make. Sometimes you will work with data that closely conforms to normality, e.g the height or weight of a population. On the other hand, many variables in the real world do not follow this pattern e.g, wages or age of a population. In this exercise you will use a log transform on the ConvertedSalary column in the so_numeric_df DataFrame as it has a large amount of its data centered around the lower values, but contains very high values also. These distributions are said to have a long right tail.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import PowerTransformer from sklearn's preprocessing module.\n",
        "Instantiate the PowerTransformer() as pow_trans.\n",
        "Fit the PowerTransformer on the ConvertedSalary column of so_numeric_df.\n",
        "Transform the same column with the scaler you just fit."
      ],
      "metadata": {
        "id": "auze7Dmep9aA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import PowerTransformer\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "# Instantiate PowerTransformer\n",
        "pow_trans = PowerTransformer()\n",
        "\n",
        "# Train the transform on the data\n",
        "pow_trans.fit(so_numeric_df[['ConvertedSalary']])\n",
        "\n",
        "# Apply the power transform to the data\n",
        "so_numeric_df['ConvertedSalary_LG'] = pow_trans.transform(so_numeric_df[['ConvertedSalary']])\n",
        "\n",
        "# Plot the data before and after the transformation\n",
        "so_numeric_df[['ConvertedSalary', 'ConvertedSalary_LG']].hist()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8gb0CkeEp91I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Percentage based outlier removal\n",
        "One way to ensure a small portion of data is not having an overly adverse effect is by removing a certain percentage of the largest and/or smallest values in the column. This can be achieved by finding the relevant quantile and trimming the data using it with a mask. This approach is particularly useful if you are concerned that the highest values in your dataset should be avoided. When using this approach, you must remember that even if there are no outliers, this will still remove the same top N percentage from the dataset.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Find the 95th quantile of the ConvertedSalary column.\n",
        "Trim the so_numeric_df DataFrame to retain all rows where ConvertedSalary is less than it's 95th quantile.\n",
        "Plot the histogram of so_numeric_df[['ConvertedSalary']].\n",
        "Plot the histogram of trimmed_df[['ConvertedSalary']]."
      ],
      "metadata": {
        "id": "sGV8qJCmq82I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the 95th quantile\n",
        "quantile = so_numeric_df['ConvertedSalary'].quantile(0.95)\n",
        "\n",
        "# Trim the outliers\n",
        "trimmed_df = so_numeric_df[so_numeric_df['ConvertedSalary'] < quantile]\n",
        "\n",
        "# The original histogram\n",
        "so_numeric_df[['ConvertedSalary']].hist()\n",
        "plt.show()\n",
        "plt.clf()\n",
        "\n",
        "# The trimmed histogram\n",
        "trimmed_df[['ConvertedSalary']].hist()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qMdPu5m0q9P5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistical outlier removal\n",
        "While removing the top N% of your data is useful for ensuring that very spurious points are removed, it does have the disadvantage of always removing the same proportion of points, even if the data is correct. A commonly used alternative approach is to remove data that sits further than three standard deviations from the mean. You can implement this by first calculating the mean and standard deviation of the relevant column to find upper and lower bounds, and applying these bounds as a mask to the DataFrame. This method ensures that only data that is genuinely different from the rest is removed, and will remove fewer points if the data is close together.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Calculate the standard deviation and mean of the ConvertedSalary column of so_numeric_df.\n",
        "Calculate the upper and lower bounds as three standard deviations away from the mean in both the directions.\n",
        "Trim the so_numeric_df DataFrame to retain all rows where ConvertedSalary is within the lower and upper bounds.\n"
      ],
      "metadata": {
        "id": "B_Xym_jErkxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the mean and standard dev\n",
        "std = so_numeric_df['ConvertedSalary'].std()\n",
        "mean = so_numeric_df['ConvertedSalary'].mean()\n",
        "\n",
        "# Calculate the cutoff\n",
        "cut_off = std * 3\n",
        "lower, upper = mean - cut_off, mean + cut_off\n",
        "\n",
        "# Trim the outliers\n",
        "trimmed_df = so_numeric_df[(so_numeric_df['ConvertedSalary'] < upper)\n",
        "                           & (so_numeric_df['ConvertedSalary'] > lower)]\n",
        "\n",
        "# The trimmed box plot\n",
        "trimmed_df[['ConvertedSalary']].boxplot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hKt3wkdqrlKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and testing transformations (I)\n",
        "So far you have created scalers based on a column, and then applied the scaler to the same data that it was trained on. When creating machine learning models you will generally build your models on historic data (train set) and apply your model to new unseen data (test set). In these cases you will need to ensure that the same scaling is being applied to both the training and test data.\n",
        "To do this in practice you train the scaler on the train set, and keep the trained scaler to apply it to the test set. You should never retrain a scaler on the test set.\n",
        "\n",
        "For this exercise and the next, we split the so_numeric_df DataFrame into train (so_train_numeric) and test (so_test_numeric) sets.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Instantiate the StandardScaler() as SS_scaler.\n",
        "Fit the StandardScaler on the Age column.\n",
        "Transform the Age column in the test set (so_test_numeric)."
      ],
      "metadata": {
        "id": "tOo4k1pTskbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import StandardScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Apply a standard scaler to the data\n",
        "SS_scaler = StandardScaler()\n",
        "\n",
        "# Fit the standard scaler to the data\n",
        "SS_scaler.fit(so_train_numeric[['Age']])\n",
        "\n",
        "# Transform the test data using the fitted scaler\n",
        "so_test_numeric['Age_ss'] = SS_scaler.transform(so_test_numeric[['Age']])\n",
        "print(so_test_numeric[['Age', 'Age_ss']].head())"
      ],
      "metadata": {
        "id": "iQ2OLahyskw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and testing transformations (II)\n",
        "Similar to applying the same scaler to both your training and test sets, if you have removed outliers from the train set, you probably want to do the same on the test set as well. Once again you should ensure that you use the thresholds calculated only from the train set to remove outliers from the test set.\n",
        "\n",
        "Similar to the last exercise, we split the so_numeric_df DataFrame into train (so_train_numeric) and test (so_test_numeric) sets.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Calculate the standard deviation and mean of the ConvertedSalary column.\n",
        "Calculate the upper and lower bounds as three standard deviations away from the mean in both the directions.\n",
        "Trim the so_test_numeric DataFrame to retain all rows where ConvertedSalary is within the lower and upper bounds."
      ],
      "metadata": {
        "id": "bDhPXN8-s7yu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_std = so_train_numeric['ConvertedSalary'].std()\n",
        "train_mean = so_train_numeric['ConvertedSalary'].mean()\n",
        "\n",
        "cut_off = train_std * 3\n",
        "train_lower, train_upper = train_mean - cut_off, train_mean + cut_off\n",
        "\n",
        "# Trim the test DataFrame\n",
        "trimmed_df = so_test_numeric[(so_test_numeric['ConvertedSalary'] < train_upper) \\\n",
        "                             & (so_test_numeric['ConvertedSalary'] > train_lower)]"
      ],
      "metadata": {
        "id": "ULopilvus8F9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning up your text\n",
        "Unstructured text data cannot be directly used in most analyses. Multiple steps need to be taken to go from a long free form string to a set of numeric columns in the right format that can be ingested by a machine learning model. The first step of this process is to standardize the data and eliminate any characters that could cause problems later on in your analytic pipeline.\n",
        "\n",
        "In this chapter you will be working with a new dataset containing the inaugural speeches of the presidents of the United States loaded as speech_df, with the speeches stored in the text column.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "2\n",
        "Print the first 5 rows of the text column to see the free text fields."
      ],
      "metadata": {
        "id": "Gj3uS7OOt9e3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first 5 rows of the text column\n",
        "print(speech_df['text'].head())"
      ],
      "metadata": {
        "id": "g6qtPlfBt9zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "50 XP\n",
        "Replace all non letter characters in the text column with a whitespace.\n",
        "Make all characters in the newly created text_clean column lower case."
      ],
      "metadata": {
        "id": "r0bpHMN7uIcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace all non letter characters with a whitespace\n",
        "speech_df['text_clean'] = speech_df['text'].str.replace('[^a-zA-Z]', ' ')\n",
        "\n",
        "# Change to lower case\n",
        "speech_df['text_clean'] = speech_df['text_clean'].str.lower()\n",
        "\n",
        "# Print the first 5 rows of the text_clean column\n",
        "print(speech_df['text_clean'].head())"
      ],
      "metadata": {
        "id": "Nh_AxBtvuImW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# High level text features\n",
        "Once the text has been cleaned and standardized you can begin creating features from the data. The most fundamental information you can calculate about free form text is its size, such as its length and number of words. In this exercise (and the rest of this chapter), you will focus on the cleaned/transformed text column (text_clean) you created in the last exercise.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Record the character length of each speech in the char_count column.\n",
        "Record the word count of each speech in the word_count column.\n",
        "Record the average word length of each speech in the avg_word_length column."
      ],
      "metadata": {
        "id": "dZyZuQPlu6zh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the length of each text\n",
        "speech_df['char_cnt'] = speech_df['text_clean'].str.len()\n",
        "\n",
        "# Count the number of words in each text\n",
        "speech_df['word_cnt'] = speech_df['text_clean'].str.split().str.len()\n",
        "\n",
        "# Find the average length of word\n",
        "speech_df['avg_word_length'] = speech_df['char_cnt'] / speech_df['word_cnt']\n",
        "\n",
        "# Print the first 5 rows of these columns\n",
        "print(speech_df[['text_clean', 'char_cnt', 'word_cnt', 'avg_word_length']])"
      ],
      "metadata": {
        "id": "2UqdH1rdu7Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Counting words (I)\n",
        "Once high level information has been recorded you can begin creating features based on the actual content of each text. One way to do this is to approach it in a similar way to how you worked with categorical variables in the earlier lessons.\n",
        "\n",
        "For each unique word in the dataset a column is created.\n",
        "For each entry, the number of times this word occurs is counted and the count value is entered into the respective column.\n",
        "These \"count\" columns can then be used to train machine learning models.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import CountVectorizer from sklearn.feature_extraction.text.\n",
        "Instantiate CountVectorizer and assign it to cv.\n",
        "Fit the vectorizer to the text_clean column.\n",
        "Print the feature names generated by the vectorizer."
      ],
      "metadata": {
        "id": "FV0K0I1LExxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Instantiate CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "\n",
        "# Fit the vectorizer\n",
        "cv.fit(speech_df['text_clean'])\n",
        "\n",
        "# Print feature names\n",
        "print(cv.get_feature_names())"
      ],
      "metadata": {
        "id": "3utZ6CsbEyJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Counting words (II)\n",
        "Once the vectorizer has been fit to the data, it can be used to transform the text to an array representing the word counts. This array will have a row per block of text and a column for each of the features generated by the vectorizer that you observed in the last exercise.\n",
        "\n",
        "The vectorizer to you fit in the last exercise (cv) is available in your workspace.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "2\n",
        "Apply the vectorizer to the text_clean column.\n",
        "Convert this transformed (sparse) array into a numpy array with counts."
      ],
      "metadata": {
        "id": "HRNfL1MoFSpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the vectorizer\n",
        "cv_transformed = cv.transform(speech_df['text_clean'])\n",
        "\n",
        "# Print the full array\n",
        "cv_array = cv_transformed.toarray()\n",
        "print(cv_array)"
      ],
      "metadata": {
        "id": "QUH3J-faFS8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "50 XP\n",
        "Print the dimensions of this numpy array."
      ],
      "metadata": {
        "id": "aIw-N2-rFZBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the vectorizer\n",
        "cv_transformed = cv.transform(speech_df['text_clean'])\n",
        "\n",
        "# Print the full array\n",
        "cv_array = cv_transformed.toarray()\n",
        "\n",
        "# Print the shape of cv_array\n",
        "print(cv_array.shape)"
      ],
      "metadata": {
        "id": "jT6NFzzIFZKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Limiting your features\n",
        "As you have seen, using the CountVectorizer with its default settings creates a feature for every single word in your corpus. This can create far too many features, often including ones that will provide very little analytical value.\n",
        "\n",
        "For this purpose CountVectorizer has parameters that you can set to reduce the number of features:\n",
        "\n",
        "min_df : Use only words that occur in more than this percentage of documents. This can be used to remove outlier words that will not generalize across texts.\n",
        "max_df : Use only words that occur in less than this percentage of documents. This is useful to eliminate very common words that occur in every corpus without adding value such as \"and\" or \"the\".\n",
        "Instructions\n",
        "100 XP\n",
        "Limit the number of features in the CountVectorizer by setting the minimum number of documents a word can appear to 20% and the maximum to 80%.\n",
        "Fit and apply the vectorizer on text_clean column in one step.\n",
        "Convert this transformed (sparse) array into a numpy array with counts.\n",
        "Print the dimensions of the new reduced array."
      ],
      "metadata": {
        "id": "4s59sk3oF_9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Specify arguements to limit the number of features generated\n",
        "cv = CountVectorizer(min_df = 0.2, max_df = 0.8)\n",
        "\n",
        "# Fit, transform, and convert into array\n",
        "cv_transformed = cv.fit_transform(speech_df['text_clean'])\n",
        "cv_array = cv_transformed.toarray()\n",
        "\n",
        "# Print the array shape\n",
        "print(cv_array.shape)"
      ],
      "metadata": {
        "id": "a6QOfAxZGASr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text to DataFrame\n",
        "Now that you have generated these count based features in an array you will need to reformat them so that they can be combined with the rest of the dataset. This can be achieved by converting the array into a pandas DataFrame, with the feature names you found earlier as the column names, and then concatenate it with the original DataFrame.\n",
        "\n",
        "The numpy array (cv_array) and the vectorizer (cv) you fit in the last exercise are available in your workspace.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create a DataFrame cv_df containing the cv_array as the values and the feature names as the column names.\n",
        "Add the prefix Counts_ to the column names for ease of identification.\n",
        "Concatenate this DataFrame (cv_df) to the original DataFrame (speech_df) column wise.\n"
      ],
      "metadata": {
        "id": "f78-U8jxGoBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame with these features\n",
        "cv_df = pd.DataFrame(cv_array, columns=cv.get_feature_names_out()).add_prefix('Counts_')\n",
        "\n",
        "# Add the new columns to the original DataFrame\n",
        "speech_df_new = pd.concat([speech_df, cv_df], axis=1, sort=False)\n",
        "print(speech_df_new.head())"
      ],
      "metadata": {
        "id": "zCoRVTQjGoWy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}