{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/kIZIM/w0q2ixc/mEmzX7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pattiecodes/DC_MLScientist_in_Py/blob/main/Module_9_ML_for_Time_Series_Data_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Module Start ---"
      ],
      "metadata": {
        "id": "0pFPHMPovtsB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting a time series (I)\n",
        "In this exercise, you'll practice plotting the values of two time series without the time component.\n",
        "\n",
        "Two DataFrames, data and data2 are available in your workspace.\n",
        "\n",
        "Unless otherwise noted, assume that all required packages are loaded with their common aliases throughout this course.\n",
        "\n",
        "Note: This course assumes some familiarity with time series data, as well as how to use them in data analytics pipelines. For an introduction to time series, we recommend the Introduction to Time Series Analysis in Python and Visualizing Time Series Data with Python courses.\n",
        "\n",
        "Instructions 1/3\n",
        "15 XP\n",
        "2\n",
        "3\n",
        "Print the first five rows of data."
      ],
      "metadata": {
        "id": "gLAdHxee8tqA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMKlfcSUvlC_"
      },
      "outputs": [],
      "source": [
        "# Print the first 5 rows of data\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/3\n",
        "15 XP\n",
        "3\n",
        "Print the first five rows of data2."
      ],
      "metadata": {
        "id": "gDJ-X2mc82I9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first 5 rows of data2\n",
        "print(data2.head())"
      ],
      "metadata": {
        "id": "xtqWTD3x82vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/3\n",
        "70 XP\n",
        "Plot the values column of both the data sets on top of one another, one per axis object."
      ],
      "metadata": {
        "id": "1GQpXO1c-rOH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the time series in each dataset\n",
        "fig, axs = plt.subplots(2, 1, figsize=(5, 10))\n",
        "data.iloc[:1000].plot(y='data_values', ax=axs[0])\n",
        "data2.iloc[:1000].plot(y='data_values', ax=axs[1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SqB_6b8N-r5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting a time series (II)\n",
        "You'll now plot both the datasets again, but with the included time stamps for each (stored in the column called \"time\"). Let's see if this gives you some more context for understanding each time series data.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Plot data and data2 on top of one another, one per axis object.\n",
        "The x-axis should represent the time stamps and the y-axis should represent the dataset values.\n"
      ],
      "metadata": {
        "id": "J1svoUELAaxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the time series in each dataset\n",
        "fig, axs = plt.subplots(2, 1, figsize=(5, 10))\n",
        "data.iloc[:1000].plot(x='time', y='data_values', ax=axs[0])\n",
        "data2.iloc[:1000].plot(x='time', y='data_values', ax=axs[1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rR4MFyySAbCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fitting a simple model: classification\n",
        "In this exercise, you'll use the iris dataset (representing petal characteristics of a number of flowers) to practice using the scikit-learn API to fit a classification model. You can see a sample plot of the data to the right.\n",
        "\n",
        "Note: This course assumes some familiarity with Machine Learning and scikit-learn. For an introduction to scikit-learn, we recommend the Supervised Learning with Scikit-Learn and Preprocessing for Machine Learning in Python courses.\n",
        "\n",
        "Instructions 1/2\n",
        "10 XP\n",
        "2\n",
        "Print the first five rows of data."
      ],
      "metadata": {
        "id": "B-6Re_k02zhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first 5 rows for inspection\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "wgcCWpwD2z9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "90 XP\n",
        "2\n",
        "Extract the \"petal length (cm)\" and \"petal width (cm)\" columns of data and assign it to X.\n",
        "Fit a model on X and y."
      ],
      "metadata": {
        "id": "t6u4doP923az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# Construct data for the model\n",
        "X = data[[\"petal length (cm)\", \"petal width (cm)\"]]\n",
        "y = data[['target']]\n",
        "\n",
        "# Fit the model\n",
        "model = LinearSVC()\n",
        "model.fit(X, y)"
      ],
      "metadata": {
        "id": "Uy3lsNYY23ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting using a classification model\n",
        "Now that you have fit your classifier, let's use it to predict the type of flower (or class) for some newly-collected flowers.\n",
        "\n",
        "Information about petal width and length for several new flowers is stored in the variable targets. Using the classifier you fit, you'll predict the type of each flower.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Predict the flower type using the array X_predict.\n",
        "Run the given code to visualize the predictions."
      ],
      "metadata": {
        "id": "rlCPBp1d3eWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create input array\n",
        "X_predict = targets[['petal length (cm)', 'petal width (cm)']]\n",
        "\n",
        "# Predict with the model\n",
        "predictions = model.predict(X_predict)\n",
        "print(predictions)\n",
        "\n",
        "# Visualize predictions and actual values\n",
        "plt.scatter(X_predict['petal length (cm)'], X_predict['petal width (cm)'],\n",
        "            c=predictions, cmap=plt.cm.coolwarm)\n",
        "plt.title(\"Predicted class values\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MdMgSeWN3erN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fitting a simple model: regression\n",
        "In this exercise, you'll practice fitting a regression model using data from the California housing market. A DataFrame called housing is available in your workspace. It contains many variables of data (stored as columns). Can you find a relationship between the following two variables?\n",
        "\n",
        "\"MedHouseVal\": the median house value for California districts (in $100,000s of dollars)\n",
        "\"AveRooms\" : average number of rooms per dwelling\n",
        "Instructions\n",
        "100 XP\n",
        "Prepare X and y DataFrames using the data in housing.\n",
        "X should be the Median House Value, y average number of rooms per dwelling.\n",
        "Fit a regression model that uses these variables (remember to shape the variables correctly!).\n",
        "Don't forget that each variable must be the correct shape for scikit-learn to use it!"
      ],
      "metadata": {
        "id": "3aTME2gH1MoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import linear_model\n",
        "\n",
        "# Prepare input and output DataFrames\n",
        "X = housing[[\"MedHouseVal\"]]\n",
        "y = housing[[\"AveRooms\"]]\n",
        "\n",
        "# Fit the model\n",
        "model = linear_model.LinearRegression()\n",
        "model.fit(X, y)"
      ],
      "metadata": {
        "id": "TRKfTuOX1NPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting using a regression model\n",
        "Now that you've fit a model with the California housing data, lets see what predictions it generates on some new data. You can investigate the underlying relationship that the model has found between inputs and outputs by feeding in a range of numbers as inputs and seeing what the model predicts for each input.\n",
        "\n",
        "A 1-D array new_inputs consisting of 100 \"new\" values for \"MedHouseVal\" (median house value) is available in your workspace along with the model you fit in the previous exercise.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Review new_inputs in the shell.\n",
        "Reshape new_inputs appropriately to generate predictions.\n",
        "Run the given code to visualize the predictions."
      ],
      "metadata": {
        "id": "O4_y3BP11z0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions with the model using those inputs\n",
        "predictions = model.predict(new_inputs.reshape(-1, 1))\n",
        "\n",
        "# Visualize the inputs and predicted values\n",
        "plt.scatter(new_inputs, predictions, color='r', s=3)\n",
        "plt.xlabel('inputs')\n",
        "plt.ylabel('predictions')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nFDBPn9q15_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inspecting the classification data\n",
        "In these final exercises of this chapter, you'll explore the two datasets you'll use in this course.\n",
        "\n",
        "The first is a collection of heartbeat sounds. Hearts normally have a predictable sound pattern as they beat, but some disorders can cause the heart to beat abnormally. This dataset contains a training set with labels for each type of heartbeat, and a testing set with no labels. You'll use the testing set to validate your models.\n",
        "\n",
        "As you have labeled data, this dataset is ideal for classification. In fact, it was originally offered as a part of a public Kaggle competition.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Use glob to return a list of the .wav files in data_dir directory.\n",
        "Import the first audio file in the list using librosa.\n",
        "Generate a time array for the data.\n",
        "Plot the waveform for this file, along with the time array."
      ],
      "metadata": {
        "id": "3WmqzY2eMevR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa as lr\n",
        "from glob import glob\n",
        "\n",
        "# List all the wav files in the folder\n",
        "audio_files = glob(data_dir + '/*.wav')\n",
        "\n",
        "# Read in the first audio file, create the time array\n",
        "audio, sfreq = lr.load(audio_files[0])\n",
        "time = np.arange(0, len(audio)) / sfreq\n",
        "\n",
        "# Plot audio over time\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(time, audio)\n",
        "ax.set(xlabel='Time (s)', ylabel='Sound Amplitude')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DkM9SfLbMfYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inspecting the regression data\n",
        "The next dataset contains information about company market value over several years of time. This is one of the most popular kind of time series data used for regression. If you can model the value of a company as it changes over time, you can make predictions about where that company will be in the future. This dataset was also originally provided as part of a public Kaggle competition.\n",
        "\n",
        "In this exercise, you'll plot the time series for a number of companies to get an understanding of how they are (or aren't) related to one another.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import the data with Pandas (stored in the file 'prices.csv').\n",
        "Convert the index of data to datetime.\n",
        "Loop through each column of data and plot the the column's values over time."
      ],
      "metadata": {
        "id": "FasDnwSRNWFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read in the data\n",
        "data = pd.read_csv('prices.csv', index_col=0)\n",
        "\n",
        "# Convert the index of the DataFrame to datetime\n",
        "data.index = pd.to_datetime(data.index)\n",
        "print(data.head())\n",
        "\n",
        "# Loop through each column, plot its values over time\n",
        "fig, ax = plt.subplots()\n",
        "for column in data:\n",
        "    data[column].plot(ax=ax, label=column)\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E1eF-M9sNWbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Many repetitions of sounds\n",
        "In this exercise, you'll start with perhaps the simplest classification technique: averaging across dimensions of a dataset and visually inspecting the result.\n",
        "\n",
        "You'll use the heartbeat data described in the last chapter. Some recordings are normal heartbeat activity, while others are abnormal activity. Let's see if you can spot the difference.\n",
        "\n",
        "Two DataFrames, normal and abnormal, each with the shape of (n_times_points, n_audio_files) containing the audio for several heartbeats are available in your workspace. Also, the sampling frequency is loaded into a variable called sfreq. A convenience plotting function show_plot_and_make_titles() is also available in your workspace.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "First, create the time array for these audio files (all audios are the same length).\n",
        "Then, stack the values of the two DataFrames together (normal and abnormal, in that order) so that you have a single array of shape (n_audio_files, n_times_points).\n",
        "Finally, use the code provided to loop through each list item / axis, and plot the audio over time in the corresponding axis object.\n",
        "You'll plot normal heartbeats in the left column, and abnormal ones in the right column"
      ],
      "metadata": {
        "id": "iD8BhGqdNvcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(3, 2, figsize=(15, 7), sharex=True, sharey=True)\n",
        "\n",
        "# Calculate the time array\n",
        "time = np.arange(normal.shape[0]) / sfreq\n",
        "\n",
        "# Stack the normal/abnormal audio so you can loop and plot\n",
        "stacked_audio = np.hstack([normal, abnormal]).T\n",
        "\n",
        "# Loop through each audio file / ax object and plot\n",
        "# .T.ravel() transposes the array, then unravels it into a 1-D vector for looping\n",
        "for iaudio, ax in zip(stacked_audio, axs.T.ravel()):\n",
        "    ax.plot(time, iaudio)\n",
        "show_plot_and_make_titles()"
      ],
      "metadata": {
        "id": "11K2CskSNxbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Invariance in time\n",
        "While you should always start by visualizing your raw data, this is often uninformative when it comes to discriminating between two classes of data points. Data is usually noisy or exhibits complex patterns that aren't discoverable by the naked eye.\n",
        "\n",
        "Another common technique to find simple differences between two sets of data is to average across multiple instances of the same class. This may remove noise and reveal underlying patterns (or, it may not).\n",
        "\n",
        "In this exercise, you'll average across many instances of each class of heartbeat sound.\n",
        "\n",
        "The two DataFrames (normal and abnormal) and the time array (time) from the previous exercise are available in your workspace.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Average across the audio files contained in normal and abnormal, leaving the time dimension.\n",
        "Visualize these averages over time."
      ],
      "metadata": {
        "id": "qhBD8JWjUR69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Average across the audio files of each DataFrame\n",
        "mean_normal = np.mean(normal, axis=1)\n",
        "mean_abnormal = np.mean(abnormal, axis=1)\n",
        "\n",
        "# Plot each average over time\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3), sharey=True)\n",
        "ax1.plot(time, mean_normal)\n",
        "ax1.set(title=\"Normal Data\")\n",
        "ax2.plot(time, mean_abnormal)\n",
        "ax2.set(title=\"Abnormal Data\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V_GUvI2YUSXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a classification model\n",
        "While eye-balling differences is a useful way to gain an intuition for the data, let's see if you can operationalize things with a model. In this exercise, you will use each repetition as a datapoint, and each moment in time as a feature to fit a classifier that attempts to predict abnormal vs. normal heartbeats using only the raw data.\n",
        "\n",
        "We've split the two DataFrames (normal and abnormal) into X_train, X_test, y_train, and y_test.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Create an instance of the Linear SVC model and fit the model using the training data.\n",
        "Use the testing data to generate predictions with the model.\n",
        "Score the model using the provided code."
      ],
      "metadata": {
        "id": "IQ4H2_c9U-qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# Initialize and fit the model\n",
        "model = LinearSVC()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Generate predictions and score them manually\n",
        "predictions = model.predict(X_test)\n",
        "print(sum(predictions == y_test.squeeze()) / len(y_test))"
      ],
      "metadata": {
        "id": "8UybAQxCU-8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating the envelope of sound\n",
        "One of the ways you can improve the features available to your model is to remove some of the noise present in the data. In audio data, a common way to do this is to smooth the data and then rectify it so that the total amount of sound energy over time is more distinguishable. You'll do this in the current exercise.\n",
        "\n",
        "A heartbeat file is available in the variable audio.\n",
        "\n",
        "Instructions 1/3\n",
        "33 XP\n",
        "2\n",
        "3\n",
        "Visualize the raw audio you'll use to calculate the envelope."
      ],
      "metadata": {
        "id": "RG8_76a2hcHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the raw data first\n",
        "audio.plot(figsize=(10, 5))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aQs4X5YphdS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/3\n",
        "33 XP\n",
        "3\n",
        "Rectify the audio.\n",
        "Plot the result."
      ],
      "metadata": {
        "id": "Sko7_wdThwq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rectify the audio signal\n",
        "audio_rectified = audio.apply(np.abs)\n",
        "\n",
        "# Plot the result\n",
        "audio_rectified.plot(figsize=(10, 5))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hZsVGI50hxNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/3\n",
        "34 XP\n",
        "Smooth the audio file by applying a rolling mean.\n",
        "Plot the result."
      ],
      "metadata": {
        "id": "rhZgxHzih_FE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Smooth by applying a rolling mean\n",
        "audio_rectified_smooth = audio_rectified.rolling(50).mean()\n",
        "\n",
        "# Plot the result\n",
        "audio_rectified_smooth.plot(figsize=(10, 5))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZNZ-fa2eh_hA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating features from the envelope\n",
        "Now that you've removed some of the noisier fluctuations in the audio, let's see if this improves your ability to classify.\n",
        "\n",
        "audio_rectified_smooth from the previous exercise is available in your workspace.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Calculate the mean, standard deviation, and maximum value for each heartbeat sound.\n",
        "Column stack these stats in the same order.\n",
        "Use cross-validation to fit a model on each CV iteration."
      ],
      "metadata": {
        "id": "yHnjCJ7qieNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate stats\n",
        "means = np.mean(audio_rectified_smooth, axis=0)\n",
        "stds = np.std(audio_rectified_smooth, axis=0)\n",
        "maxs = np.max(audio_rectified_smooth, axis=0)\n",
        "\n",
        "# Create the X and y arrays\n",
        "X = np.column_stack([means, stds, maxs])\n",
        "y = labels.reshape(-1, 1)\n",
        "\n",
        "# Fit the model and score on testing data\n",
        "from sklearn.model_selection import cross_val_score\n",
        "percent_score = cross_val_score(model, X, y, cv=5)\n",
        "print(np.mean(percent_score))"
      ],
      "metadata": {
        "id": "PYVd4g3miejW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Derivative features: The tempogram\n",
        "One benefit of cleaning up your data is that it lets you compute more sophisticated features. For example, the envelope calculation you performed is a common technique in computing tempo and rhythm features. In this exercise, you'll use librosa to compute some tempo and rhythm features for heartbeat data, and fit a model once more.\n",
        "\n",
        "Note that librosa functions tend to only operate on numpy arrays instead of DataFrames, so we'll access our Pandas data as a Numpy array with the .values attribute.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "2\n",
        "Use librosa to calculate a tempogram of each heartbeat audio.\n",
        "Calculate the mean, standard deviation, and maximum of each tempogram (this time using DataFrame methods)\n"
      ],
      "metadata": {
        "id": "utnAzMp4ym80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the tempo of the sounds\n",
        "tempos = []\n",
        "for col, i_audio in audio.items():\n",
        "    tempos.append(lr.beat.tempo(i_audio.values, sr=sfreq, hop_length=2**6, aggregate=None))\n",
        "\n",
        "# Convert the list to an array so you can manipulate it more easily\n",
        "tempos = np.array(tempos)\n",
        "\n",
        "# Calculate statistics of each tempo\n",
        "tempos_mean = tempos.mean(axis=-1)\n",
        "tempos_std = tempos.std(axis=-1)\n",
        "tempos_max = tempos.max(axis=-1)"
      ],
      "metadata": {
        "id": "l90nnJbVynW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "50 XP\n",
        "Column stack these tempo features (mean, standard deviation, and maximum) in the same order.\n",
        "Score the classifier with cross-validation."
      ],
      "metadata": {
        "id": "W9fytscczPDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the X and y arrays\n",
        "X = np.column_stack([means, stds, maxs, tempos_mean, tempos_std, tempos_max])\n",
        "y = labels.reshape(-1, 1)\n",
        "\n",
        "# Fit the model and score on testing data\n",
        "percent_score = cross_val_score(model, X, y, cv=5)\n",
        "print(np.mean(percent_score))"
      ],
      "metadata": {
        "id": "hBsjIPDvzPNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spectrograms of heartbeat audio\n",
        "Spectral engineering is one of the most common techniques in machine learning for time series data. The first step in this process is to calculate a spectrogram of sound. This describes what spectral content (e.g., low and high pitches) are present in the sound over time. In this exercise, you'll calculate a spectrogram of a heartbeat audio file.\n",
        "\n",
        "We've loaded a single heartbeat sound in the variable audio.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "2\n",
        "Import the short-time fourier transform (stft) function from librosa.core.\n",
        "Calculate the spectral content (using the short-time fourier transform function) of audio."
      ],
      "metadata": {
        "id": "iQtYKGbWkChp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the stft function\n",
        "from librosa.core import stft\n",
        "\n",
        "# Prepare the STFT\n",
        "HOP_LENGTH = 2**4\n",
        "spec = stft(audio, hop_length=HOP_LENGTH, n_fft=2**7)"
      ],
      "metadata": {
        "id": "X9hLhAhOkC3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "50 XP\n",
        "Convert the spectogram (spec) to decibels.\n",
        "Visualize the spectogram."
      ],
      "metadata": {
        "id": "AZwLJwL2kage"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from librosa.core import amplitude_to_db\n",
        "from librosa.display import specshow\n",
        "\n",
        "# Convert into decibels\n",
        "spec_db = amplitude_to_db(spec)\n",
        "\n",
        "# Compare the raw audio to the spectrogram of the audio\n",
        "fig, axs = plt.subplots(2, 1, figsize=(10, 10), sharex=True)\n",
        "axs[0].plot(time, audio)\n",
        "specshow(spec_db, sr=sfreq, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH, ax=axs[1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mk6uBA78kaqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Engineering spectral features\n",
        "As you can probably tell, there is a lot more information in a spectrogram compared to a raw audio file. By computing the spectral features, you have a much better idea of what's going on. As such, there are all kinds of spectral features that you can compute using the spectrogram as a base. In this exercise, you'll look at a few of these features.\n",
        "\n",
        "The spectogram spec from the previous exercise is available in your workspace.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "2\n",
        "Calculate the spectral bandwidth as well as the spectral centroid of the spectrogram by using functions in librosa.feature."
      ],
      "metadata": {
        "id": "auEJ7-Lvsbrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa as lr\n",
        "\n",
        "# Calculate the spectral centroid and bandwidth for the spectrogram\n",
        "bandwidths = lr.feature.spectral_bandwidth(S=spec)[0]\n",
        "centroids = lr.feature.spectral_centroid(S=spec)[0]"
      ],
      "metadata": {
        "id": "1XaeRDpascZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "50 XP\n",
        "Convert the spectrogram to decibels for visualization.\n",
        "Plot the spectrogram over time."
      ],
      "metadata": {
        "id": "iqLcSLMwtiJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from librosa.core import amplitude_to_db\n",
        "from librosa.display import specshow\n",
        "\n",
        "# Convert spectrogram to decibels for visualization\n",
        "spec_db = amplitude_to_db(spec)\n",
        "\n",
        "# Display these features on top of the spectrogram\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "specshow(spec_db, x_axis='time', y_axis='hz', hop_length=HOP_LENGTH, ax=ax)\n",
        "ax.plot(times_spec, centroids)\n",
        "ax.fill_between(times_spec, centroids - bandwidths / 2, centroids + bandwidths / 2, alpha=.5)\n",
        "ax.set(ylim=[None, 6000])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BYSa54qetihg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combining many features in a classifier\n",
        "You've spent this lesson engineering many features from the audio data - some contain information about how the audio changes in time, others contain information about the spectral content that is present.\n",
        "\n",
        "The beauty of machine learning is that it can handle all of these features at the same time. If there is different information present in each feature, it should improve the classifier's ability to distinguish the types of audio. Note that this often requires more advanced techniques such as regularization, which we'll cover in the next chapter.\n",
        "\n",
        "For the final exercise in the chapter, we've loaded many of the features that you calculated before. Combine all of them into an array that can be fed into the classifier, and see how it does.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "2\n",
        "Loop through each spectrogram, calculating the mean spectral bandwidth and centroid of each."
      ],
      "metadata": {
        "id": "fwh3gMeRxHlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through each spectrogram\n",
        "bandwidths = []\n",
        "centroids = []\n",
        "\n",
        "for spec in spectrograms:\n",
        "    # Calculate the mean spectral bandwidth\n",
        "    this_mean_bandwidth = np.mean(lr.feature.spectral_bandwidth(S=spec))\n",
        "    # Calculate the mean spectral centroid\n",
        "    this_mean_centroid = np.mean(lr.feature.spectral_centroid(S=spec))\n",
        "    # Collect the values\n",
        "    bandwidths.append(this_mean_bandwidth)\n",
        "    centroids.append(this_mean_centroid)"
      ],
      "metadata": {
        "id": "mXVR4m0ExIGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "50 XP\n",
        "Column stack all the features to create the array X.\n",
        "Score the classifier with cross-validation."
      ],
      "metadata": {
        "id": "PkJ0TIPyy14E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create X and y arrays\n",
        "X = np.column_stack([means, stds, maxs, tempo_mean, tempo_max, tempo_std, bandwidths, centroids])\n",
        "y = labels.reshape(-1, 1)\n",
        "\n",
        "# Fit the model and score on testing data\n",
        "percent_score = cross_val_score(model, X, y, cv=5)\n",
        "print(np.mean(percent_score))"
      ],
      "metadata": {
        "id": "xuEV0WKIy2NL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducing the dataset\n",
        "As mentioned in the video, you'll deal with stock market prices that fluctuate over time. In this exercise you've got historical prices from two tech companies (Ebay and Yahoo) in the DataFrame prices. You'll visualize the raw data for the two companies, then generate a scatter plot showing how the values for each company compare with one another. Finally, you'll add in a \"time\" dimension to your scatter plot so you can see how this relationship changes over time.\n",
        "\n",
        "The data has been loaded into a DataFrame called prices.\n",
        "\n",
        "Instructions 1/3\n",
        "35 XP\n",
        "Plot the data in prices. Pay attention to any irregularities you notice."
      ],
      "metadata": {
        "id": "TddbTCY96B2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the raw values over time\n",
        "prices.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y3VO0LtS6fRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/3\n",
        "\n",
        "Generate a scatter plot with the values of Ebay on the x-axis, and Yahoo on the y-axis. Look up the symbols for both companies from the column names of the DataFrame."
      ],
      "metadata": {
        "id": "PAE-7Hz56iC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatterplot with one company per axis\n",
        "prices.plot.scatter('EBAY', 'YHOO')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rRGW_KFn6jfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/3\n",
        "\n",
        "Finally, encode time as the color of each datapoint in order to visualize how the relationship between these two variables changes."
      ],
      "metadata": {
        "id": "-Qb5tGjT6-Y1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatterplot with color relating to time\n",
        "prices.plot.scatter('EBAY', 'YHOO', c=prices.index, cmap=plt.cm.viridis, colorbar=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n0nTvuJ76_Uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fitting a simple regression model\n",
        "Now we'll look at a larger number of companies. Recall that we have historical price values for many companies. Let's use data from several companies to predict the value of a test company. You'll attempt to predict the value of the Apple stock price using the values of NVidia, Ebay, and Yahoo. Each of these is stored as a column in the all_prices DataFrame. Below is a mapping from company name to column name:\n",
        "```\n",
        "ebay: \"EBAY\"\n",
        "nvidia: \"NVDA\"\n",
        "yahoo: \"YHOO\"\n",
        "apple: \"AAPL\"\n",
        "We'll use these columns to define the input/output arrays in our model.\n",
        "```\n",
        "Instructions\n",
        "100 XP\n",
        "Create the X and y arrays by using the column names provided.\n",
        "The input values should be from the companies \"ebay\", \"nvidia\", and \"yahoo\"\n",
        "The output values should be from the company \"apple\"\n",
        "Use the data to train and score the model with cross-validation."
      ],
      "metadata": {
        "id": "pFFC8THY3_ZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Use stock symbols to extract training data\n",
        "X = all_prices[['EBAY', 'NVDA', 'YHOO']]\n",
        "y = all_prices[['AAPL']]\n",
        "\n",
        "# Fit and score the model with cross-validation\n",
        "scores = cross_val_score(Ridge(), X, y, cv=3)\n",
        "print(scores)"
      ],
      "metadata": {
        "id": "hR6gzdM-4BIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing predicted values\n",
        "When dealing with time series data, it's useful to visualize model predictions on top of the \"actual\" values that are used to test the model.\n",
        "\n",
        "In this exercise, after splitting the data (stored in the variables X and y) into training and test sets, you'll build a model and then visualize the model's predictions on top of the testing data in order to estimate the model's performance.\n",
        "\n",
        "Instructions 1/2\n",
        "50 XP\n",
        "2\n",
        "Split the data (X and y) into training and test sets.\n",
        "Use the training data to train the regression model.\n",
        "Then use the testing data to generate predictions for the model."
      ],
      "metadata": {
        "id": "MCmlgqPk4cSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Split our data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    train_size=.8, shuffle=False)\n",
        "\n",
        "# Fit our model and generate predictions\n",
        "model = Ridge()\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "score = r2_score(y_test, predictions)\n",
        "print(score)"
      ],
      "metadata": {
        "id": "RggQlglv4cl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "50 XP\n",
        "Plot a time series of the predicted and \"actual\" values of the testing data."
      ],
      "metadata": {
        "id": "2mUEtQ3y4zNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize our predictions along with the \"true\" values, and print the score\n",
        "fig, ax = plt.subplots(figsize=(15, 5))\n",
        "ax.plot(y_test, color='k', lw=3)\n",
        "ax.plot(predictions, color='r', lw=2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IaJ3tibp4ziA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing messy data\n",
        "Let's take a look at a new dataset - this one is a bit less-clean than what you've seen before.\n",
        "\n",
        "As always, you'll first start by visualizing the raw data. Take a close look and try to find datapoints that could be problematic for fitting models.\n",
        "\n",
        "The data has been loaded into a DataFrame called prices.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Visualize the time series data using Pandas.\n",
        "Calculate the number of missing values in each time series. Note any irregularities that you can see. What do you think they are?"
      ],
      "metadata": {
        "id": "8nePZCR1AUrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the dataset\n",
        "prices.plot(legend=False)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Count the missing values of each time series\n",
        "missing_values = prices.isna().sum()\n",
        "print(missing_values)"
      ],
      "metadata": {
        "id": "34ipFO3rAvhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imputing missing values\n",
        "When you have missing data points, how can you fill them in?\n",
        "\n",
        "In this exercise, you'll practice using different interpolation methods to fill in some missing values, visualizing the result each time. But first, you will create the function (interpolate_and_plot()) you'll use to interpolate missing data points and plot them.\n",
        "\n",
        "A single time series has been loaded into a DataFrame called prices.\n",
        "\n",
        "Instructions 1/4\n",
        "25 XP\n",
        "Create a boolean mask for missing values and interpolate the missing values using the interpolation argument of the function."
      ],
      "metadata": {
        "id": "dCem-x-vBWkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function we'll use to interpolate and plot\n",
        "def interpolate_and_plot(prices, interpolation):\n",
        "\n",
        "    # Create a boolean mask for missing values\n",
        "    missing_values = prices.isna()\n",
        "\n",
        "    # Interpolate the missing values\n",
        "    prices_interp = prices.interpolate(interpolation)\n",
        "\n",
        "    # Plot the results, highlighting the interpolated values in black\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    prices_interp.plot(color='k', alpha=.6, ax=ax, legend=False)\n",
        "\n",
        "    # Now plot the interpolated values on top in red\n",
        "    prices_interp[missing_values].plot(ax=ax, color='r', lw=3, legend=False)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "icIewvyfBXCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/4\n",
        "\n",
        "Interpolate using the latest non-missing value and plot the results.\n",
        "\n",
        "Recall that interpolate_and_plot's second input is a string specifying the kind of interpolation to use."
      ],
      "metadata": {
        "id": "_59YWXLWCT8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Interpolate using the latest non-missing value\n",
        "interpolation_type = 'zero'\n",
        "interpolate_and_plot(prices, interpolation_type)"
      ],
      "metadata": {
        "id": "obKt15rYCU4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 3/4\n",
        "\n",
        "Interpolate linearly and plot the results."
      ],
      "metadata": {
        "id": "t2Sir-dXCcPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Interpolate linearly\n",
        "interpolation_type = 'linear'\n",
        "interpolate_and_plot(prices, interpolation_type)"
      ],
      "metadata": {
        "id": "2dZnDqcCCdqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 4/4\n",
        "\n",
        "Interpolate with a quadratic function and plot the results."
      ],
      "metadata": {
        "id": "gFRj_W6DCv3I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Interpolate with a quadratic function\n",
        "interpolation_type = 'quadratic'\n",
        "interpolate_and_plot(prices, interpolation_type)"
      ],
      "metadata": {
        "id": "MQNhk1NtCyMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transforming raw data\n",
        "In the last chapter, you calculated the rolling mean. In this exercise, you will define a function that calculates the percent change of the latest data point from the mean of a window of previous data points. This function will help you calculate the percent change over a rolling window.\n",
        "\n",
        "This is a more stable kind of time series that is often useful in machine learning.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Define a percent_change function that takes an input time series and does the following:\n",
        "Extract all but the last value of the input series (assigned to previous_values) and the only the last value of the timeseries ( assigned to last_value)\n",
        "Calculate the percentage difference between the last value and the mean of earlier values.\n",
        "Using a rolling window of 20, apply this function to prices, and visualize it using the given code."
      ],
      "metadata": {
        "id": "YU329a-jEAYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your custom function\n",
        "def percent_change(series):\n",
        "    # Collect all *but* the last value of this window, then the final value\n",
        "    previous_values = series[:-1]\n",
        "    last_value = series[-1]\n",
        "\n",
        "    # Calculate the % difference between the last value and the mean of earlier values\n",
        "    percent_change = (last_value - np.mean(previous_values)) / np.mean(previous_values)\n",
        "    return percent_change\n",
        "\n",
        "# Apply your custom function and plot\n",
        "prices_perc = prices.rolling(20).apply(percent_change)\n",
        "prices_perc.loc[\"2014\":\"2015\"].plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hXZn3jjsEAz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handling outliers\n",
        "In this exercise, you'll handle outliers - data points that are so different from the rest of your data, that you treat them differently from other \"normal-looking\" data points. You'll use the output from the previous exercise (percent change over time) to detect the outliers. First you will write a function that replaces outlier data points with the median value from the entire time series.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Define a function that takes an input series and does the following:\n",
        "Calculates the absolute value of each datapoint's distance from the series mean, then creates a boolean mask for datapoints that are three times the standard deviation from the mean.\n",
        "Use this boolean mask to replace the outliers with the median of the entire series.\n",
        "Apply this function to your data and visualize the results using the given code."
      ],
      "metadata": {
        "id": "fWSVQndJSHvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_outliers(series):\n",
        "    # Calculate the absolute difference of each timepoint from the series mean\n",
        "    absolute_differences_from_mean = np.abs(series - np.mean(series))\n",
        "\n",
        "    # Calculate a mask for the differences that are > 3 standard deviations from zero\n",
        "    this_mask = absolute_differences_from_mean > (np.std(series) * 3)\n",
        "\n",
        "    # Replace these values with the median accross the data\n",
        "    series[this_mask] = np.nanmedian(series)\n",
        "    return series\n",
        "\n",
        "# Apply your preprocessing function to the timeseries and plot the results\n",
        "prices_perc = prices_perc.apply(replace_outliers)\n",
        "prices_perc.loc[\"2014\":\"2015\"].plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NYjLHC8uSIWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Engineering multiple rolling features at once\n",
        "Now that you've practiced some simple feature engineering, let's move on to something more complex. You'll calculate a collection of features for your time series data and visualize what they look like over time. This process resembles how many other time series models operate.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Define a list consisting of four features you will calculate: the minimum, maximum, mean, and standard deviation (in that order).\n",
        "Using the rolling window (prices_perc_rolling) we defined for you, calculate the features from features_to_calculate.\n",
        "Plot the results over time, along with the original time series using the given code."
      ],
      "metadata": {
        "id": "1PY5TLktTrKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a rolling window with Pandas, excluding the right-most datapoint of the window\n",
        "prices_perc_rolling = prices_perc.rolling(20, min_periods=5, closed='right')\n",
        "\n",
        "# Define the features you'll calculate for each window\n",
        "features_to_calculate = [np.min, np.max, np.mean, np.std]\n",
        "\n",
        "# Calculate these features for your rolling window object\n",
        "features = prices_perc_rolling.aggregate(features_to_calculate)\n",
        "\n",
        "# Plot the results\n",
        "ax = features.loc[:\"2011-01\"].plot()\n",
        "prices_perc.loc[:\"2011-01\"].plot(ax=ax, color='k', alpha=.2, lw=3)\n",
        "ax.legend(loc=(1.01, .6))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IhHe2NaZTrrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Percentiles and partial functions\n",
        "In this exercise, you'll practice how to pre-choose arguments of a function so that you can pre-configure how it runs. You'll use this to calculate several percentiles of your data using the same percentile() function in numpy.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import partial from functools.\n",
        "Use the partial() function to create several feature generators that calculate percentiles of your data using a list comprehension.\n",
        "Using the rolling window (prices_perc_rolling) we defined for you, calculate the quantiles using percentile_functions.\n",
        "Visualize the results using the code given to you."
      ],
      "metadata": {
        "id": "mlFUwUcnUR8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import partial from functools\n",
        "from functools import partial\n",
        "percentiles = [1, 10, 25, 50, 75, 90, 99]\n",
        "\n",
        "# Use a list comprehension to create a partial function for each quantile\n",
        "percentile_functions = [partial(np.percentile, q=percentile) for percentile in percentiles]\n",
        "\n",
        "# Calculate each of these quantiles on the data using a rolling window\n",
        "prices_perc_rolling = prices_perc.rolling(20, min_periods=5, closed='right')\n",
        "features_percentiles = prices_perc_rolling.aggregate(percentile_functions)\n",
        "\n",
        "# Plot a subset of the result\n",
        "ax = features_percentiles.loc[:\"2011-01\"].plot(cmap=plt.cm.viridis)\n",
        "ax.legend(percentiles, loc=(1.01, .5))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oCfLMyUiUSRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using \"date\" information\n",
        "It's easy to think of timestamps as pure numbers, but don't forget they generally correspond to things that happen in the real world. That means there's often extra information encoded in the data such as \"is it a weekday?\" or \"is it a holiday?\". This information is often useful in predicting timeseries data.\n",
        "\n",
        "In this exercise, you'll extract these date/time based features. A single time series has been loaded in a variable called prices.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Calculate the day of the week, week number in a year, and month number in a year.\n",
        "Add each one as a column to the prices_perc DataFrame, under the names day_of_week, week_of_year and month_of_year, respectively."
      ],
      "metadata": {
        "id": "x95TLlrwXYYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract date features from the data, add them as columns\n",
        "prices_perc['day_of_week'] = prices_perc.index.day_of_week\n",
        "prices_perc['week_of_year'] = prices_perc.index.week\n",
        "prices_perc['month_of_year'] = prices_perc.index.month\n",
        "\n",
        "# Print prices_perc\n",
        "print(prices_perc)"
      ],
      "metadata": {
        "id": "YT-C0SFdXYyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating time-shifted features\n",
        "In machine learning for time series, it's common to use information about previous time points to predict a subsequent time point.\n",
        "\n",
        "In this exercise, you'll \"shift\" your raw data and visualize the results. You'll use the percent change time series that you calculated in the previous chapter, this time with a very short window. A short window is important because, in a real-world scenario, you want to predict the day-to-day fluctuations of a time series, not its change over a longer window of time.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Use a dictionary comprehension to create multiple time-shifted versions of prices_perc using the lags specified in shifts.\n",
        "Convert the result into a DataFrame.\n",
        "Use the given code to visualize the results."
      ],
      "metadata": {
        "id": "Kl0txPwrBOF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These are the \"time lags\"\n",
        "shifts = np.arange(1, 11).astype(int)\n",
        "\n",
        "# Use a dictionary comprehension to create name: value pairs, one pair per shift\n",
        "shifted_data = {\"lag_{}_day\".format(day_shift): prices_perc.shift(day_shift) for day_shift in shifts}\n",
        "\n",
        "# Convert into a DataFrame for subsequent use\n",
        "prices_perc_shifted = pd.DataFrame(shifted_data)\n",
        "\n",
        "# Plot the first 100 samples of each\n",
        "ax = prices_perc_shifted.iloc[:100].plot(cmap=plt.cm.viridis)\n",
        "prices_perc.iloc[:100].plot(color='r', lw=2)\n",
        "ax.legend(loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XHQ7SWABBOfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Special case: Auto-regressive models\n",
        "Now that you've created time-shifted versions of a single time series, you can fit an auto-regressive model. This is a regression model where the input features are time-shifted versions of the output time series data. You are using previous values of a timeseries to predict current values of the same timeseries (thus, it is auto-regressive).\n",
        "\n",
        "By investigating the coefficients of this model, you can explore any repetitive patterns that exist in a timeseries, and get an idea for how far in the past a data point is predictive of the future.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Replace missing values in prices_perc_shifted with the median of the DataFrame and assign it to X.\n",
        "Replace missing values in prices_perc with the median of the series and assign it to y.\n",
        "Fit a regression model using the X and y arrays."
      ],
      "metadata": {
        "id": "3TwV-dRNB9N5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace missing values with the median for each column\n",
        "X = prices_perc_shifted.fillna(np.nanmedian(prices_perc_shifted))\n",
        "y = prices_perc.fillna(np.nanmedian(prices_perc))\n",
        "\n",
        "# Fit the model\n",
        "model = Ridge()\n",
        "model.fit(X, y)"
      ],
      "metadata": {
        "id": "28R0BnubB9hM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize regression coefficients\n",
        "Now that you've fit the model, let's visualize its coefficients. This is an important part of machine learning because it gives you an idea for how the different features of a model affect the outcome.\n",
        "\n",
        "The shifted time series DataFrame (prices_perc_shifted) and the regression model (model) are available in your workspace.\n",
        "\n",
        "In this exercise, you will create a function that, given a set of coefficients and feature names, visualizes the coefficient values.\n",
        "\n",
        "Instructions 1/2\n",
        "80 XP\n",
        "2\n",
        "Define a function (called visualize_coefficients) that takes as input an array of coefficients, an array of each coefficient's name, and an instance of a Matplotlib axis object. It should then generate a bar plot for the input coefficients, with their names on the x-axis."
      ],
      "metadata": {
        "id": "8HYYMhwgDA82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_coefficients(coefs, names, ax):\n",
        "    # Make a bar plot for the coefficients, including their names on the x-axis\n",
        "    ax.bar(names, coefs)\n",
        "    ax.set(xlabel='Coefficient name', ylabel='Coefficient value')\n",
        "\n",
        "    # Set formatting so it looks nice\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
        "    return ax"
      ],
      "metadata": {
        "id": "UxzlEnRyDBTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions 2/2\n",
        "20 XP\n",
        "Use this function (visualize_coefficients()) with the coefficients contained in the model variable and column names of prices_perc_shifted."
      ],
      "metadata": {
        "id": "DDbEBWw-D9R5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the output data up to \"2011-01\"\n",
        "fig, axs = plt.subplots(2, 1, figsize=(10, 5))\n",
        "y.loc[:'2011-01'].plot(ax=axs[0])\n",
        "\n",
        "# Run the function to visualize model's coefficients\n",
        "visualize_coefficients(model.coef_, prices_perc_shifted.columns, ax=axs[1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xAjH7uOTD9mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Auto-regression with a smoother time series\n",
        "Now, let's re-run the same procedure using a smoother signal. You'll use the same percent change algorithm as before, but this time use a much larger window (40 instead of 20). As the window grows, the difference between neighboring timepoints gets smaller, resulting in a smoother signal. What do you think this will do to the auto-regressive model?\n",
        "\n",
        "prices_perc_shifted and model (updated to use a window of 40) are available in your workspace.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Using the function (visualize_coefficients()) you created in the last exercise, generate a plot with coefficients of model and column names of prices_perc_shifted."
      ],
      "metadata": {
        "id": "-IeX3e3aEwGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the output data up to \"2011-01\"\n",
        "fig, axs = plt.subplots(2, 1, figsize=(10, 5))\n",
        "y.loc[:'2011-01'].plot(ax=axs[0])\n",
        "\n",
        "# Run the function to visualize model's coefficients\n",
        "visualize_coefficients(model.coef_, prices_perc_shifted.columns, ax=axs[1])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5ezGfX-PEwZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-validation with shuffling\n",
        "As you'll recall, cross-validation is the process of splitting your data into training and test sets multiple times. Each time you do this, you choose a different training and test set. In this exercise, you'll perform a traditional ShuffleSplit cross-validation on the company value data from earlier. Later we'll cover what changes need to be made for time series data. The data we'll use is the same historical price data for several large companies.\n",
        "\n",
        "An instance of the Linear regression object (model) is available in your workspace along with the function r2_score() for scoring. Also, the data is stored in arrays X and y. We've also provided a helper function (visualize_predictions()) to help visualize the results.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Initialize a ShuffleSplit cross-validation object with 10 splits.\n",
        "Iterate through CV splits using this object. On each iteration:\n",
        "Fit a model using the training indices.\n",
        "Generate predictions using the test indices, score the model (R^2) using the predictions, and collect the results."
      ],
      "metadata": {
        "id": "WUnhySXkGvpc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import ShuffleSplit and create the cross-validation object\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "cv = ShuffleSplit(n_splits=10, random_state=1)\n",
        "\n",
        "# Iterate through CV splits\n",
        "results = []\n",
        "for tr, tt in cv.split(X, y):\n",
        "    # Fit the model on training data\n",
        "    model.fit(X[tr], y[tr])\n",
        "\n",
        "    # Generate predictions on the test data, score the predictions, and collect\n",
        "    prediction = model.predict(X[tt])\n",
        "    score = r2_score(y[tt], prediction)\n",
        "    results.append((prediction, score, tt))\n",
        "\n",
        "# Custom function to quickly visualize predictions\n",
        "visualize_predictions(results)"
      ],
      "metadata": {
        "id": "18jpzKpkGxtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cross-validation without shuffling\n",
        "Now, re-run your model fit using block cross-validation (without shuffling all datapoints). In this case, neighboring time-points will be kept close to one another. How do you think the model predictions will look in each cross-validation loop?\n",
        "\n",
        "An instance of the Linear regression model object is available in your workspace. Also, the arrays X and y (training data) are available too.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Instantiate another cross-validation object, this time using KFold cross-validation with 10 splits and no shuffling.\n",
        "Iterate through this object to fit a model using the training indices and generate predictions using the test indices.\n",
        "Visualize the predictions across CV splits using the helper function ```(visualize_predictions())``` we've provided.\n"
      ],
      "metadata": {
        "id": "JN8NwnnKHfwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create KFold cross-validation object\n",
        "from sklearn.model_selection import KFold\n",
        "cv = KFold(n_splits=10, shuffle=False)\n",
        "\n",
        "# Iterate through CV splits\n",
        "results = []\n",
        "for tr, tt in cv.split(X, y):\n",
        "    # Fit the model on training data\n",
        "    model.fit(X[tr], y[tr])\n",
        "\n",
        "    # Generate predictions on the test data and collect\n",
        "    prediction = model.predict(X[tt])\n",
        "    results.append((prediction, tt))\n",
        "\n",
        "# Custom function to quickly visualize predictions\n",
        "visualize_predictions(results)"
      ],
      "metadata": {
        "id": "xmqnjBL-Hiqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Time-based cross-validation\n",
        "Finally, let's visualize the behavior of the time series cross-validation iterator in scikit-learn. Use this object to iterate through your data one last time, visualizing the training data used to fit the model on each iteration.\n",
        "\n",
        "An instance of the Linear regression model object is available in your workpsace. Also, the arrays X and y (training data) are available too.\n",
        "\n",
        "Instructions\n",
        "100 XP\n",
        "Import TimeSeriesSplit from sklearn.model_selection.\n",
        "Instantiate a time series cross-validation iterator with 10 splits.\n",
        "Iterate through CV splits. On each iteration, visualize the values of the input data that would be used to train the model for that iteration."
      ],
      "metadata": {
        "id": "3sPyO1DHH_hp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import TimeSeriesSplit\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "# Create time-series cross-validation object\n",
        "cv = TimeSeriesSplit(n_splits = 10)\n",
        "\n",
        "# Iterate through CV splits\n",
        "fig, ax = plt.subplots()\n",
        "for ii, (tr, tt) in enumerate(cv.split(X, y)):\n",
        "    # Plot the training data on each iteration, to see the behavior of the CV\n",
        "    ax.plot(tr, ii + y[tr])\n",
        "\n",
        "ax.set(title='Training data on each CV iteration', ylabel='CV iteration')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KHq559PPH_0r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}